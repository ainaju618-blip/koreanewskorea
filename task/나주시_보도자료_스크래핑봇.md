# 나주시 보도자료 스크래핑봇

> Korea NEWS 뉴스 자동화 시스템 - 나주시 보도자료 수집 모듈
> 
> **작성일:** 2024-12-06  
> **버전:** v1.0  
> **작성자:** Korea NEWS 개발팀

---

## 1. 개요

### 1.1 프로젝트 배경
Korea NEWS는 광주·전남 지역 뉴스를 전문으로 다루는 인터넷신문사입니다. 
지역 보도자료를 신속하게 수집하고 발행하기 위해 자동화 시스템이 필요했습니다.

### 1.2 시스템 개요
나주시 보도자료 스크래핑봇은 나주시청 공식 홈페이지에서 보도자료를 자동으로 수집하여 
Korea NEWS 데이터베이스(Supabase)에 저장하는 Python 기반 자동화 도구입니다.

### 1.3 핵심 정보

| 항목 | 내용 |
|------|------|
| **봇 이름** | 나주뉴스스크래핑 (`나주뉴스스크래핑.py`) |
| **대상 사이트** | https://www.naju.go.kr |
| **수집 대상** | 보도자료 게시판 (`/www/administration/reporting/coverage`) |
| **일일 생산량** | 평균 6건 |
| **기준 시작일** | 2024-09-19 (신문사 등록일) |
| **누적 수집량** | 1,787건 (2024-09-19 ~ 2025-12-05) |

---

## 2. 목적

### 2.1 주요 목적

1. **수동 작업 최소화**
   - 기자가 매일 나주시 홈페이지를 방문하여 보도자료를 확인하는 수고를 덜어줌
   - 자동 수집으로 누락 방지

2. **신속한 뉴스 발행**
   - 보도자료 게시 후 빠른 시간 내 수집 및 저장
   - 관리자 승인 후 즉시 발행 가능

3. **데이터 아카이빙**
   - 신문사 등록일(2024-09-19) 이후 모든 보도자료 축적
   - 과거 데이터 검색 및 활용 가능

### 2.2 운영 시나리오

```
[평일 오전 9시]
    ↓
스케줄러가 봇 자동 실행
    ↓
나주시 홈페이지에서 신규 보도자료 6건 감지
    ↓
본문, 이미지, 날짜 등 상세 정보 추출
    ↓
Supabase posts 테이블에 status="draft"로 저장
    ↓
(선택) 텔레그램으로 발행인에게 알림
    ↓
발행인이 Admin 페이지에서 확인 후 [승인] 클릭
    ↓
Korea NEWS 메인 페이지에 기사 노출
```

---

## 3. 개발 과정

### 3.1 Phase 1: 문제 분석 및 요구사항 정의

**해결해야 할 문제:**
- 나주시 보도자료 페이지는 일반적인 HTML 구조가 아닌 특수한 방식으로 본문을 저장
- 본문이 `og:description` 메타 태그에 **Base64 인코딩** + **PHP serialize** 형식으로 저장됨
- 기존 단순 스크래퍼로는 본문 추출 불가 (26자만 추출되는 문제 발생)

**요구사항:**
- 2024-09-19(신문사 등록일)부터 현재까지 모든 보도자료 수집
- 본문 최대 5,000자 추출
- 이미지 URL 추출
- 작성일을 ISO 8601 형식으로 변환
- 랜덤 조회수(30~70) 부여

### 3.2 Phase 2: 핵심 로직 개발

#### 3.2.1 Base64 + PHP Serialize 디코딩

나주시 홈페이지의 특수한 인코딩 방식을 해독하는 로직 개발:

```python
def extract_content_from_og_description(html: str) -> str:
    """og:description 메타 태그에서 Base64 인코딩된 본문 추출"""
    soup = BeautifulSoup(html, 'html.parser')
    og_desc = soup.find('meta', property='og:description')
    
    if not og_desc or not og_desc.get('content'):
        return ''
    
    encoded = og_desc['content']
    
    # 1단계: Base64 디코딩
    decoded_bytes = base64.b64decode(encoded)
    decoded_str = decoded_bytes.decode('utf-8')
    
    # 2단계: PHP serialize 형식에서 문자열 추출
    if decoded_str.startswith('a:'):
        content = decode_php_serialize(decoded_str)
    else:
        content = decoded_str
    
    # 3단계: HTML 태그 제거 및 정규화
    content = re.sub(r'<[^>]+>', ' ', content)
    content = re.sub(r'\s+', ' ', content).strip()
    
    return content[:5000]
```

#### 3.2.2 페이지네이션 처리

보도자료 목록이 여러 페이지에 걸쳐 있으므로, 자동으로 다음 페이지로 이동:

```python
def collect_articles(start_date, end_date):
    page = 1
    while not stop_collecting:
        items = fetch_press_list(page)
        
        for item in items:
            if item['date'] < start_date:
                stop_collecting = True
                break
            # 상세 페이지 수집...
        
        page += 1
```

#### 3.2.3 날짜 범위 필터링

신문사 등록일 이후 기사만 수집하도록 날짜 필터링:

```python
REGISTRATION_DATE = '2024-09-19'  # 신문사 등록일

def is_within_date_range(date_str, start_date, end_date):
    article_date = datetime.strptime(date_str, '%Y-%m-%d')
    start = datetime.strptime(start_date, '%Y-%m-%d')
    end = datetime.strptime(end_date, '%Y-%m-%d')
    return start <= article_date <= end
```

### 3.3 Phase 3: 데이터 정규화 및 저장

수집된 데이터를 Supabase 스키마에 맞게 변환:

```python
article = {
    'idx': item['idx'],
    'title': item['title'],
    'content': detail['content'],
    'content_length': detail['content_length'],
    'published_at': format_published_at(item['date']),  # ISO 8601
    'date': item['date'],
    'department': item['department'],
    'image_url': detail['image_url'],
    'view_count': generate_random_view_count(30, 70),
    'source': '나주시',
    'source_url': item['url'],
    'category': '지역',
}
```

### 3.4 Phase 4: 테스트 및 검증

**테스트 결과:**
- 총 수집 기사: 1,787건
- 수집 기간: 2024-09-19 ~ 2025-12-05
- 평균 본문 길이: 941자
- 이미지 포함률: 약 70%

---

## 4. 데이터 매핑

### 4.1 스크래핑 데이터 → Supabase 매핑

| 스크래핑 필드 | DB 필드 (`posts`) | 타입 | 필수 | 설명 |
|---------------|-------------------|------|------|------|
| `title` | `title` | text | ✅ | 보도자료 제목 |
| `content` | `content` | text | ✅ | 본문 (최대 5000자) |
| `published_at` | `published_at` | timestamptz | ✅ | 작성일 (ISO 8601) |
| `source_url` | `original_link` | text | ✅ | 원본 URL (중복 체크용) |
| `source` | `source` | text | ✅ | "나주시" (고정) |
| `image_url` | `thumbnail_url` | text | ❌ | 대표 이미지 URL |
| `view_count` | `view_count` | int | ❌ | 랜덤 (30~70) |
| `category` | `category` | text | ✅ | "지역" 또는 "나주" |
| - | `status` | text | ✅ | "draft" (기본값) |
| - | `ai_summary` | text | ❌ | AI 요약 (선택) |
| - | `created_at` | timestamptz | ✅ | 자동 생성 |

### 4.2 저장 예시

```json
{
  "title": "나주시, 2024년 친환경농업 우수단체 선정",
  "content": "나주시가 전라남도 친환경농업 평가에서 우수단체로 선정되었다. 시는 지난해부터 친환경 인증 농가 확대와 유기농 재배 면적 증가를 위해 다양한 지원 사업을 추진해왔다...(중략)...앞으로도 지속가능한 농업 발전을 위해 노력하겠다고 밝혔다.",
  "published_at": "2024-12-06T09:00:00+09:00",
  "original_link": "https://www.naju.go.kr/www/administration/reporting/coverage?idx=12345",
  "source": "나주시",
  "thumbnail_url": "https://www.naju.go.kr/upload/image/press_20241206.jpg",
  "view_count": 52,
  "category": "지역",
  "status": "draft"
}
```

---

## 5. 파일 구조

```
d:\cbt\koreanews\
├── 나주뉴스스크래핑.py      # 메인 실행 파일
├── scrapers/
│   └── naju_scraper.py     # 스크래퍼 모듈 (원본)
├── db_loader.py            # Supabase 저장 모듈
├── naju_articles.json      # 수집된 기사 (1,787건)
├── .env                    # 환경변수 (Supabase 키)
└── task/
    └── 나주시_보도자료_스크래핑봇.md  # 본 문서
```

---

## 6. 실행 가이드

### 6.1 환경 설정

```bash
# 가상환경 활성화
cd d:\cbt\koreanews
.\venv\Scripts\activate

# 의존성 설치 (최초 1회)
pip install -r requirements.txt
```

### 6.2 수동 실행

```bash
# 전체 기간 수집 (2024-09-19 ~ 오늘)
python 나주뉴스스크래핑.py
```

### 6.3 자동 실행 (예정)

| 방식 | 설명 | 추천 |
|------|------|------|
| Windows 작업 스케줄러 | 로컬 PC에서 정해진 시간에 실행 | 개발/테스트용 |
| GitHub Actions | 클라우드에서 cron 기반 실행 | 운영용 ✅ |
| 서버 crontab | Linux 서버 cron | 서버 운영 시 |

---

## 7. 향후 개발 계획

### 7.1 즉시 구현 예정
- [ ] Supabase 직접 저장 기능 통합
- [ ] 중복 체크 로직 (`original_link` 기반)
- [ ] 일일 실행 스케줄러 설정

### 7.2 추후 개선
- [ ] AI 1줄 요약 자동 생성
- [ ] 텔레그램 알림 연동
- [ ] 에러 로깅 및 재시도 로직
- [ ] 다른 지자체 확장 (광주시, 전남도, 목포시 등)

---

## 8. 버전 이력

| 날짜 | 버전 | 변경 내용 |
|------|------|-----------|
| 2024-12-06 | v1.0 | 초기 개발 완료, 1,787건 수집 성공 |
| 2024-12-09 | v1.1 | (예정) Supabase 직접 저장 기능 추가 |
