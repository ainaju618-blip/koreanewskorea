# 통합 지역 뉴스 자동화 시스템 구축 계획 (Integrated Regional News Automation Plan)

## 1. 개요 (Overview)
본 문서는 Korea NEWS의 핵심 경쟁력인 **"압도적인 지역 뉴스 커버리지"**를 달성하기 위한 자동화 시스템 구축 계획이다.
나주시 보도자료 스크래핑 봇의 성공적인 프로토타입을 기반으로, **광주·전남 전역(27개 시군구)**의 보도자료를 **자동 수집, 가공, 송고**하는 시스템을 구축한다.

> **핵심 목표:** "기자 없이도, 전남 22개 시군과 광주 5개 구의 소식이 매일 실시간으로 업데이트되는 살아있는 뉴스 포털 구축"

---

## 2. 수집 대상 (Target Coverage)
**총 29개 기관**의 공식 홈페이지 내 '보도자료' 게시판을 타겟팅한다.

### A. 광역 자치단체 (2개)
1.  **전라남도청** (`www.jeonnam.go.kr`)
2.  **광주광역시청** (`www.gwangju.go.kr`)

### B. 전라남도 기초 지자체 (22개 시군)
*   **시 단위 (5):** 목포시, 여수시, 순천시, 나주시, 광양시
*   **군 단위 (17):** 담양군, 곡성군, 구례군, 고흥군, 보성군, 화순군, 장흥군, 강진군, 해남군, 영암군, 무안군, 함평군, 영광군, 장성군, 완도군, 진도군, 신안군

### C. 광주광역시 구청 (5개)
1.  동구청
2.  서구청
3.  남구청
4.  북구청
5.  광산구청

### D. 교육청 및 교육지원청 (24개)
*   **광역 교육청 (2):** 전라남도교육청, 광주광역시교육청
*   **지역 교육지원청 (22):** 목포, 여수, 순천, 나주, 광양, 담양, 곡성, 구례, 고흥, 보성, 화순, 장흥, 강진, 해남, 영암, 무안, 함평, 영광, 장성, 완도, 진도, 신안 교육지원청

> **총 수집 대상:** 29개(지자체) + 24개(교육청) = **53개 기관**

---

## 3. 실행 단계 (Implementation Phases)

### Phase 1. 과거 데이터 일괄 수집 (Back-filling)
*   **목표:** 사이트 오픈 시점에 풍부한 콘텐츠를 확보하고, 아카이브로서의 가치를 창출한다.
*   **수집 기간:** **2024년 9월 19일 (신문사 등록일)** ~ 현재
*   **작업 내용:**
    1.  각 지자체별 크롤러를 동시/순차 가동.
    2.  과거 보도자료 전량 수집 (약 1만 건 예상).
    3.  Supabase DB에 적재 (초기 데이터 베이스 구축).

### Phase 2. 데일리 루틴 자동화 (Daily Routine)
*   **목표:** 매일 발생하는 새로운 소식을 타 매체보다 빠르거나 비슷하게 송고한다.
*   **실행 스케줄 (1일 3회):**
    *   **09:00:** 밤사이 올라온 보도자료 수집 (조간용)
    *   **13:00:** 오전 중 발생한 보도자료 수집 (속보용)
    *   **17:00:** 오후 마감 보도자료 수집 (석간/내일용)
*   **자동화 범위:** 수집 -> 이미지 다운로드 -> 본문 추출 -> DB 저장 (`Draft` 상태).

### Phase 3. 클릭수 부스팅 & 가상 기자단 (Operation Strategy)
*   **클릭수 목표:** 2026년 3월까지 일일 평균 클릭수를 발행기관 평균 수준으로 상향.
*   **알고리즘:** 
    *   초기 등록 시: 30~70회 (랜덤)
    *   시간 경과에 따른 자연 증가 로직 (S-Curve 형태) 적용.
    *   인기 키워드(축제, 지원금 등) 가중치 부여.
*   **가상 기자단 운영:**
    *   각 지역별 '담당 기자(Bot)' ID 부여 (예: 나주 담당 '나빛가람 기자', 여수 담당 '여수바다 기자').
    *   독자에게는 현장 취재 기자가 있는 듯한 인상 제공.

---

## 4. 기술적 접근 (Technical Architecture)

### 4.1 통합 크롤러 구조 (Universal Scraper)
나주시청의 복잡한 구조(Base64/PHP Serialize)까지 해결한 강력한 파서(Parser) 엔진을 활용한다.

```python
class UniversalScraper:
    def __init__(self, target_config):
        self.base_url = target_config['url']
        self.board_pattern = target_config['pattern']
    
    def fetch_list(self, page=1):
        # 공통 리스트 수집 로직
        pass

    def fetch_detail(self, url):
        # 본문 및 첨부파일(이미지) 추출
        # HWP 등 첨부파일 텍스트 변환 로직 포함
        pass
```

### 4.2 데이터 파이프라인
1.  **Collection:** Scraper가 각 시군 홈페이지 순회.
2.  **Normalization:** 제목, 본문, 작성일, 부서명, 이미지 등 데이터 표준 포맷으로 변환.
3.  **AI Processing:** (선택사항) 보도자료 어조를 기사체로 변환 ("~했다" 등).
4.  **Storage:** Supabase `posts` 테이블에 `Insert`.
    *   중복 방지: 원본 URL (`original_link`)을 Key로 체크.

---

## 5. UI/UX 연동 계획
*   **메뉴 구조:** [전남] 메뉴 하위에 22개 시군 서브 메뉴를 두지 않고, 태그/필터 방식으로 처리하여 깔끔하게 구성.
*   **메인 노출:** 수집된 기사 중 **이미지가 포함된 기사**를 우선적으로 메인 또는 지역 섹션에 노출.

---

## 6. 향후 일정 (Next Steps)
1.  전체 29개 기관 **보도자료 게시판 URL 리스트업** 완료.
2.  **공통 크롤러(Universal Scraper)** 코딩 및 테스트.
3.  Phase 1 (과거 데이터 수집) 실행 및 DB 적재 확인.
4.  스케줄러 등록 및 자동화 개시.
