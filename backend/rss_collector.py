"""
ê¸€ë¡œë²Œ AI ë‰´ìŠ¤ RSS ìˆ˜ì§‘ê¸°
- í•´ì™¸ Tech ë¸”ë¡œê·¸(TechCrunch, Wired AI) RSS ìˆ˜ì§‘
- Next.js API ì—°ë™ (/api/bot/ingest)
- [AI ë²ˆì—­ ì¤€ë¹„ì¤‘] íƒœê·¸ ì ìš©
"""

import feedparser
from datetime import datetime
import json
import os
import sys
from bs4 import BeautifulSoup
from dateutil import parser  # Robust date parsing
from typing import List, Dict

# ê³µí†µ API í´ë¼ì´ì–¸íŠ¸ ìž„í¬íŠ¸
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'scrapers'))
try:
    from scrapers.utils.api_client import send_article_to_server
except ImportError:
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰ ì‹œ
    from utils.api_client import send_article_to_server

# RSS Feed URLs (NEXT_MISSION.md ìŠ¤íŽ™)
RSS_FEEDS = {
    "TechCrunch": "https://techcrunch.com/category/artificial-intelligence/feed/",
    "Wired AI": "https://www.wired.com/feed/tag/ai/latest/rss",
    "MIT Tech Review": "https://www.technologyreview.com/topic/artificial-intelligence/feed",
    "AI News": "https://artificialintelligence-news.com/feed/",
    "Unite AI": "https://www.unite.ai/feed/"
}

# ê° í”¼ë“œì—ì„œ ê°€ì ¸ì˜¬ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
MAX_ARTICLES_PER_FEED = 5


class RSSCollector:
    def __init__(self):
        self.collected_news: List[Dict] = []
        self.stats = {'created': 0, 'skipped': 0, 'failed': 0}

    def parse_feed(self, source_name: str, url: str):
        """ë‹¨ì¼ RSS í”¼ë“œ íŒŒì‹±"""
        print(f"[*] {source_name} ìˆ˜ì§‘ ì¤‘...")
        try:
            feed = feedparser.parse(url)
            
            if not feed.entries:
                print(f"    âš ï¸ í”¼ë“œì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            for entry in feed.entries[:MAX_ARTICLES_PER_FEED]:
                # ë‚ ì§œ íŒŒì‹±
                published_at = self._standardize_date(entry.get("published", ""))
                if not published_at:
                    published_at = datetime.now().isoformat()

                # ë³¸ë¬¸ ì •ë¦¬
                summary_text = self._clean_html(entry.get("summary", ""))
                if not summary_text:
                    summary_text = self._clean_html(entry.get("description", ""))
                
                # [AI ë²ˆì—­ ì¤€ë¹„ì¤‘] íƒœê·¸ ì¶”ê°€ (NEXT_MISSION.md ìš”êµ¬ì‚¬í•­)
                content_with_tag = f"[AI ë²ˆì—­ ì¤€ë¹„ì¤‘]\n\n{summary_text}"
                
                # ì¸ë„¤ì¼ ì¶”ì¶œ ì‹œë„
                thumbnail_url = self._extract_thumbnail(entry)

                news_item = {
                    "source": source_name,
                    "title": entry.title,
                    "link": entry.link,
                    "published_at": published_at,
                    "content": content_with_tag,
                    "summary": summary_text,
                    "thumbnail_url": thumbnail_url,
                    "category": "AI",  # NEXT_MISSION.md ìŠ¤íŽ™
                }
                self.collected_news.append(news_item)
                print(f"    ðŸ“° {news_item['title'][:50]}...")
                
        except Exception as e:
            print(f"    âŒ ì˜¤ë¥˜: {e}")

    def _standardize_date(self, date_str: str) -> str:
        """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ ISO 8601ë¡œ ë³€í™˜"""
        if not date_str:
            return None
        try:
            dt = parser.parse(date_str)
            return dt.isoformat()
        except Exception as e:
            print(f"    âš ï¸ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜ '{date_str}': {e}")
            return None

    def _clean_html(self, html_content: str) -> str:
        """HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if not html_content:
            return ""
        try:
            soup = BeautifulSoup(html_content, "html.parser")
            text = soup.get_text(strip=True)
            # 5000ìž ì œí•œ
            return text[:5000]
        except Exception:
            return html_content[:5000]

    def _extract_thumbnail(self, entry) -> str:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ ì¸ë„¤ì¼ URL ì¶”ì¶œ"""
        # media:thumbnail íƒœê·¸
        if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
            return entry.media_thumbnail[0].get('url', '')
        
        # media:content íƒœê·¸
        if hasattr(entry, 'media_content') and entry.media_content:
            return entry.media_content[0].get('url', '')
        
        # enclosure íƒœê·¸
        if hasattr(entry, 'enclosures') and entry.enclosures:
            for enc in entry.enclosures:
                if enc.get('type', '').startswith('image'):
                    return enc.get('href', enc.get('url', ''))
        
        # summary/contentì—ì„œ img íƒœê·¸ ì¶”ì¶œ
        content = entry.get('summary', '') or entry.get('description', '')
        if content:
            soup = BeautifulSoup(content, 'html.parser')
            img = soup.find('img')
            if img and img.get('src'):
                return img['src']
        
        return None

    def run(self, mode: str = 'api'):
        """
        RSS ìˆ˜ì§‘ ë° ì „ì†¡ ì‹¤í–‰
        
        Args:
            mode: 'api' - APIë¡œ ì „ì†¡ (ê¸°ë³¸ê°’)
                  'json' - JSON íŒŒì¼ë¡œ ì €ìž¥
                  'both' - ë‘˜ ë‹¤ ì‹¤í–‰
        """
        print("=" * 50)
        print("ðŸŒ ê¸€ë¡œë²Œ AI ë‰´ìŠ¤ RSS ìˆ˜ì§‘ê¸° ì‹œìž‘")
        print("=" * 50)
        
        # ëª¨ë“  í”¼ë“œ ìˆ˜ì§‘
        for name, url in RSS_FEEDS.items():
            self.parse_feed(name, url)
        
        if not self.collected_news:
            print("\nâš ï¸ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nâœ… ì´ {len(self.collected_news)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ëª¨ë“œë³„ ì²˜ë¦¬
        if mode in ('json', 'both'):
            self._save_to_json()
        
        if mode in ('api', 'both'):
            self._send_to_api()
        
        print("\n" + "=" * 50)
        print("ðŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        print(f"   - ì´ ìˆ˜ì§‘: {len(self.collected_news)}ê±´")
        if mode in ('api', 'both'):
            print(f"   - ì‹ ê·œ ì €ìž¥: {self.stats['created']}ê±´")
            print(f"   - ì¤‘ë³µ ìŠ¤í‚µ: {self.stats['skipped']}ê±´")
            print(f"   - ì‹¤íŒ¨: {self.stats['failed']}ê±´")
        print("=" * 50)

    def _send_to_api(self):
        """ìˆ˜ì§‘ëœ ê¸°ì‚¬ë¥¼ APIë¡œ ì „ì†¡"""
        print(f"\nðŸŒ API ì „ì†¡ ì‹œìž‘ ({len(self.collected_news)}ê°œ)...\n")
        
        for news in self.collected_news:
            payload = {
                'title': news['title'],
                'content': news['content'],
                'original_link': news['link'],  # ì¤‘ë³µ ë°©ì§€ í‚¤
                'published_at': news['published_at'],
                'source': news['source'],  # "TechCrunch", "Wired AI" ë“±
                'category': news['category'],  # "AI"
                'thumbnail_url': news.get('thumbnail_url'),
            }
            
            result = send_article_to_server(payload)
            
            if result['status'] == 'created':
                self.stats['created'] += 1
            elif result['status'] == 'exists':
                self.stats['skipped'] += 1
            else:
                self.stats['failed'] += 1

    def _save_to_json(self):
        """JSON íŒŒì¼ë¡œ ì €ìž¥"""
        output_file = "ai_news_raw.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(self.collected_news, f, ensure_ascii=False, indent=4)
        print(f"ðŸ’¾ JSON ì €ìž¥ ì™„ë£Œ: {output_file}")


if __name__ == "__main__":
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ìžë¡œ ëª¨ë“œ ì§€ì • ê°€ëŠ¥
    # python rss_collector.py api|json|both
    mode = sys.argv[1] if len(sys.argv) > 1 else 'api'
    
    collector = RSSCollector()
    collector.run(mode=mode)

