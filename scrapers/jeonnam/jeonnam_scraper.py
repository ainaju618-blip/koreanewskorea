# -*- coding: utf-8 -*-
"""ì „ë¼ë‚¨ë„ì²­ ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼ v3.1 (Cloudinary Integration)
- Collect & Visit íŒ¨í„´ ì ìš©
- Strict Verification ë¡œì§ ì¶”ê°€
- Cloudinary ì´ë¯¸ì§€ ì—…ë¡œë“œ í†µí•©
- ìµœì¢…ìˆ˜ì •: 2025-12-11
"""

import sys
import os
import time
import re
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
from urllib.parse import urljoin

from playwright.sync_api import sync_playwright, Page

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.api_client import send_article_to_server, log_to_server
from utils.scraper_utils import safe_goto, wait_and_find, safe_get_text, safe_get_attr
from utils.cloudinary_uploader import download_and_upload_image

REGION_CODE = 'jeonnam'
REGION_NAME = 'ì „ë¼ë‚¨ë„'
CATEGORY_NAME = 'ì „ë‚¨'
BASE_URL = 'https://www.jeonnam.go.kr'
LIST_URL = 'https://www.jeonnam.go.kr/M7116/boardList.do?menuId=jeonnam0202000000'

LIST_SELECTORS = ['tbody tr']
LINK_SELECTORS = ['td.title a', 'td a']
CONTENT_SELECTORS = ['div.bbs_view_contnet', 'div.preview_area', 'div.bbs_view', 'div.contents']


def normalize_date(date_str: str) -> str:
    """ë‚ ì§œ ë¬¸ìì—´ì„ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
    if not date_str:
        return datetime.now().strftime('%Y-%m-%d')
    date_str = date_str.strip().replace('.', '-').replace('/', '-')
    try:
        match = re.search(r'(\d{4}-\d{1,2}-\d{1,2})', date_str)
        if match:
            return match.group(1)
    except:
        pass
    return datetime.now().strftime('%Y-%m-%d')


def validate_article(article_data: Dict) -> Tuple[bool, str]:
    """ì—„ê²©í•œ ë°ì´í„° ê²€ì¦ ë¡œì§"""
    # 1. ì œëª© ê²€ì¦
    if not article_data.get('title') or len(article_data['title']) < 5:
        return False, "âŒ [ê²€ì¦ ì‹¤íŒ¨] ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì—†ìŠµë‹ˆë‹¤."
    
    # 2. ë³¸ë¬¸ ê²€ì¦
    content = article_data.get('content', '')
    if not content or len(content) < 50:
        return False, f"âŒ [ê²€ì¦ ì‹¤íŒ¨] ë³¸ë¬¸ ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. (ê¸¸ì´: {len(content)})"
    if "ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in content:
        return False, "âŒ [ê²€ì¦ ì‹¤íŒ¨] ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."

    # 3. ì´ë¯¸ì§€ URL ê²€ì¦ (ì„ íƒì ì´ì§€ë§Œ, ìˆìœ¼ë©´ ìœ íš¨í•´ì•¼ í•¨)
    img_url = article_data.get('thumbnail_url')
    if img_url and not img_url.startswith('http'):
        return False, f"âŒ [ê²€ì¦ ì‹¤íŒ¨] ì´ë¯¸ì§€ URLì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {img_url}"
    
    return True, "âœ… [ê²€ì¦ í†µê³¼]"


def fetch_detail(page: Page, url: str) -> Tuple[str, Optional[str], Optional[str]]:
    """ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸/ì´ë¯¸ì§€/ë‚ ì§œ ì¶”ì¶œ"""
    if not safe_goto(page, url, timeout=20000):
        print(f"   âš ï¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {url}")
        return "", None, None

    # 1. ë³¸ë¬¸ ì¶”ì¶œ
    content = ""
    for sel in CONTENT_SELECTORS:
        content_elem = page.locator(sel)
        if content_elem.count() > 0:
            text = safe_get_text(content_elem)
            if text and len(text) > 50:
                content = text[:5000]
                break
    
    # 2. ì´ë¯¸ì§€ ì¶”ì¶œ - ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ì—ì„œ
    thumbnail_url = None
    original_image_url = None
    try:
        download_links = page.locator('a[href*="boardDown.do"]')
        for i in range(download_links.count()):
            link = download_links.nth(i)
            title = safe_get_attr(link, 'title') or ""
            href = safe_get_attr(link, 'href') or ""
            
            if any(ext in title.lower() for ext in ['.jpg', '.png', '.gif', '.jpeg']):
                original_image_url = urljoin(BASE_URL, href)
                print(f"      ğŸ“ ì²¨ë¶€íŒŒì¼ ì´ë¯¸ì§€ ë°œê²¬: {title}")
                break
    except Exception as e:
        print(f"   âš ï¸ ì²¨ë¶€íŒŒì¼ ì´ë¯¸ì§€ ì¶”ì¶œ ì—ëŸ¬: {str(e)}")
    
    # ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ fallback
    if not original_image_url:
        try:
            for sel in CONTENT_SELECTORS:
                imgs = page.locator(f'{sel} img')
                if imgs.count() > 0:
                    src = safe_get_attr(imgs.first, 'src')
                    if src and 'icon' not in src.lower() and 'button' not in src.lower():
                        original_image_url = urljoin(BASE_URL, src)
                        print(f"      ğŸ–¼ï¸ ë³¸ë¬¸ ì´ë¯¸ì§€ fallback: {src[:50]}...")
                        break
        except Exception as e:
            print(f"   âš ï¸ ë³¸ë¬¸ ì´ë¯¸ì§€ ì¶”ì¶œ ì—ëŸ¬: {str(e)}")

    # 3. Cloudinary ì—…ë¡œë“œ (ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´)
    if original_image_url:
        try:
            cloudinary_url = download_and_upload_image(original_image_url, BASE_URL, folder="jeonnam")
            if cloudinary_url and cloudinary_url.startswith('https://res.cloudinary.com'):
                thumbnail_url = cloudinary_url
                print(f"      â˜ï¸ Cloudinary ì—…ë¡œë“œ ì™„ë£Œ")
            else:
                thumbnail_url = original_image_url  # Fallback to original
                print(f"      âš ï¸ Cloudinary ì—…ë¡œë“œ ì‹¤íŒ¨, ì›ë³¸ URL ì‚¬ìš©")
        except Exception as e:
            thumbnail_url = original_image_url  # Fallback to original
            print(f"      âš ï¸ Cloudinary ì—…ë¡œë“œ ì—ëŸ¬: {str(e)[:50]}")

    # 4. ë‚ ì§œ ì¶”ì¶œ
    pub_date = None
    try:
        date_elem = page.locator('span:has-text("ë“±ë¡ì¼"), li:has-text("ë“±ë¡ì¼"), td.date')
        if date_elem.count() > 0:
            date_text = safe_get_text(date_elem.first)
            pub_date = normalize_date(date_text)
    except:
        pass

    return content, thumbnail_url, pub_date


def collect_articles(days: int = 3) -> List[Dict]:
    print(f"ğŸ›ï¸ {REGION_NAME} ë³´ë„ìë£Œ ìˆ˜ì§‘ ì‹œì‘ (Strict Verification Mode)")
    log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f'{REGION_NAME} ìŠ¤í¬ë˜í¼ v3.0 ì‹œì‘', 'info')
    
    collected_links = []
    
    # ============================================
    # Phase 1: Collect Phase - ë§í¬ ìˆ˜ì§‘
    # ============================================
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            viewport={'width': 1280, 'height': 1024}
        )
        page = context.new_page()
        
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        
        # 1~3í˜ì´ì§€ ìˆœíšŒí•˜ë©° ë§í¬ ìˆ˜ì§‘
        for page_num in range(1, 4):
            list_url = f'{LIST_URL}&pageIndex={page_num}'
            print(f"   ğŸ“„ ëª©ë¡ í˜ì´ì§€ {page_num} ìŠ¤ìº” ì¤‘...")
            
            if not safe_goto(page, list_url):
                print(f"   âš ï¸ í˜ì´ì§€ {page_num} ì ‘ì† ì‹¤íŒ¨, ê±´ë„ˆëœ€")
                continue
            
            rows = wait_and_find(page, LIST_SELECTORS, timeout=10000)
            if not rows:
                print("   âš ï¸ ê¸°ì‚¬ ëª©ë¡ì„ ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                continue
            
            count = rows.count()
            print(f"      ğŸ”— {count}ê°œ í–‰ ë°œê²¬")
            
            stop_collecting = False
            for i in range(count):
                try:
                    row = rows.nth(i)
                    link_elem = wait_and_find(row, LINK_SELECTORS, timeout=3000)
                    if not link_elem:
                        continue
                    
                    title = safe_get_text(link_elem)
                    href = safe_get_attr(link_elem, 'href')
                    full_url = urljoin(BASE_URL, href) if href else ""
                    
                    # ë‚ ì§œ í•„í„°ë§
                    date_elem = row.locator('td.date')
                    n_date = normalize_date(safe_get_text(date_elem))
                    
                    if n_date < start_date:
                        stop_collecting = True
                        break
                    if n_date > end_date:
                        continue
                    
                    if title and full_url and 'boardView' in full_url:
                        collected_links.append({'title': title, 'url': full_url, 'date': n_date})
                except Exception as e:
                    print(f"      âš ï¸ ë§í¬ íŒŒì‹± ì—ëŸ¬: {str(e)}")
            
            if stop_collecting:
                print("      ğŸ›‘ ìˆ˜ì§‘ ê¸°ê°„ ì´ˆê³¼, ë§í¬ ìˆ˜ì§‘ ì¢…ë£Œ")
                break
            
            time.sleep(0.5)
        
        print(f"âœ… ì´ {len(collected_links)}ê°œì˜ ìˆ˜ì§‘ ëŒ€ìƒ ë§í¬ í™•ë³´ ì™„ë£Œ.")
        
        # ============================================
        # Phase 2: Visit Phase - ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸
        # ============================================
        success_count = 0
        processed_count = 0
        
        # ì•ˆì •í™”ë¥¼ ìœ„í•´ ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ì²˜ë¦¬
        target_links = collected_links[:10]
        
        for item in target_links:
            url = item['url']
            title = item['title']
            list_date = item['date']
            
            print(f"   ğŸ” [{processed_count+1}] ë¶„ì„ ì¤‘: {title[:30]}...")
            
            content, thumbnail_url, pub_date = fetch_detail(page, url)
            
            # ë‚ ì§œ ê²°ì • (ìƒì„¸ í˜ì´ì§€ > ëª©ë¡ í˜ì´ì§€)
            final_date = pub_date if pub_date else list_date
            
            # ë°ì´í„° ê°ì²´ ìƒì„±
            article_data = {
                'title': title,
                'content': content,
                'published_at': f"{final_date}T09:00:00+09:00",
                'original_link': url,
                'source': REGION_NAME,
                'category': CATEGORY_NAME,
                'region': REGION_CODE,
                'thumbnail_url': thumbnail_url,
            }
            
            # ============================================
            # Phase 3: Verification Phase - ì—„ê²©í•œ ê²€ì¦
            # ============================================
            is_valid, msg = validate_article(article_data)
            print(f"      {msg}")
            
            if is_valid:
                # Phase 4: Ingestion - DB ì ì¬
                result = send_article_to_server(article_data)
                if result and result.get('status') == 'created':
                    print(f"      âœ… [DB ì €ì¥ ì™„ë£Œ] ID: {result.get('id', 'Unknown')}")
                    success_count += 1
                    log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f"ì„±ê³µ: {title[:10]}...", 'success')
                else:
                    print(f"      âš ï¸ [DB ì €ì¥ ì‹¤íŒ¨] API ì‘ë‹µ: {result}")
            
            processed_count += 1
            time.sleep(1)  # ë¶€í•˜ ì¡°ì ˆ
        
        browser.close()
    
    final_msg = f"ì‘ì—… ì¢…ë£Œ: ì´ {processed_count}ê±´ ì²˜ë¦¬ / {success_count}ê±´ ì €ì¥ ì„±ê³µ"
    print(f"ğŸ‰ {final_msg}")
    log_to_server(REGION_CODE, 'ì„±ê³µ', final_msg, 'success')
    return []


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--days', type=int, default=3)
    parser.add_argument('--dry-run', action='store_true')
    args = parser.parse_args()
    collect_articles(days=args.days)


if __name__ == "__main__":
    main()
