"""
{ì§€ì—­ëª…} ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼ í…œí”Œë¦¿
- ë²„ì „: v2.0
- ìµœì¢…ìˆ˜ì •: 2025-12-10
- ë‹´ë‹¹: AI Agent

âš ï¸ ì‚¬ìš©ë²•:
1. ì´ íŒŒì¼ì„ ë³µì‚¬í•˜ì—¬ {region}_scraper.pyë¡œ ì €ì¥
2. TODO ì£¼ì„ì„ ì°¾ì•„ í•´ë‹¹ ì§€ì—­ì— ë§ê²Œ ìˆ˜ì •
3. í…ŒìŠ¤íŠ¸: python {region}_scraper.py --days 1
"""

# ============================================================
# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ============================================================
import sys
import os
import time
import re
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
from urllib.parse import urljoin

# ============================================================
# 2. ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ============================================================
from playwright.sync_api import sync_playwright, Page

# ============================================================
# 3. ë¡œì»¬ ëª¨ë“ˆ
# ============================================================
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.api_client import send_article_to_server, log_to_server
from utils.scraper_utils import (
    safe_goto, wait_and_find, safe_get_text, safe_get_attr, log_scraper_result
)

# ============================================================
# 4. ìƒìˆ˜ ì •ì˜ (TODO: ì§€ì—­ì— ë§ê²Œ ìˆ˜ì •)
# ============================================================
REGION_CODE = 'template'                           # TODO: ì˜ë¬¸ ì½”ë“œ (ì˜ˆ: gwangju, naju)
REGION_NAME = 'í…œí”Œë¦¿ì‹œ'                            # TODO: í•œê¸€ ì§€ì—­ëª…
CATEGORY_NAME = 'ì „ë‚¨'                              # TODO: ì¹´í…Œê³ ë¦¬ (ê´‘ì£¼/ì „ë‚¨ ë“±)
BASE_URL = 'https://www.example.go.kr'             # TODO: ê¸°ë³¸ URL
LIST_URL = 'https://www.example.go.kr/news/press'  # TODO: ë³´ë„ìë£Œ ëª©ë¡ URL

# ì…€ë ‰í„° ì •ì˜ (TODO: ì‹¤ì œ ì‚¬ì´íŠ¸ DOM êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
LIST_SELECTORS = [
    'tbody tr',              # í…Œì´ë¸” êµ¬ì¡°
    '.board_list tr',        # ëŒ€ì²´ ì…€ë ‰í„° 1
    'ul.list li',            # ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°
]

CONTENT_SELECTORS = [
    'div.view_content',      # ë³¸ë¬¸ ì˜ì—­
    'div.board_view',        # ëŒ€ì²´ ì…€ë ‰í„° 1
    'div.bbs_view',          # ëŒ€ì²´ ì…€ë ‰í„° 2
]


# ============================================================
# 5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def normalize_date(date_str: str) -> str:
    """ë‚ ì§œ ë¬¸ìì—´ì„ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
    if not date_str:
        return datetime.now().strftime('%Y-%m-%d')
    
    date_str = date_str.strip().replace('.', '-').replace('/', '-')
    try:
        match = re.search(r'(\d{4}-\d{1,2}-\d{1,2})', date_str)
        if match:
            return match.group(1)
    except:
        pass
    return datetime.now().strftime('%Y-%m-%d')


# ============================================================
# 6. ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ í•¨ìˆ˜
# ============================================================
def fetch_detail(page: Page, url: str) -> Tuple[str, Optional[str]]:
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ê³¼ ì´ë¯¸ì§€ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        page: Playwright Page ê°ì²´
        url: ìƒì„¸ í˜ì´ì§€ URL
    
    Returns:
        (ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì¸ë„¤ì¼ URL) íŠœí”Œ
    """
    if not safe_goto(page, url, timeout=20000):
        return "", None
    
    # 1. ë³¸ë¬¸ ì¶”ì¶œ - ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„
    content = ""
    for sel in CONTENT_SELECTORS:
        content_elem = page.locator(sel)
        if content_elem.count() > 0:
            text = safe_get_text(content_elem)
            if text and len(text) > 50:
                content = text[:5000]
                break
    
    # 2. ì´ë¯¸ì§€ ì¶”ì¶œ - ì²¨ë¶€íŒŒì¼ ìš°ì„ 
    thumbnail_url = None
    
    # ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ì—ì„œ ì´ë¯¸ì§€ ì°¾ê¸°
    download_links = page.locator('a[href*="download"], a[href*="fileDown"], a[href*="boardDown"]')
    for i in range(download_links.count()):
        link = download_links.nth(i)
        title = safe_get_attr(link, 'title') or ""
        href = safe_get_attr(link, 'href') or ""
        
        if any(ext in title.lower() for ext in ['.jpg', '.png', '.gif', '.jpeg']):
            thumbnail_url = urljoin(BASE_URL, href)
            break
    
    # ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ fallback
    if not thumbnail_url:
        for sel in CONTENT_SELECTORS:
            imgs = page.locator(f'{sel} img')
            if imgs.count() > 0:
                src = safe_get_attr(imgs.first, 'src')
                if src and not any(x in src.lower() for x in ['icon', 'btn', 'logo', 'banner']):
                    thumbnail_url = urljoin(BASE_URL, src)
                    break
    
    return content, thumbnail_url


# ============================================================
# 7. ë©”ì¸ ìˆ˜ì§‘ í•¨ìˆ˜
# ============================================================
def collect_articles(days: int = 3) -> List[Dict]:
    """
    ë³´ë„ìë£Œë¥¼ ìˆ˜ì§‘í•˜ê³  ì„œë²„ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
    
    Args:
        days: ìˆ˜ì§‘í•  ê¸°ê°„ (ì¼)
    
    Returns:
        ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ (ì´ë¯¸ ì „ì†¡ë¨)
    """
    print(f"ğŸ›ï¸ {REGION_NAME} ë³´ë„ìë£Œ ìˆ˜ì§‘ ì‹œì‘ (ìµœê·¼ {days}ì¼)")
    log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f'{REGION_NAME} ìŠ¤í¬ë˜í¼ ì‹œì‘', 'info')
    
    end_date = datetime.now().strftime('%Y-%m-%d')
    start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
    
    collected_count = 0
    success_count = 0
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            viewport={'width': 1280, 'height': 1024}
        )
        page = context.new_page()
        
        page_num = 1
        stop = False
        
        while page_num <= 3 and not stop:
            # TODO: í˜ì´ì§€ë„¤ì´ì…˜ URL íŒ¨í„´ í™•ì¸
            list_url = f'{LIST_URL}?page={page_num}'
            print(f"   ğŸ“„ í˜ì´ì§€ {page_num} ìˆ˜ì§‘ ì¤‘...")
            log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f'í˜ì´ì§€ {page_num} íƒìƒ‰', 'info')
            
            if not safe_goto(page, list_url):
                page_num += 1
                continue
            
            # ëª©ë¡ ì•„ì´í…œ ì°¾ê¸°
            rows = wait_and_find(page, LIST_SELECTORS, timeout=10000)
            if not rows:
                print("      âš ï¸ ê¸°ì‚¬ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            count = rows.count()
            print(f"      ğŸ“° {count}ê°œ ê¸°ì‚¬ ë°œê²¬")
            
            # ë§í¬ ì •ë³´ ìˆ˜ì§‘
            link_data = []
            for i in range(count):
                try:
                    row = rows.nth(i)
                    
                    # TODO: ì œëª©/ë§í¬ ì…€ë ‰í„° í™•ì¸
                    link_elem = row.locator('a').first
                    if not link_elem or link_elem.count() == 0:
                        continue
                    
                    title = safe_get_text(link_elem)
                    href = safe_get_attr(link_elem, 'href')
                    full_url = urljoin(BASE_URL, href) if href else ""
                    
                    # TODO: ë‚ ì§œ ì…€ë ‰í„° í™•ì¸
                    date_elem = row.locator('td').nth(3)  # ë‚ ì§œ ì»¬ëŸ¼ ìœ„ì¹˜
                    n_date = normalize_date(safe_get_text(date_elem))
                    
                    # ë‚ ì§œ í•„í„°ë§
                    if n_date < start_date:
                        stop = True
                        break
                    if n_date > end_date:
                        continue
                    
                    if title and full_url:
                        link_data.append({'title': title, 'url': full_url, 'date': n_date})
                except:
                    continue
            
            # ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ ë° ì „ì†¡
            for item in link_data:
                title = item['title']
                full_url = item['url']
                n_date = item['date']
                
                print(f"      ğŸ“° {title[:30]}... ({n_date})")
                log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f"ìˆ˜ì§‘ ì¤‘: {title[:20]}...", 'info')
                
                content, thumbnail_url = fetch_detail(page, full_url)
                if not content:
                    content = f"ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì›ë³¸ ë§í¬: {full_url}"
                
                article_data = {
                    'title': title,
                    'content': content,
                    'published_at': f"{n_date}T09:00:00+09:00",
                    'original_link': full_url,
                    'source': REGION_NAME,
                    'category': CATEGORY_NAME,
                    'region': REGION_CODE,
                    'thumbnail_url': thumbnail_url,
                }
                
                # ì„œë²„ë¡œ ì „ì†¡
                result = send_article_to_server(article_data)
                collected_count += 1
                if result.get('status') == 'created':
                    success_count += 1
                    log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f"ì €ì¥ ì™„ë£Œ: {title[:15]}...", 'success')
                
                # ëª©ë¡ í˜ì´ì§€ë¡œ ë³µê·€
                safe_goto(page, list_url)
            
            page_num += 1
            if stop:
                print("      ğŸ›‘ ìˆ˜ì§‘ ê¸°ê°„ ì´ˆê³¼, ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            time.sleep(1)
        
        browser.close()
    
    final_msg = f"ìˆ˜ì§‘ ì™„ë£Œ (ì´ {collected_count}ê°œ, ì‹ ê·œ {success_count}ê°œ)"
    log_to_server(REGION_CODE, 'ì„±ê³µ', final_msg, 'success')
    print(f"âœ… {final_msg}")
    return []


# ============================================================
# 8. CLI ì§„ì…ì 
# ============================================================
def main():
    import argparse
    parser = argparse.ArgumentParser(description=f'{REGION_NAME} ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼')
    parser.add_argument('--days', type=int, default=3, help='ìˆ˜ì§‘ ê¸°ê°„ (ì¼)')
    parser.add_argument('--dry-run', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì„œë²„ ì „ì†¡ ì•ˆí•¨)')
    args = parser.parse_args()
    
    collect_articles(args.days)


if __name__ == "__main__":
    main()
