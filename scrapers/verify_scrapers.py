"""
ìŠ¤í¬ë˜í¼ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
- ëª¨ë“  *_scraper.py íŒŒì¼ ê²€ì¦
- êµ¬ë¬¸ ê²€ì‚¬, í•„ìˆ˜ í•¨ìˆ˜ ì¡´ì¬ ì—¬ë¶€, ì‹¤í–‰ í…ŒìŠ¤íŠ¸
"""

import os
import sys
import ast
import subprocess
import json
from datetime import datetime
from typing import List, Dict, Tuple

# ê²€ì¦ ê²°ê³¼ ì €ì¥
RESULTS = {
    'timestamp': datetime.now().isoformat(),
    'total': 0,
    'passed': 0,
    'failed': 0,
    'warnings': 0,
    'details': []
}

# í•„ìˆ˜ ìš”ì†Œ ì •ì˜
REQUIRED_FUNCTIONS = ['collect_articles', 'main']
RECOMMENDED_FUNCTIONS = ['fetch_list', 'fetch_detail', 'normalize_date']
REQUIRED_IMPORTS = ['requests', 'BeautifulSoup']

# í’ˆì§ˆ ê¸°ì¤€
QUALITY_CRITERIA = {
    'has_timeout': 'íƒ€ì„ì•„ì›ƒ ì„¤ì • (timeout=)',
    'has_headers': 'User-Agent í—¤ë” ì„¤ì •',
    'has_error_handling': 'ì˜ˆì™¸ ì²˜ë¦¬ (try/except)',
    'has_rate_limit': 'ì†ë„ ì œí•œ (time.sleep)',
    'has_date_filter': 'ë‚ ì§œ ë²”ìœ„ í•„í„°ë§',
    'has_api_send': 'API ì „ì†¡ ë¡œì§',
}


def check_syntax(filepath: str) -> Tuple[bool, str]:
    """êµ¬ë¬¸ ê²€ì‚¬"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            source = f.read()
        ast.parse(source)
        return True, "âœ… êµ¬ë¬¸ ì •ìƒ"
    except SyntaxError as e:
        return False, f"âŒ êµ¬ë¬¸ ì˜¤ë¥˜: {e}"


def check_required_functions(filepath: str) -> Tuple[bool, List[str], List[str]]:
    """í•„ìˆ˜ í•¨ìˆ˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            source = f.read()
        tree = ast.parse(source)
        
        function_names = []
        class_methods = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                function_names.append(node.name)
            elif isinstance(node, ast.ClassDef):
                for item in node.body:
                    if isinstance(item, ast.FunctionDef):
                        class_methods.append(item.name)
        
        all_funcs = set(function_names + class_methods)
        
        missing = []
        for req in REQUIRED_FUNCTIONS:
            # ë‹¤ì–‘í•œ íŒ¨í„´ í—ˆìš©
            if req not in all_funcs:
                # _collect_articles, run, main ë“± ë‹¤ì–‘í•œ íŒ¨í„´ í—ˆìš©
                alternatives = [f'_{req}', f'{req}s', 'run']
                if not any(alt in all_funcs for alt in alternatives):
                    missing.append(req)
        
        found = [f for f in REQUIRED_FUNCTIONS if f in all_funcs or f'_{f}' in all_funcs]
        
        return len(missing) == 0, found, missing
        
    except Exception as e:
        return False, [], [str(e)]


def check_quality_criteria(filepath: str) -> Dict[str, bool]:
    """í’ˆì§ˆ ê¸°ì¤€ ê²€ì‚¬"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        results = {}
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        results['has_timeout'] = 'timeout=' in content or 'timeout =' in content
        
        # User-Agent
        results['has_headers'] = 'User-Agent' in content or 'HEADERS' in content or 'DEFAULT_HEADERS' in content
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        results['has_error_handling'] = 'try:' in content and 'except' in content
        
        # ì†ë„ ì œí•œ
        results['has_rate_limit'] = 'time.sleep' in content
        
        # ë‚ ì§œ í•„í„°ë§
        results['has_date_filter'] = 'start_date' in content or 'end_date' in content or 'days' in content
        
        # API ì „ì†¡ (UniversalScraper ìƒì†ë„ í¬í•¨)
        has_api = 'send_to_api' in content or 'api_client' in content or 'send_article' in content or '_send_to_api' in content
        inherits_universal = 'UniversalScraper' in content  # ìƒì† ì‹œ API ë¡œì§ í¬í•¨
        results['has_api_send'] = has_api or inherits_universal
        
        return results
        
    except Exception as e:
        return {k: False for k in QUALITY_CRITERIA}


def run_dry_test(filepath: str) -> Tuple[bool, str]:
    """Dry-run í…ŒìŠ¤íŠ¸ (ì„ íƒì )"""
    try:
        # êµ¬ë¬¸ ê²€ì‚¬ë§Œ ìˆ˜í–‰ (ì‹¤ì œ ì‹¤í–‰ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
        result = subprocess.run(
            [sys.executable, '-m', 'py_compile', filepath],
            capture_output=True,
            text=True,
            timeout=10
        )
        if result.returncode == 0:
            return True, "âœ… ì»´íŒŒì¼ ì„±ê³µ"
        else:
            return False, f"âŒ ì»´íŒŒì¼ ì‹¤íŒ¨: {result.stderr}"
    except subprocess.TimeoutExpired:
        return False, "âš ï¸ íƒ€ì„ì•„ì›ƒ"
    except Exception as e:
        return False, f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}"


def verify_scraper(filepath: str) -> Dict:
    """ë‹¨ì¼ ìŠ¤í¬ë˜í¼ ê²€ì¦"""
    filename = os.path.basename(filepath)
    result = {
        'file': filename,
        'path': filepath,
        'status': 'unknown',
        'issues': [],
        'warnings': [],
        'quality': {}
    }
    
    # 1. êµ¬ë¬¸ ê²€ì‚¬
    syntax_ok, syntax_msg = check_syntax(filepath)
    if not syntax_ok:
        result['status'] = 'failed'
        result['issues'].append(syntax_msg)
        return result
    
    # 2. í•„ìˆ˜ í•¨ìˆ˜ ê²€ì‚¬
    funcs_ok, found, missing = check_required_functions(filepath)
    if not funcs_ok:
        result['warnings'].append(f"âš ï¸ ëˆ„ë½ëœ í•¨ìˆ˜: {missing}")
    
    # 3. í’ˆì§ˆ ê¸°ì¤€ ê²€ì‚¬
    quality = check_quality_criteria(filepath)
    result['quality'] = quality
    
    for key, has in quality.items():
        if not has:
            result['warnings'].append(f"âš ï¸ {QUALITY_CRITERIA[key]} ì—†ìŒ")
    
    # 4. ì»´íŒŒì¼ í…ŒìŠ¤íŠ¸
    compile_ok, compile_msg = run_dry_test(filepath)
    if not compile_ok:
        result['status'] = 'failed'
        result['issues'].append(compile_msg)
        return result
    
    # ìµœì¢… ìƒíƒœ ê²°ì •
    if result['issues']:
        result['status'] = 'failed'
    elif result['warnings']:
        result['status'] = 'warning'
    else:
        result['status'] = 'passed'
    
    return result


def verify_all_scrapers(directory: str) -> Dict:
    """ëª¨ë“  ìŠ¤í¬ë˜í¼ ê²€ì¦"""
    global RESULTS
    
    print("=" * 60)
    print("ğŸ” ìŠ¤í¬ë˜í¼ ê²€ì¦ ì‹œì‘")
    print("=" * 60)
    
    scrapers = [f for f in os.listdir(directory) if f.endswith('_scraper.py')]
    RESULTS['total'] = len(scrapers)
    
    for scraper in sorted(scrapers):
        filepath = os.path.join(directory, scraper)
        print(f"\nğŸ“„ ê²€ì¦ ì¤‘: {scraper}")
        
        result = verify_scraper(filepath)
        RESULTS['details'].append(result)
        
        if result['status'] == 'passed':
            RESULTS['passed'] += 1
            print(f"   âœ… í†µê³¼")
        elif result['status'] == 'warning':
            RESULTS['warnings'] += 1
            print(f"   âš ï¸ ê²½ê³ : {len(result['warnings'])}ê±´")
        else:
            RESULTS['failed'] += 1
            print(f"   âŒ ì‹¤íŒ¨: {result['issues']}")
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ê²€ì¦ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    print(f"ì´ ìŠ¤í¬ë˜í¼: {RESULTS['total']}")
    print(f"âœ… í†µê³¼: {RESULTS['passed']}")
    print(f"âš ï¸ ê²½ê³ : {RESULTS['warnings']}")
    print(f"âŒ ì‹¤íŒ¨: {RESULTS['failed']}")
    
    return RESULTS


def generate_report(results: Dict, output_path: str):
    """ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±"""
    report = f"""# ğŸ” ìŠ¤í¬ë˜í¼ ê²€ì¦ ë³´ê³ ì„œ

> **ìƒì„± ì‹œê°:** {results['timestamp']}

## ğŸ“Š ìš”ì•½

| í•­ëª© | ìˆ˜ì¹˜ |
|------|------|
| ì´ ìŠ¤í¬ë˜í¼ | {results['total']} |
| âœ… í†µê³¼ | {results['passed']} |
| âš ï¸ ê²½ê³  | {results['warnings']} |
| âŒ ì‹¤íŒ¨ | {results['failed']} |

---

## ğŸ“‹ ìƒì„¸ ê²°ê³¼

"""
    
    # ì‹¤íŒ¨í•œ ìŠ¤í¬ë˜í¼
    failed = [d for d in results['details'] if d['status'] == 'failed']
    if failed:
        report += "### âŒ ì‹¤íŒ¨ (ì¦‰ì‹œ ìˆ˜ì • í•„ìš”)\n\n"
        for f in failed:
            report += f"#### {f['file']}\n"
            for issue in f['issues']:
                report += f"- {issue}\n"
            report += "\n"
    
    # ê²½ê³  ìŠ¤í¬ë˜í¼
    warnings = [d for d in results['details'] if d['status'] == 'warning']
    if warnings:
        report += "### âš ï¸ ê²½ê³  (ê°œì„  ê¶Œì¥)\n\n"
        for w in warnings:
            report += f"#### {w['file']}\n"
            for warn in w['warnings']:
                report += f"- {warn}\n"
            report += "\n"
    
    # í†µê³¼ ìŠ¤í¬ë˜í¼
    passed = [d for d in results['details'] if d['status'] == 'passed']
    if passed:
        report += "### âœ… í†µê³¼\n\n"
        for p in passed:
            report += f"- {p['file']}\n"
    
    # í’ˆì§ˆ í†µê³„
    report += """
---

## ğŸ“ˆ í’ˆì§ˆ ê¸°ì¤€ í†µê³„

| ê¸°ì¤€ | ì¶©ì¡± ìˆ˜ | ë¹„ìœ¨ |
|------|--------|------|
"""
    
    for key, desc in QUALITY_CRITERIA.items():
        count = sum(1 for d in results['details'] if d.get('quality', {}).get(key, False))
        pct = count / results['total'] * 100 if results['total'] > 0 else 0
        report += f"| {desc} | {count}/{results['total']} | {pct:.0f}% |\n"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ“ ë³´ê³ ì„œ ì €ì¥: {output_path}")


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='Scraper Verification Tool')
    parser.add_argument('--dir', default='.', help='Directory containing scrapers')
    parser.add_argument('--report', default='verification_report.md', help='Report output path')
    args = parser.parse_args()
    
    results = verify_all_scrapers(args.dir)
    generate_report(results, args.report)
    
    # JSON ê²°ê³¼ë„ ì €ì¥
    json_path = args.report.replace('.md', '.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“Š JSON ê²°ê³¼: {json_path}")
