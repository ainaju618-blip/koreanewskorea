"""
ë‹´ì–‘êµ°ì²­ ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼
- ë²„ì „: v2.0
- ìµœì¢…ìˆ˜ì •: 2025-12-12
- ë‹´ë‹¹: AI Agent

ë³€ê²½ì  (v2.0):
- cloudinary_uploader â†’ local_image_saver ì „í™˜
- ì´ë¯¸ì§€ ê²½ë¡œ: /images/damyang/{filename} í˜•íƒœë¡œ ë°˜í™˜
"""

import sys
import os
import time
import re
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
from urllib.parse import urljoin, parse_qs, urlparse

from playwright.sync_api import sync_playwright, Page

# ë¡œì»¬ ëª¨ë“ˆ ê²½ë¡œ ì„¤ì •
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.api_client import send_article_to_server, log_to_server
from utils.scraper_utils import (
    safe_goto, wait_and_find, safe_get_text, safe_get_attr, clean_article_content, detect_category
)
from utils.cloudinary_uploader import download_and_upload_image

def normalize_date(date_str: str) -> str:
    """ë‚ ì§œ ë¬¸ìì—´ì„ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
    if not date_str:
        return datetime.now().strftime('%Y-%m-%d')
    
    date_str = date_str.strip().replace('.', '-').replace('/', '-')
    try:
        match = re.search(r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})', date_str)
        if match:
            y, m, d = match.groups()
            return f"{y}-{int(m):02d}-{int(d):02d}"
    except:
        pass
    return datetime.now().strftime('%Y-%m-%d')

# ============================================================
# ìƒìˆ˜ ì •ì˜
# ============================================================
REGION_CODE = 'damyang'
REGION_NAME = 'ë‹´ì–‘êµ°'
CATEGORY_NAME = 'ì „ë‚¨'
BASE_URL = 'https://www.damyang.go.kr'
LIST_URL = 'https://www.damyang.go.kr/board/list?domainId=DOM_0000001&boardId=BBS_0000007&contentsSid=12&menuCd=DOM_000000190001005001'

# ì…€ë ‰í„°
LIST_ROW_SELECTOR = 'table:has(caption:has-text("ë³´ë„ìë£Œ")) tbody tr'
CONTENT_SELECTORS = [
    '.con-wrap',      # ë‹´ì–‘êµ° SPA ë³¸ë¬¸ ì˜ì—­
    'div.view_con',
    'div.board_view', 
    'td.content',
    'div.bbs_view'
]

# ë³¸ë¬¸ì—ì„œ ì œê±°í•  íŒ¨í„´ë“¤ (ë‹´ì–‘êµ° í‘¸í„°)
CONTENT_CUT_PATTERNS = [
    'ì²¨ë¶€íŒŒì¼',
    'ì´ì „ê¸€',
    'ë‹¤ìŒê¸€', 
    'ëª©ë¡',
    'ê³µê³µëˆ„ë¦¬',
    'ë‹´ë‹¹ë¶€ì„œ',
    'ì´ í˜ì´ì§€ì—ì„œ ì œê³µí•˜ëŠ” ì •ë³´ì—',
    'QR CODE',
    'ì˜ê²¬ë‚¨ê¸°ê¸°',
    'ê²°ê³¼ë³´ê¸°'
]

# ë³¸ë¬¸ ìƒë‹¨ì—ì„œ ì œê±°í•  ë©”íƒ€ì •ë³´ íŒ¨í„´
HEADER_PATTERNS = [
    r'ë“±ë¡ì¼\s*\d{4}[-./]\d{1,2}[-./]\d{1,2}',
    r'ì¡°íšŒìˆ˜\s*\d+',
    r'ì‘ì„±ì\s*\S+'
]

def clean_content(text: str, title: str = "") -> Tuple[str, Optional[str]]:
    """
    ë³¸ë¬¸ì—ì„œ ë¶ˆí•„ìš”í•œ ë©”íƒ€ì •ë³´ ë° í‘¸í„° í…ìŠ¤íŠ¸ ì œê±°
    
    Args:
        text: ì›ë³¸ ë³¸ë¬¸ í…ìŠ¤íŠ¸
        title: ê¸°ì‚¬ ì œëª© (ì¤‘ë³µ ì œê±°ìš©)
    
    Returns:
        (ì •ì œëœ ë³¸ë¬¸, ë¶€ì œëª©)
    """
    if not text:
        return "", None
    
    subtitle = None
    
    # 1. í•˜ë‹¨ í‘¸í„° ì œê±° (ê°€ì¥ ë¨¼ì € ë‚˜ì˜¤ëŠ” íŒ¨í„´ ìœ„ì¹˜ì—ì„œ ìë¥´ê¸°)
    cut_position = len(text)
    for pattern in CONTENT_CUT_PATTERNS:
        idx = text.find(pattern)
        if idx != -1 and idx < cut_position:
            cut_position = idx
    
    result = text[:cut_position].strip()
    
    # 2. ìƒë‹¨ ë©”íƒ€ì •ë³´ ì œê±° (ë“±ë¡ì¼, ì¡°íšŒìˆ˜, ì‘ì„±ì)
    for pattern in HEADER_PATTERNS:
        result = re.sub(pattern, '', result)
    
    # 3. ì œëª© ì¤‘ë³µ ì œê±° (ë³¸ë¬¸ì—ì„œ ì œëª©ê³¼ ë™ì¼í•œ í…ìŠ¤íŠ¸ ì œê±°)
    if title:
        title_clean = title.strip()
        # ì œëª©ì´ ì—¬ëŸ¬ ë²ˆ ë“±ì¥í•˜ë©´ ëª¨ë‘ ì œê±°
        result = result.replace(title_clean, '')
    
    # 4. ë¶€ì œëª© ì¶”ì¶œ ("-"ë¡œ ì‹œì‘í•˜ëŠ” ì²« ë²ˆì§¸ ë¼ì¸)
    lines = result.split('\n')
    new_lines = []
    for line in lines:
        line_stripped = line.strip()
        if line_stripped.startswith('- ') and subtitle is None:
            # ë¶€ì œëª© ì¶”ì¶œ (ì²« ë²ˆì§¸ "-" ë¼ì¸ë§Œ)
            subtitle = line_stripped[2:].strip()  # "- " ì œê±°
        else:
            new_lines.append(line)
    
    result = '\n'.join(new_lines)
    
    # 5. ì—°ì† ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
    result = re.sub(r'\n{3,}', '\n\n', result)
    result = re.sub(r' {2,}', ' ', result)
    result = result.strip()
    
    # 6. ìµœëŒ€ ê¸¸ì´ ì œí•œ
    return result[:5000], subtitle



def fetch_detail(page: Page, url: str, title: str = "") -> Tuple[str, Optional[str], str, Optional[str], Optional[str]]:
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸, ì´ë¯¸ì§€, ë‚ ì§œ, ë¶€ì„œ, ë¶€ì œëª© ì¶”ì¶œ
    
    Returns:
        (ë³¸ë¬¸, ì¸ë„¤ì¼URL, ë‚ ì§œ, ë‹´ë‹¹ë¶€ì„œ, ë¶€ì œëª©)
    """
    if not safe_goto(page, url):
        return "", None, datetime.now().strftime('%Y-%m-%d'), None, None

    # ë‹´ì–‘êµ° SPA ì‚¬ì´íŠ¸: ë™ì  ë¡œë”© ëŒ€ê¸°
    try:
        page.wait_for_selector('button.downBtn, .con-wrap', timeout=10000)
    except:
        pass
    time.sleep(1)

    content = ""
    thumbnail_url = None
    pub_date = datetime.now().strftime('%Y-%m-%d')
    department = None
    subtitle = None

    # 1. ë‚ ì§œ ë° ë¶€ì„œ ì¶”ì¶œ
    # ë‹´ì–‘êµ°ì²­ ìƒì„¸: ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë©”íƒ€ë°ì´í„° ì˜ˆìƒ (ì‘ì„±ì, ë“±ë¡ì¼, ì¡°íšŒìˆ˜ ë“±)
    # ì˜ˆ: <ul><li><span class="tit">ë“±ë¡ì¼</span><span class="txt">2025-12-12</span></li>...</ul>
    info_items = page.locator('.view_info li, .board_info li, dl.info dd')
    count = info_items.count()
    for i in range(count):
        text = safe_get_text(info_items.nth(i))
        if 'ë“±ë¡ì¼' in text:
            # "ë“±ë¡ì¼ : 2025-12-12" í˜•íƒœ ì²˜ë¦¬
            date_match = re.search(r'(\d{4}[-.]\d{1,2}[-.]\d{1,2})', text)
            if date_match:
                pub_date = normalize_date(date_match.group(1))
        if 'ë‹´ë‹¹ë¶€ì„œ' in text:
            dept_match = text.replace('ë‹´ë‹¹ë¶€ì„œ', '').replace(':', '').strip()
            if dept_match:
                department = dept_match

    # Fallback: ë³¸ë¬¸ ìƒë‹¨/í•˜ë‹¨ì—ì„œ ì°¾ê¸°
    if department is None:
        dept_elem = page.locator('span:has-text("ë‹´ë‹¹ë¶€ì„œ")')
        if dept_elem.count() > 0:
             department = safe_get_text(dept_elem).replace('ë‹´ë‹¹ë¶€ì„œ', '').strip()

    # 2. ë³¸ë¬¸ ì¶”ì¶œ
    for sel in CONTENT_SELECTORS:
        content_elem = page.locator(sel)
        if content_elem.count() > 0:
            raw_content = safe_get_text(content_elem)
            if raw_content and len(raw_content) > 50:
                # ë³¸ë¬¸ ì •ì œ: ì œëª© ì¤‘ë³µ ì œê±°, ë¶€ì œëª© ì¶”ì¶œ, í‘¸í„° ì œê±°
                content, subtitle = clean_content(raw_content, title)
                # clean_article_content ì ìš© (ì¶”ê°€ ì •ì œ)
                content = clean_article_content(content)
                break
    
    # 3. ì´ë¯¸ì§€ ì¶”ì¶œ - ë‹´ì–‘êµ° íŒ¨í„´: button.downBtn + expect_download
    # ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
    if not thumbnail_url:
        import tempfile
        import shutil
        
        download_btns = page.locator('button.downBtn')
        for i in range(download_btns.count()):
            btn = download_btns.nth(i)
            file_nm = btn.get_attribute('data-file-nm') or ''
            
            # ì´ë¯¸ì§€ íŒŒì¼ì¸ì§€ í™•ì¸
            if any(ext in file_nm.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']):
                print(f"      ğŸ“ ì´ë¯¸ì§€ ì²¨ë¶€íŒŒì¼ ë°œê²¬: {file_nm[:40]}...")
                
                try:
                    # expect_downloadë¡œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                    with page.expect_download(timeout=15000) as download_info:
                        btn.click()
                    download = download_info.value
                    
                    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                    temp_path = os.path.join(tempfile.gettempdir(), download.suggested_filename or f"damyang_{i}.jpg")
                    download.save_as(temp_path)
                    
                    # ë¡œì»¬ ì´ë¯¸ì§€ ì €ì¥ì†Œë¡œ ì´ë™
                    from utils.local_image_saver import ensure_directory, generate_filename
                    folder = ensure_directory(REGION_CODE)
                    filename = generate_filename(REGION_CODE, file_nm)
                    final_path = os.path.join(folder, filename)
                    shutil.copy2(temp_path, final_path)
                    os.remove(temp_path)
                    
                    thumbnail_url = f"/images/{REGION_CODE}/{filename}"
                    print(f"      ğŸ’¾ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {thumbnail_url}")
                    break
                    
                except Exception as e:
                    print(f"      âš ï¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
    
    # ì „ëµ B: ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ (fallback)
    if not thumbnail_url:
        imgs = page.locator('div.view_con img, div.board_view img, .bbs_view img')
        for i in range(imgs.count()):
            src = safe_get_attr(imgs.nth(i), 'src')
            if src and not any(x in src.lower() for x in ['icon', 'button', 'logo', 'blank', 'data:image']):
                img_url = urljoin(BASE_URL, src)
                local_path = download_and_upload_image(img_url, BASE_URL, REGION_CODE)
                if local_path:
                    thumbnail_url = local_path
                    print(f"      ğŸ’¾ ë³¸ë¬¸ ì´ë¯¸ì§€ ì €ì¥: {local_path}")
                    break
                    
    return content, thumbnail_url, pub_date, department, subtitle

def collect_articles(days: int = 3, max_articles: int = 10, start_date: str = None, end_date: str = None):
    print(f"ğŸ›ï¸ {REGION_NAME} ë³´ë„ìë£Œ ìˆ˜ì§‘ ì‹œì‘")
    log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f'{REGION_NAME} ìŠ¤í¬ë˜í¼ ì‹œì‘', 'info')

    if not end_date:
        end_date = datetime.now().strftime('%Y-%m-%d')
    if not start_date:
        start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
    
    collected_count = 0
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        page_num = 1
        stop = False
        
        while page_num <= 5 and not stop and collected_count < max_articles:
            # URL íŒŒë¼ë¯¸í„°ë¡œ í˜ì´ì§€ ì´ë™ ì‹œë„ (`&page=N`)
            # ë§Œì•½ ì´ê²Œ ì•ˆë˜ë©´ JS click ë¡œì§ìœ¼ë¡œ ë³€ê²½í•´ì•¼ í•¨
            curr_url = f"{LIST_URL}&page={page_num}"
            print(f"   ğŸ“„ í˜ì´ì§€ {page_num} íƒìƒ‰: {curr_url}")
            
            if not safe_goto(page, curr_url):
                page_num += 1
                continue
            
            rows = wait_and_find(page, [LIST_ROW_SELECTOR])
            if not rows or rows.count() == 0:
                print("      âš ï¸ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                break
                
            count = rows.count()
            print(f"      ğŸ“° {count}ê°œ ê²Œì‹œë¬¼ ë°œê²¬")
            
            # ëª©ë¡ ë°ì´í„° ìˆ˜ì§‘
            items = []
            for i in range(count):
                row = rows.nth(i)
                try:
                    # ì œëª©/ë§í¬
                    title_link = row.locator('td.subject a, td.title a, a[href*="detail"]').first
                    if title_link.count() == 0:
                        continue
                        
                    title = safe_get_text(title_link)
                    href = safe_get_attr(title_link, 'href')
                    
                    # ë‚ ì§œ
                    date_elem = row.locator('td').nth(3) # ë³´í†µ 4ë²ˆì§¸ê°€ ë‚ ì§œ
                    date_text = safe_get_text(date_elem)
                    n_date = normalize_date(date_text)
                    
                    if n_date < start_date:
                        stop = True
                        break
                    if n_date > end_date:
                        continue

                    # ìƒì„¸ URL êµ¬ì„±
                    full_url = ""
                    if href:
                        if 'javascript' in href:
                            # href="javascript:view('1234')" ê°™ì€ í˜•íƒœì¼ ê²½ìš° ì •ê·œì‹ìœ¼ë¡œ ID ì¶”ì¶œ
                            # ê°€ì´ë“œì— ë”°ë¥´ë©´ href="/board/detail?dataSid=..." í˜•íƒœì¼ ìˆ˜ë„ ìˆìŒ
                            match = re.search(r"dataSid=(\d+)", href)
                            if match:
                                # ê¸°ë³¸ URL íŒŒë¼ë¯¸í„° ì¡°í•©
                                # ìƒì„¸ í˜ì´ì§€ URL íŒ¨í„´: /board/detail?dataSid={ID}&boardId=BBS_0000007&domainId=DOM_0000001&contentsSid=12&menuCd=DOM_000000190001005001
                                sid = match.group(1)
                                full_url = f"{BASE_URL}/board/detail?dataSid={sid}&boardId=BBS_0000007&domainId=DOM_0000001&contentsSid=12&menuCd=DOM_000000190001005001"
                        else:
                            full_url = urljoin(BASE_URL, href)
                    
                    if title and full_url:
                        items.append({
                            'title': title,
                            'url': full_url,
                            'date': n_date
                        })

                except Exception as e:
                    print(f"      âš ï¸ í•­ëª© íŒŒì‹± ì—ëŸ¬: {e}")
                    continue
            
            # ìƒì„¸ ìˆ˜ì§‘
            for item in items:
                if collected_count >= max_articles: 
                    break
                    
                print(f"      Reading: {item['title']} ({item['date']})")
                
                # ì œëª©ì„ ì „ë‹¬í•˜ì—¬ ë³¸ë¬¸ì—ì„œ ì¤‘ë³µ ì œê±°
                content, thumb, final_date, dept, subtitle = fetch_detail(page, item['url'], item['title'])
                
                # ë‚ ì§œ ìš°ì„ ìˆœìœ„: ìƒì„¸ > ëª©ë¡
                pub_at = final_date if final_date else item['date']
                
                # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
                cat_code, cat_name = detect_category(item['title'], content)

                article = {
                    'title': item['title'],
                    'subtitle': subtitle,  # ë¶€ì œëª© ì¶”ê°€
                    'content': content,
                    'published_at': f"{pub_at}T09:00:00+09:00",
                    'original_link': item['url'],
                    'source': REGION_NAME,
                    'category': cat_name,  # ìë™ ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬
                    'region': REGION_CODE,
                    'thumbnail_url': thumb
                }
                
                res = send_article_to_server(article)
                if res.get('status') == 'created':
                    print("         âœ… Saved")
                    collections_msg = "ì´ë¯¸ì§€ í¬í•¨" if thumb else "í…ìŠ¤íŠ¸ë§Œ"
                    log_to_server(REGION_CODE, 'ì„±ê³µ', f"ì €ì¥: {item['title']} ({collections_msg})", 'success')
                    collected_count += 1
                elif res.get('status') == 'exists':
                    print("         â© Skipped (Exists)")
                
                # ëª©ë¡ìœ¼ë¡œ ëŒì•„ê°€ê±°ë‚˜ ê·¸ëƒ¥ URL ì´ë™ (SPAê°€ ì•„ë‹ˆë©´ URL ì´ë™ì´ ë‚˜ìŒ)
                # ì—¬ê¸°ì„œëŠ” ê·¸ëƒ¥ ë‹¤ìŒ ë£¨í”„ì—ì„œ goto í•˜ë¯€ë¡œ back ì•ˆ í•¨
                time.sleep(0.5)

            page_num += 1
        
        browser.close()

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description=f'{REGION_NAME} ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼')
    parser.add_argument('--days', type=int, default=3, help='ìˆ˜ì§‘ ê¸°ê°„ (ì¼)')
    parser.add_argument('--max-articles', type=int, default=10, help='ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜')
    parser.add_argument('--dry-run', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ')
    # bot-service.ts í˜¸í™˜ ì¸ì (í•„ìˆ˜)
    parser.add_argument('--start-date', type=str, default=None, help='ìˆ˜ì§‘ ì‹œì‘ì¼ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, default=None, help='ìˆ˜ì§‘ ì¢…ë£Œì¼ (YYYY-MM-DD)')
    args = parser.parse_args()

    collect_articles(
        days=args.days,
        max_articles=args.max_articles,
        start_date=args.start_date,
        end_date=args.end_date
    )
