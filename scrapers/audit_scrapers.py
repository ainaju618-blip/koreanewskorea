
import sys
import os
import importlib
import inspect
import json
import time
from datetime import datetime
from typing import List, Dict, Any

# Ensure project root is in path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
# Add parent directory to path to allow imports from utils
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def run_audit(target_scrapers: List[str] = None, days: int = 1):
    """
    Audit scrapers by running them in dry-run mode (collect_articles).
    """
    results = {
        'timestamp': datetime.now().isoformat(),
        'scrapers': {}
    }

    base_dir = os.path.dirname(os.path.abspath(__file__))
    
    # Recursively find scrapers
    files_to_run = []
    for root, dirs, files in os.walk(base_dir):
        if 'utils' in root or 'docs' in root or '__pycache__' in root:
            continue
        for f in files:
            if f.endswith('_scraper.py') and f != 'universal_scraper.py' and f != 'audit_scrapers.py':
                # full path
                full_path = os.path.join(root, f)
                # relative path components
                rel_path = os.path.relpath(full_path, base_dir)
                # module path conversion (e.g. naju\naju_scraper.py -> scrapers.naju.naju_scraper)
                module_path = "scrapers." + rel_path.replace(os.path.sep, ".")[:-3]
                
                # Filter if targets provided
                if target_scrapers and not any(t in f for t in target_scrapers):
                    continue
                    
                files_to_run.append((f, module_path))

    print(f"ğŸ” Starting Audit for {len(files_to_run)} scrapers...")

    for filename, module_name in sorted(files_to_run):
        print(f"\nExample: Auditing {filename} ({module_name})...")
        
        try:
            # Dynamic import
            if module_name in sys.modules:
                module = importlib.reload(sys.modules[module_name])
            else:
                module = importlib.import_module(module_name)
            
            # Check for collect_articles
            if hasattr(module, 'collect_articles'):
                func = module.collect_articles
                # Check signature to see if it accepts 'days'
                sig = inspect.signature(func)
                
                start_time = time.time()
                try:
                    # Run scraper
                    print(f"   ğŸƒ Running collect_articles(days={days})...")
                    kwargs = {}
                    if 'days' in sig.parameters:
                        kwargs['days'] = days
                    if 'max_articles' in sig.parameters:
                        kwargs['max_articles'] = 1  # 1ê°œë§Œ ìˆ˜ì§‘í•´ì„œ ë¹ ë¥´ê²Œ ê²€ì¦

                    articles = func(**kwargs)
                    
                    duration = time.time() - start_time
                    
                    # Analyze results
                    article_count = len(articles)
                    images_count = sum(1 for a in articles if a.get('thumbnail_url'))
                    
                    # Detailed checks
                    issues = []
                    if article_count == 0:
                        issues.append("No articles found")
                    
                    # Check first article for mandatory fields
                    if article_count > 0:
                        sample = articles[0]
                        required_fields = ['title', 'content', 'published_at', 'original_link']
                        missing_fields = [f for f in required_fields if not sample.get(f)]
                        if missing_fields:
                            issues.append(f"Missing fields: {missing_fields}")
                        
                        if not sample.get('thumbnail_url'):
                            issues.append("Thumbnail missing in sample")
                        elif 'http' not in sample['thumbnail_url']:
                             issues.append(f"Invalid thumbnail URL: {sample['thumbnail_url']}")

                    status = 'passed' if not issues and article_count > 0 else 'failed'
                    if article_count > 0 and images_count == 0:
                         status = 'warning' # Articles found but no images (might be intended, but suspicious)
                         issues.append("No images found in any article")

                    results['scrapers'][filename] = {
                        'status': status,
                        'duration': round(duration, 2),
                        'articles_found': article_count,
                        'images_found': images_count,
                        'issues': issues,
                        'sample': articles[:3] if articles else []
                    }
                    print(f"   {'âœ…' if status == 'passed' else 'âš ï¸' if status == 'warning' else 'âŒ'} Result: {status} ({article_count} articles, {images_count} images)")

                except Exception as e:
                    print(f"   âŒ Execution Error: {e}")
                    results['scrapers'][filename] = {
                        'status': 'error',
                        'error': str(e)
                    }
            else:
                print(f"   âŒ collect_articles not found")
                results['scrapers'][filename] = {
                    'status': 'error',
                    'error': 'collect_articles function missing'
                }

        except Exception as e:
            print(f"   âŒ Import Error: {e}")
            results['scrapers'][filename] = {
                'status': 'error',
                'error': f"Import failed: {str(e)}"
            }


    # Save report
    with open(os.path.join(base_dir, 'audit_report.json'), 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # Generate Markdown Summary
    md_report = f"# ìŠ¤í¬ë˜í¼ ì „ìˆ˜ ê°ì‚¬ ë³´ê³ ì„œ\n\nê²€ì‚¬ ì¼ì‹œ: {results['timestamp']}\n\n"
    md_report += "| ìŠ¤í¬ë˜í¼ | ìƒíƒœ | ê¸°ì‚¬ ìˆ˜ | ì´ë¯¸ì§€ ìˆ˜ | ì†Œìš”ì‹œê°„ (ì´ˆ) | íŠ¹ì´ì‚¬í•­ |\n"
    md_report += "|---------|--------|----------|--------|--------------|--------|\n"
    
    for name, res in results['scrapers'].items():
        status_map = {'passed': 'âœ… ì •ìƒ', 'warning': 'âš ï¸ ê²½ê³ ', 'failed': 'âŒ ì‹¤íŒ¨', 'error': 'ğŸ’¥ ì˜¤ë¥˜'}
        status_icon = status_map.get(res['status'], 'â“ ì•Œìˆ˜ì—†ìŒ')
        issues = "<br>".join(res.get('issues', [])) or res.get('error', '-')
        
        # Translate common issues if possible
        issues = issues.replace("No articles found", "ê¸°ì‚¬ ë¯¸ë°œê²¬")
        issues = issues.replace("No images found in any article", "ì´ë¯¸ì§€ ë¯¸ë°œê²¬ (ì „ì²´)")
        issues = issues.replace("Missing fields", "í•„ìˆ˜ í•„ë“œ ëˆ„ë½")
        issues = issues.replace("Thumbnail missing in sample", "ì¸ë„¤ì¼ ëˆ„ë½ (ìƒ˜í”Œ)")
        
        md_report += f"| {name} | {status_icon} | {res.get('articles_found', '-')} | {res.get('images_found', '-')} | {res.get('duration', '-')} | {issues} |\n"

    # Add Details Section
    md_report += "\n## ìƒì„¸ ìƒ˜í”Œ (ìƒìœ„ 3ê±´)\n"
    for name, res in results['scrapers'].items():
        if res.get('sample'):
            md_report += f"\n### {name}\n"
            for idx, article in enumerate(res['sample']):
                thumb = "âœ… ìˆìŒ" if article.get('thumbnail_url') else "âŒ ì—†ìŒ"
                md_report += f"- **[{idx+1}] {article.get('title', 'ì œëª© ì—†ìŒ')}**\n"
                md_report += f"  - ë‚ ì§œ: {article.get('published_at')}\n"
                md_report += f"  - ì´ë¯¸ì§€: {thumb} ({article.get('thumbnail_url', 'N/A')})\n"
                md_report += f"  - ë§í¬: {article.get('original_link')}\n"

    with open(os.path.join(base_dir, 'audit_report.md'), 'w', encoding='utf-8') as f:
        f.write(md_report)
    
    print(f"\nğŸ“„ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {os.path.join(base_dir, 'audit_report.md')}")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--targets', nargs='+', help='Specific scrapers to audit (partial name match)')
    parser.add_argument('--days', type=int, default=1, help='Days to scrape')
    args = parser.parse_args()
    
    run_audit(args.targets, args.days)
