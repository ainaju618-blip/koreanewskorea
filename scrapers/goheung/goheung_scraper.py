"""
ê³ í¥êµ° ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼
- ë²„ì „: v1.0
- ìµœì¢…ìˆ˜ì •: 2025-12-13
- ë‹´ë‹¹: AI Agent

ë³€ê²½ì  (v1.0):
- ì‚¬ìš©ì ì œê³µ ìƒì„¸ ë¶„ì„ ë°ì´í„° ê¸°ë°˜ ìµœì´ˆ ì‘ì„±
- URL íŒ¨í„´: boardView.do?pageId=www102&boardId=BD_00025&seq={ID}&movePage=1
- ì²¨ë¶€íŒŒì¼: /fileDownload.do?action=fileDown&mode=&boardId=BD_00025&seq={ID}&fileSn={ìˆœë²ˆ}
- ì •ì  HTML ì„œë²„ ë Œë”ë§ ë°©ì‹ (JavaScript ë™ì  ë¡œë”© ì—†ìŒ)
"""

# ============================================================
# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ============================================================
import sys
import os
import time
import re
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
from urllib.parse import urljoin, parse_qs, urlparse

# ============================================================
# 2. ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ============================================================
from playwright.sync_api import sync_playwright, Page

# ============================================================
# 3. ë¡œì»¬ ëª¨ë“ˆ
# ============================================================
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.api_client import send_article_to_server, log_to_server
from utils.scraper_utils import safe_goto, wait_and_find, safe_get_text, safe_get_attr
from utils.cloudinary_uploader import download_and_upload_image

# ============================================================
# 4. ìƒìˆ˜ ì •ì˜
# ============================================================
REGION_CODE = 'goheung'
REGION_NAME = 'ê³ í¥êµ°'
CATEGORY_NAME = 'ì „ë‚¨'
BASE_URL = 'https://www.goheung.go.kr'

# ëª©ë¡ í˜ì´ì§€ URL (ë³´ë„ìë£Œ)
# ê³ í¥êµ°ì€ boardList.do ëŒ€ì‹  í˜ì´ì§€ì—ì„œ ì§ì ‘ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
LIST_PATH = '/boardList.do?pageId=www102&boardId=BD_00025'
LIST_URL = f'{BASE_URL}{LIST_PATH}'

# ìƒì„¸ í˜ì´ì§€ URL íŒ¨í„´: /boardView.do?pageId=www102&boardId=BD_00025&seq={ID}&movePage=1

# ì²¨ë¶€íŒŒì¼ URL íŒ¨í„´: /fileDownload.do?action=fileDown&mode=&boardId=BD_00025&seq={ID}&fileSn={ìˆœë²ˆ}

# ëª©ë¡ í˜ì´ì§€ ì…€ë ‰í„° (ul > li êµ¬ì¡°)
LIST_ITEM_SELECTORS = [
    'a[href*="boardView.do"][href*="seq="]',    # ìƒì„¸ í˜ì´ì§€ ë§í¬
    'ul li a[href*="boardView"]',
    'table tbody tr a[href*="seq="]',
]

# ìƒì„¸ í˜ì´ì§€/ë³¸ë¬¸ ì…€ë ‰í„° (ìš°ì„ ìˆœìœ„ ìˆœ)
CONTENT_SELECTORS = [
    '.board_view_content',
    '.view_content',
    '.board_view',
    '.con-wrap',
    '.view-con',
    '.bbs_view_cont',
    'article',
    'div[class*="view"]',
]

# ë‚ ì§œ íŒ¨í„´
DATE_PATTERNS = [
    r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})',  # YYYY-MM-DD or YYYY.MM.DD
]


# ============================================================
# 5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def normalize_date(date_str: str) -> str:
    """ë‚ ì§œ ë¬¸ìì—´ì„ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
    if not date_str:
        return datetime.now().strftime('%Y-%m-%d')
    
    date_str = date_str.strip().replace('.', '-').replace('/', '-')
    try:
        match = re.search(r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})', date_str)
        if match:
            y, m, d = match.groups()
            return f"{y}-{int(m):02d}-{int(d):02d}"
    except:
        pass
    return datetime.now().strftime('%Y-%m-%d')


def extract_seq(href: str) -> Optional[str]:
    """hrefì—ì„œ seq(ê²Œì‹œê¸€ ID) ì¶”ì¶œ"""
    if not href:
        return None
    
    # URL íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ
    try:
        parsed = urlparse(href)
        params = parse_qs(parsed.query)
        if 'seq' in params:
            return params['seq'][0]
    except:
        pass
    
    # ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ
    match = re.search(r'seq[=]?(\d+)', href)
    if match:
        return match.group(1)
    
    return None


def build_detail_url(seq: str) -> str:
    """ê²Œì‹œê¸€ ID(seq)ë¡œ ìƒì„¸ í˜ì´ì§€ URL ìƒì„±"""
    return f'{BASE_URL}/boardView.do?pageId=www102&boardId=BD_00025&seq={seq}&movePage=1'


def build_list_url(page: int = 1) -> str:
    """page ê¸°ë°˜ ëª©ë¡ í˜ì´ì§€ URL ìƒì„±"""
    if page == 1:
        return LIST_URL
    return f'{LIST_URL}&movePage={page}'


def build_file_download_url(seq: str, file_sn: int = 1) -> str:
    """ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ URL ìƒì„±
    íŒ¨í„´: /fileDownload.do?action=fileDown&mode=&boardId=BD_00025&seq={seq}&fileSn={file_sn}
    """
    return f'{BASE_URL}/fileDownload.do?action=fileDown&mode=&boardId=BD_00025&seq={seq}&fileSn={file_sn}'


# ============================================================
# 6. ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ í•¨ìˆ˜
# ============================================================
def fetch_detail(page: Page, url: str, seq: str = '') -> Tuple[str, Optional[str], str, Optional[str]]:
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸, ì´ë¯¸ì§€, ë‚ ì§œ, ë‹´ë‹¹ë¶€ì„œë¥¼ ì¶”ì¶œ

    Returns:
        (ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì¸ë„¤ì¼ URL, ë‚ ì§œ, ë‹´ë‹¹ë¶€ì„œ)
    """
    if not safe_goto(page, url, timeout=20000):
        return "", None, datetime.now().strftime('%Y-%m-%d'), None
    
    time.sleep(1)  # í˜ì´ì§€ ì•ˆì •í™”
    
    # 1. ë‚ ì§œ ì¶”ì¶œ (í˜•ì‹: YYYY-MM-DD HH:mm ë˜ëŠ” ì‘ì„±ì¼ : YYYY-MM-DD HH:mm)
    pub_date = datetime.now().strftime('%Y-%m-%d')
    
    try:
        # í˜ì´ì§€ ì „ì²´ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
        page_text = page.locator('body').inner_text()
        
        # ì‘ì„±ì¼ íŒ¨í„´: "ì‘ì„±ì¼ : YYYY-MM-DD HH:mm"
        date_match = re.search(r'ì‘ì„±ì¼\s*[:\s]+(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{2}', page_text)
        if date_match:
            y, m, d = date_match.groups()
            pub_date = f"{y}-{int(m):02d}-{int(d):02d}"
        else:
            # ì¼ë°˜ ë‚ ì§œ íŒ¨í„´
            date_match = re.search(r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})', page_text[:3000])
            if date_match:
                y, m, d = date_match.groups()
                pub_date = f"{y}-{int(m):02d}-{int(d):02d}"
    except Exception as e:
        print(f"      âš ï¸ ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    # 2. ë‹´ë‹¹ë¶€ì„œ ì¶”ì¶œ (í˜•ì‹: "ì‘ì„±ì : ë¶€ì„œëª…(íŒ€ëª…)")
    department = None
    try:
        page_text = page.locator('body').inner_text()
        # "ì‘ì„±ì : í–‰ì •ê³¼(ê³ í–¥ì‚¬ë‘)" íŒ¨í„´
        dept_match = re.search(r'ì‘ì„±ì\s*[:\s]+([^\n]+)', page_text)
        if dept_match:
            department = dept_match.group(1).strip()
    except Exception as e:
        print(f"      âš ï¸ ë‹´ë‹¹ë¶€ì„œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    # 3. ë³¸ë¬¸ ì¶”ì¶œ
    content = ""
    
    try:
        # JavaScriptë¡œ ë³¸ë¬¸ ì¶”ì¶œ (ì œëª©, ë©”íƒ€ì •ë³´ ì œì™¸)
        js_code = """
        () => {
            // ê³ í¥êµ° íŠ¹í™”: heading ì œëª© ë‹¤ìŒì˜ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
            
            // ë°©ë²• 1: ì¼ë°˜ì ì¸ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ
            const contentSelectors = [
                '.board_view_content', '.view_content', '.board_view',
                '.con-wrap', '.view-con', '.bbs_view_cont', 'article'
            ];
            
            for (const sel of contentSelectors) {
                const elem = document.querySelector(sel);
                if (elem) {
                    const text = elem.innerText?.trim();
                    if (text && text.length > 100) {
                        return text;
                    }
                }
            }
            
            // ë°©ë²• 2: div[class*="view"] íƒìƒ‰
            const viewDivs = document.querySelectorAll('div[class*="view"]');
            for (const div of viewDivs) {
                const text = div.innerText?.trim();
                if (text && text.length > 200) {
                    return text;
                }
            }
            
            // ë°©ë²• 3: ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ div ì°¾ê¸°
            const divs = document.querySelectorAll('div');
            let maxText = '';
            
            for (const div of divs) {
                const text = div.innerText?.trim();
                // ë©”ë‰´ í…ìŠ¤íŠ¸ í•„í„°ë§
                if (text && text.length > maxText.length && 
                    !text.includes('ë¡œê·¸ì¸') && !text.includes('íšŒì›ê°€ì…') &&
                    text.length < 10000) {  // ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ì œì™¸ (ì „ì²´ í˜ì´ì§€)
                    maxText = text;
                }
            }
            
            // ìµœì†Œ ê¸¸ì´ 100ì ì´ìƒ
            if (maxText.length > 100) {
                return maxText;
            }
            
            return '';
        }
        """
        content = page.evaluate(js_code)
        if content:
            # ë©”íƒ€ì •ë³´ ì œê±° (ì‘ì„±ì, ì‘ì„±ì¼ ë“±)
            content = re.sub(r'ì‘ì„±ì\s*[:\s]+[^\n]+', '', content)
            content = re.sub(r'ì‘ì„±ì¼\s*[:\s]+[^\n]+', '', content)
            content = re.sub(r'ì¡°íšŒìˆ˜\s*[:\s]+\d+', '', content)
            content = content.strip()[:5000]
    except Exception as e:
        print(f"      âš ï¸ JS ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    # Fallback: ì¼ë°˜ ì…€ë ‰í„°
    if not content or len(content) < 50:
        for sel in CONTENT_SELECTORS:
            try:
                content_elem = page.locator(sel)
                if content_elem.count() > 0:
                    text = safe_get_text(content_elem)
                    if text and len(text) > 50:
                        content = text[:5000]
                        break
            except:
                continue
    
    # 4. ì´ë¯¸ì§€ ì¶”ì¶œ (ì²¨ë¶€íŒŒì¼ ìš°ì„ )
    thumbnail_url = None
    
    # ì „ëµ 1: ì²¨ë¶€íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ URL ì¶”ì¶œ
    # ê³ í¥êµ° íŒ¨í„´: /fileDownload.do?action=fileDown&mode=&boardId=BD_00025&seq={seq}&fileSn={ìˆœë²ˆ}
    try:
        # ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
        attach_links = page.locator('a[href*="fileDownload.do"]')
        for i in range(min(attach_links.count(), 5)):
            link = attach_links.nth(i)
            link_text = safe_get_text(link) or ''
            href = safe_get_attr(link, 'href')
            
            # ì´ë¯¸ì§€ íŒŒì¼ í™•ì¥ì í™•ì¸
            if href and any(ext in link_text.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                full_url = urljoin(BASE_URL, href) if not href.startswith('http') else href
                print(f"      ğŸ“¥ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„: {link_text[:50]}...")
                
                # ë¡œì»¬ ì €ì¥
                saved_path = download_and_upload_image(full_url, url, REGION_CODE)
                if saved_path:
                    thumbnail_url = saved_path
                    break
    except Exception as e:
        print(f"      âš ï¸ ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ì „ëµ 2: seq ê¸°ë°˜ìœ¼ë¡œ ì§ì ‘ ì²¨ë¶€íŒŒì¼ URL êµ¬ì„±í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ì‹œë„
    if not thumbnail_url and seq:
        try:
            # ì²« ë²ˆì§¸ ì²¨ë¶€íŒŒì¼ ì‹œë„
            file_url = build_file_download_url(seq, 1)
            print(f"      ğŸ“¥ ì²¨ë¶€íŒŒì¼ URL ì§ì ‘ ì‹œë„: fileSn=1")
            saved_path = download_and_upload_image(file_url, url, REGION_CODE)
            if saved_path:
                thumbnail_url = saved_path
        except Exception as e:
            print(f"      âš ï¸ ì²¨ë¶€íŒŒì¼ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ì „ëµ 3: ë³¸ë¬¸ ë‚´ img íƒœê·¸ì—ì„œ ì¶”ì¶œ
    if not thumbnail_url:
        try:
            imgs = page.locator('img[src*=".jpg"], img[src*=".png"], img[src*=".jpeg"], img[src*=".JPG"]')
            for i in range(min(imgs.count(), 5)):
                src = safe_get_attr(imgs.nth(i), 'src')
                if src and not any(x in src.lower() for x in ['icon', 'btn', 'logo', 'banner', 'bg', 'arrow', 'bullet', 'blank']):
                    download_url = urljoin(BASE_URL, src) if not src.startswith('http') else src
                    saved_path = download_and_upload_image(download_url, url, REGION_CODE)
                    if saved_path:
                        thumbnail_url = saved_path
                        break
        except Exception as e:
            print(f"      âš ï¸ ë³¸ë¬¸ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    return content, thumbnail_url, pub_date, department


# ============================================================
# 7. ë©”ì¸ ìˆ˜ì§‘ í•¨ìˆ˜
# ============================================================
def collect_articles(max_articles: int = 10, days: Optional[int] = None, start_date: str = None, end_date: str = None, dry_run: bool = False) -> List[Dict]:
    """
    ë³´ë„ìë£Œë¥¼ ìˆ˜ì§‘í•˜ê³  ì„œë²„ë¡œ ì „ì†¡ (ê°œìˆ˜ ê¸°ë°˜)

    Args:
        max_articles: ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ 10ê°œ)
        days: ì„ íƒì  ë‚ ì§œ í•„í„° (Noneì´ë©´ ë¹„í™œì„±í™”)
        start_date: ìˆ˜ì§‘ ì‹œì‘ì¼ (YYYY-MM-DD)
        end_date: ìˆ˜ì§‘ ì¢…ë£Œì¼ (YYYY-MM-DD)
        dry_run: í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì„œë²„ ì „ì†¡ ì•ˆí•¨)
    """
    if not start_date and days:
        start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')

    if not end_date:
        end_date = datetime.now().strftime('%Y-%m-%d')

    if start_date:
        print(f"ğŸ›ï¸ {REGION_NAME} ë³´ë„ìë£Œ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_articles}ê°œ, {start_date} ~ {end_date})")
    else:
        print(f"ğŸ›ï¸ {REGION_NAME} ë³´ë„ìë£Œ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_articles}ê°œ, ë‚ ì§œ í•„í„° ì—†ìŒ)")
    
    if dry_run:
        print("   ğŸ§ª DRY-RUN ëª¨ë“œ: ì„œë²„ ì „ì†¡ ì•ˆí•¨")
    
    log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f'{REGION_NAME} ìŠ¤í¬ë˜í¼ v1.0 ì‹œì‘', 'info')
    
    collected_count = 0
    success_count = 0
    collected_articles = []  # dry-run ì‹œ ë°˜í™˜ìš©
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': 1280, 'height': 1024}
        )
        context.set_extra_http_headers({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        })
        
        page = context.new_page()
        
        page_num = 1
        max_pages = 10  # ìµœëŒ€ 10í˜ì´ì§€ê¹Œì§€ íƒìƒ‰
        
        while page_num <= max_pages and collected_count < max_articles:
            list_url = build_list_url(page_num)
            print(f"   ğŸ“„ í˜ì´ì§€ {page_num} ìˆ˜ì§‘ ì¤‘...")
            log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f'í˜ì´ì§€ {page_num} íƒìƒ‰', 'info')
            
            if not safe_goto(page, list_url):
                page_num += 1
                continue
            
            time.sleep(1.5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # ëª©ë¡ì—ì„œ ê¸°ì‚¬ ë§í¬ ì°¾ê¸° (ul > li êµ¬ì¡° ë˜ëŠ” table êµ¬ì¡°)
            article_links = page.locator('a[href*="boardView.do"][href*="seq="]')
            article_count = article_links.count()
            
            if article_count == 0:
                # Fallback: ë‹¤ë¥¸ ì…€ë ‰í„° ì‹œë„
                for sel in LIST_ITEM_SELECTORS:
                    article_links = page.locator(sel)
                    article_count = article_links.count()
                    if article_count > 0:
                        break
            
            if article_count == 0:
                print("      âš ï¸ ê¸°ì‚¬ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            print(f"      ğŸ“° {article_count}ê°œ ê¸°ì‚¬ ë§í¬ ë°œê²¬")
            
            # ë§í¬ ì •ë³´ ìˆ˜ì§‘
            link_data = []
            seen_seqs = set()  # ì¤‘ë³µ seq ì²´í¬ìš©
            
            for i in range(article_count):
                if collected_count + len(link_data) >= max_articles:
                    break
                
                try:
                    link = article_links.nth(i)
                    
                    # ì œëª©ê³¼ URL ì¶”ì¶œ
                    title = safe_get_text(link)
                    if title:
                        title = title.strip()
                    href = safe_get_attr(link, 'href')
                    
                    if not title or not href:
                        continue
                    
                    # seq ì¶”ì¶œ
                    seq = extract_seq(href)
                    if not seq:
                        continue
                    
                    # ì¤‘ë³µ seq ì²´í¬
                    if seq in seen_seqs:
                        continue
                    seen_seqs.add(seq)
                    
                    # ìƒì„¸ í˜ì´ì§€ URL êµ¬ì„±
                    full_url = build_detail_url(seq)
                    
                    # ëª©ë¡ì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„ (YYYY-MM-DD í˜•ì‹)
                    list_date = None
                    try:
                        # ë¶€ëª¨ ìš”ì†Œì—ì„œ ë‚ ì§œ ì°¾ê¸°
                        parent = link.locator('xpath=ancestor::*[self::li or self::tr][1]')
                        if parent.count() > 0:
                            parent_text = safe_get_text(parent)
                            if parent_text:
                                date_match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', parent_text)
                                if date_match:
                                    y, m, d = date_match.groups()
                                    list_date = f"{y}-{int(m):02d}-{int(d):02d}"
                    except:
                        pass
                    
                    # ë‚ ì§œ í•„í„° (ëª©ë¡ ë‹¨ê³„)
                    if start_date and list_date and list_date < start_date:
                        print(f"      â© ëª©ë¡ì—ì„œ ë‚ ì§œ í•„í„°: {list_date} < {start_date}")
                        continue
                    
                    link_data.append({
                        'title': title,
                        'url': full_url,
                        'seq': seq,
                        'list_date': list_date
                    })
                    
                except Exception as e:
                    continue
            
            # ì´ í˜ì´ì§€ì—ì„œ ìœ íš¨í•œ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ íƒìƒ‰ ì¤‘ì§€
            if len(link_data) == 0:
                print("      â¹ï¸ ì´ í˜ì´ì§€ì— ìœ íš¨í•œ ê¸°ì‚¬ê°€ ì—†ìŒ, íƒìƒ‰ ì¤‘ì§€")
                break
            
            # ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ ë° ì „ì†¡
            consecutive_old = 0  # ì—°ì† ì˜¤ë˜ëœ ê¸°ì‚¬ ì¹´ìš´í„°
            stop_scraping = False
            
            for item in link_data:
                if collected_count >= max_articles or stop_scraping:
                    break
                
                title = item['title']
                full_url = item['url']
                seq = item['seq']
                
                print(f"      ğŸ“° {title[:40]}...")
                log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f"ìˆ˜ì§‘ ì¤‘: {title[:20]}...", 'info')
                
                content, thumbnail_url, detail_date, department = fetch_detail(page, full_url, seq)
                
                # ë‚ ì§œ ê²°ì • (ìƒì„¸ > ëª©ë¡ > í˜„ì¬)
                final_date = detail_date or item.get('list_date') or datetime.now().strftime('%Y-%m-%d')
                
                # ë‚ ì§œ í•„í„° + ì¡°ê¸° ì¢…ë£Œ ë¡œì§
                if start_date and final_date < start_date:
                    consecutive_old += 1
                    print(f"         â© ë‚ ì§œ í•„í„°ë¡œ ìŠ¤í‚µ: {final_date} (ì—°ì† {consecutive_old}ê°œ)")
                    
                    if consecutive_old >= 3:
                        print("         â¹ï¸ ì˜¤ë˜ëœ ê¸°ì‚¬ 3ê°œ ì—°ì† ë°œê²¬, í˜ì´ì§€ íƒìƒ‰ ì¤‘ì§€")
                        stop_scraping = True
                        break
                    continue
                
                # ìœ íš¨í•œ ê¸°ì‚¬ ë°œê²¬ ì‹œ ì¹´ìš´í„° ë¦¬ì…‹
                consecutive_old = 0
                
                if not content:
                    content = f"ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì›ë³¸ ë§í¬: {full_url}"
                
                article_data = {
                    'title': title,
                    'content': content,
                    'published_at': f"{final_date}T09:00:00+09:00",
                    'original_link': full_url,
                    'source': REGION_NAME,
                    'category': CATEGORY_NAME,
                    'region': REGION_CODE,
                    'thumbnail_url': thumbnail_url,
                }
                
                if dry_run:
                    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì„œë²„ ì „ì†¡ ì•ˆí•¨
                    collected_count += 1
                    success_count += 1
                    img_status = "âœ“ì´ë¯¸ì§€" if thumbnail_url else "âœ—ì´ë¯¸ì§€"
                    content_status = f"âœ“ë³¸ë¬¸({len(content)}ì)" if content and len(content) > 50 else "âœ—ë³¸ë¬¸"
                    print(f"         ğŸ§ª [DRY-RUN] {img_status}, {content_status}")
                    collected_articles.append(article_data)
                else:
                    # ì„œë²„ë¡œ ì „ì†¡
                    result = send_article_to_server(article_data)
                    collected_count += 1
                    
                    if result.get('status') == 'created':
                        success_count += 1
                        img_status = "âœ“ì´ë¯¸ì§€" if thumbnail_url else "âœ—ì´ë¯¸ì§€"
                        print(f"         âœ… ì €ì¥ ì™„ë£Œ ({img_status})")
                        log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f"ì €ì¥ ì™„ë£Œ: {title[:15]}...", 'success')
                    elif result.get('status') == 'exists':
                        print(f"         â© ì´ë¯¸ ì¡´ì¬")
                    else:
                        print(f"         âš ï¸ ì „ì†¡ ì‹¤íŒ¨: {result}")
                
                time.sleep(1)  # Rate limiting
            
            # ì¡°ê¸° ì¢…ë£Œ ì‹œ ë£¨í”„ íƒˆì¶œ
            if stop_scraping:
                break
            
            page_num += 1
            time.sleep(1)
        
        browser.close()
    
    final_msg = f"ìˆ˜ì§‘ ì™„ë£Œ (ì´ {collected_count}ê°œ, ì‹ ê·œ {success_count}ê°œ)"
    print(f"âœ… {final_msg}")
    log_to_server(REGION_CODE, 'ì„±ê³µ', final_msg, 'success')
    
    return collected_articles


# ============================================================
# 8. CLI ì§„ì…ì 
# ============================================================
def main():
    import argparse
    parser = argparse.ArgumentParser(description=f'{REGION_NAME} ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼ v1.0')
    parser.add_argument('--max-articles', type=int, default=10, help='ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ 10)')
    parser.add_argument('--days', type=int, default=None, help='ì„ íƒì  ë‚ ì§œ í•„í„° (ì¼). ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ë‚ ì§œ í•„í„° ì—†ìŒ')
    parser.add_argument('--dry-run', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì„œë²„ ì „ì†¡ ì•ˆí•¨)')
    # bot-service.ts í˜¸í™˜ ì¸ì (í•„ìˆ˜!)
    parser.add_argument('--start-date', type=str, default=None, help='ìˆ˜ì§‘ ì‹œì‘ì¼ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, default=None, help='ìˆ˜ì§‘ ì¢…ë£Œì¼ (YYYY-MM-DD)')
    args = parser.parse_args()

    collect_articles(
        max_articles=args.max_articles,
        days=args.days,
        start_date=args.start_date,
        end_date=args.end_date,
        dry_run=args.dry_run
    )


if __name__ == "__main__":
    main()
