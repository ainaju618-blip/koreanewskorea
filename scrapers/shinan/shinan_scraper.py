"""
ì‹ ì•ˆêµ° ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼
- ë²„ì „: v2.0
- ìµœì¢…ìˆ˜ì •: 2025-12-13
- ë‹´ë‹¹: AI Agent

ì‚¬ì´íŠ¸ íŠ¹ì§•:
- wscms ê¸°ë°˜ ì‚¬ì´íŠ¸
- ëª©ë¡ URL: https://www.shinan.go.kr/home/www/openinfo/participation_07/participation_07_03/page.wscms
- ìƒì„¸ í˜ì´ì§€: /show/{ID} íŒ¨í„´
- í˜ì´ì§€ë„¤ì´ì…˜: ?page={N}
- ë³¸ë¬¸ êµ¬ì¡°: table.show_form ë‚´ label íƒœê·¸ ê¸°ë°˜
"""

# ============================================================
# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ============================================================
import sys
import os
import time
import re
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
from urllib.parse import urljoin

# ============================================================
# 2. ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ============================================================
from playwright.sync_api import sync_playwright, Page

# ============================================================
# 3. ë¡œì»¬ ëª¨ë“ˆ
# ============================================================
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.api_client import send_article_to_server, log_to_server, ensure_server_running
from utils.scraper_utils import safe_goto, wait_and_find, safe_get_text, safe_get_attr, clean_article_content, extract_subtitle
from utils.cloudinary_uploader import download_and_upload_image
from utils.category_detector import detect_category

# ============================================================
# 4. ìƒìˆ˜ ì •ì˜
# ============================================================
REGION_CODE = 'shinan'
REGION_NAME = 'ì‹ ì•ˆêµ°'
CATEGORY_NAME = 'ì „ë‚¨'
BASE_URL = 'https://www.shinan.go.kr'
LIST_URL = 'https://www.shinan.go.kr/home/www/openinfo/participation_07/participation_07_03/page.wscms'

# ëª©ë¡ í˜ì´ì§€ ì…€ë ‰í„°
LIST_ITEM_SELECTORS = [
    'table.bbsListTbl tbody tr',
    'table tbody tr',
    '.bbs_list tbody tr',
]

# ë³¸ë¬¸ í˜ì´ì§€ ì…€ë ‰í„°
CONTENT_SELECTORS = [
    'div.bbsV_cont',
    'div.view_content',
    'div.board_view',
    'div.contents',
    'div.con-wrap',
    'section[role="region"]',
]


# ============================================================
# 5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def normalize_date(date_str: str) -> str:
    """
    ë‚ ì§œ ë¬¸ìì—´ì„ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
    
    ì‹ ì•ˆêµ° ë‚ ì§œ í˜•ì‹:
    - ëª©ë¡: YYYY-MM-DD ë˜ëŠ” YYYY.MM.DD
    """
    if not date_str:
        return datetime.now().strftime('%Y-%m-%d')
    
    date_str = date_str.strip().replace('.', '-').replace('/', '-')
    try:
        match = re.search(r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})', date_str)
        if match:
            y, m, d = match.groups()
            return f"{y}-{int(m):02d}-{int(d):02d}"
    except:
        pass
    return datetime.now().strftime('%Y-%m-%d')


def extract_article_id(href: str) -> Optional[str]:
    """hrefì—ì„œ ê²Œì‹œë¬¼ ID ì¶”ì¶œ (/show/{ID} íŒ¨í„´)"""
    if not href:
        return None
    # /show/{ID} íŒ¨í„´
    match = re.search(r'/show/(\d+)', href)
    if match:
        return match.group(1)
    # idx={ID} íŒ¨í„´ (í´ë°±)
    match = re.search(r'idx=(\d+)', href)
    return match.group(1) if match else None


# ============================================================
# 6. ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ í•¨ìˆ˜
# ============================================================
def fetch_detail(page: Page, url: str) -> Tuple[str, Optional[str], str]:
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸, ì´ë¯¸ì§€, ë‚ ì§œë¥¼ ì¶”ì¶œ
    
    ì‹ ì•ˆêµ° ìƒì„¸ í˜ì´ì§€ êµ¬ì¡°:
    - table.show_form: ë©”ì¸ í…Œì´ë¸”
    - <label>ì œëª©</label> -> ë‹¤ìŒ tdì— ì œëª©
    - <label>ë‚´ìš©</label> -> ë‹¤ìŒ tdì— ë³¸ë¬¸
    - <label>ë“±ë¡ì¼</label> -> ë‹¤ìŒ tdì— ë‚ ì§œ
    
    Returns:
        (ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì¸ë„¤ì¼ URL, ë‚ ì§œ)
    """
    if not safe_goto(page, url, timeout=20000):
        return "", None, datetime.now().strftime('%Y-%m-%d')
    
    time.sleep(1.5)  # í˜ì´ì§€ ì•ˆì •í™”
    
    # JavaScriptë¡œ label ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ
    try:
        result = page.evaluate("""
        () => {
            const data = {
                content: '',
                date: '',
                images: []
            };
            
            // table.show_form ë‚´ì—ì„œ label ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ì¶”ì¶œ
            const labels = document.querySelectorAll('table.show_form label, table label');
            
            for (const label of labels) {
                const labelText = label.innerText?.trim();
                
                // ë¶€ëª¨ th/tdì˜ ë‹¤ìŒ í˜•ì œ tdì—ì„œ ê°’ ì¶”ì¶œ
                const parentCell = label.closest('th') || label.closest('td');
                const valueCell = parentCell?.nextElementSibling;
                
                if (!valueCell) continue;
                
                if (labelText === 'ë‚´ìš©') {
                    // ë³¸ë¬¸ ì¶”ì¶œ - HTML íƒœê·¸ ìœ ì§€í•˜ë©° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    data.content = valueCell.innerText?.trim() || '';
                    
                    // ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ ì¶”ì¶œ
                    const imgs = valueCell.querySelectorAll('img');
                    for (const img of imgs) {
                        const src = img.src;
                        if (src && !src.includes('icon') && !src.includes('btn') && 
                            !src.includes('logo') && !src.includes('bullet')) {
                            data.images.push(src);
                        }
                    }
                }
                else if (labelText === 'ë“±ë¡ì¼') {
                    data.date = valueCell.innerText?.trim() || '';
                }
            }
            
            // í´ë°±: labelì´ ì—†ëŠ” ê²½ìš° ê°€ì¥ ê¸´ td ì°¾ê¸°
            if (!data.content) {
                const tds = document.querySelectorAll('table td');
                let longestTd = null;
                let maxLen = 0;
                
                for (const td of tds) {
                    const text = td.innerText?.trim();
                    // ë©”ë‰´ í…ìŠ¤íŠ¸ ì œì™¸
                    if (text && text.length > maxLen && text.length < 10000 &&
                        !text.includes('ì‹ ì•ˆêµ°ì†Œê°œ') && 
                        !text.includes('ì „ìë¯¼ì›') &&
                        !text.includes('ì—´ë¦°êµ°ì •') &&
                        !text.includes('ì°¸ì—¬ë§ˆë‹¹') &&
                        !text.includes('ë¶„ì•¼ë³„ì •ë³´') &&
                        !text.includes('ì‚¬ì´íŠ¸ë§µ')) {
                        maxLen = text.length;
                        longestTd = td;
                    }
                }
                
                if (longestTd && maxLen > 50) {
                    data.content = longestTd.innerText?.trim();
                    
                    // ì´ë¯¸ì§€ ì¶”ì¶œ
                    const imgs = longestTd.querySelectorAll('img');
                    for (const img of imgs) {
                        const src = img.src;
                        if (src && !src.includes('icon') && !src.includes('btn')) {
                            data.images.push(src);
                        }
                    }
                }
            }
            
            // ì²¨ë¶€íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ URL ì°¾ê¸°
            if (data.images.length === 0) {
                const attachments = document.querySelectorAll('a[href*="download"], a[href*="/data/"]');
                for (const a of attachments) {
                    const href = a.href;
                    if (href && (href.includes('.jpg') || href.includes('.jpeg') || 
                                 href.includes('.png') || href.includes('.gif'))) {
                        data.images.push(href);
                    }
                }
            }
            
            // ë³¸ë¬¸ ë‚´ ì¸ë¼ì¸ ì´ë¯¸ì§€ URL íŒ¨í„´ ì°¾ê¸° (img íƒœê·¸ê°€ ì•„ë‹Œ ê²½ìš°)
            if (data.images.length === 0) {
                const allImgs = document.querySelectorAll('img[src*="/board/data/"], img[src*="/images/board/"]');
                for (const img of allImgs) {
                    const src = img.src;
                    if (src && !src.includes('list') && !src.includes('admin') &&
                        !src.includes('icon') && !src.includes('btn')) {
                        data.images.push(src);
                    }
                }
            }
            
            return data;
        }
        """)
        
        content = result.get('content', '')
        date_str = result.get('date', '')
        images = result.get('images', [])
        
    except Exception as e:
        print(f"      [WARN] JS extraction failed: {e}")
        content = ""
        date_str = ""
        images = []
    
    # ë‚ ì§œ íŒŒì‹±
    pub_date = datetime.now().strftime('%Y-%m-%d')
    if date_str:
        date_match = re.search(r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})', date_str)
        if date_match:
            y, m, d = date_match.groups()
            pub_date = f"{y}-{int(m):02d}-{int(d):02d}"
    
    # ë³¸ë¬¸ ì •ë¦¬ - clean_article_content í•¨ìˆ˜ ì‚¬ìš©
    content = clean_article_content(content)
    
    # ì´ë¯¸ì§€ ì²˜ë¦¬
    thumbnail_url = None
    if images:
        for img_url in images[:3]:  # ìµœëŒ€ 3ê°œ ì‹œë„
            try:
                full_url = urljoin(BASE_URL, img_url) if not img_url.startswith('http') else img_url
                # Cloudinary ì—…ë¡œë“œ
                cloudinary_url = download_and_upload_image(full_url, BASE_URL, folder=REGION_CODE)
                if cloudinary_url:
                    thumbnail_url = cloudinary_url
                    break
                else:
                    thumbnail_url = full_url
                    break
            except Exception as e:
                continue
    
    return content, thumbnail_url, pub_date


# ============================================================
# 7. ë©”ì¸ ìˆ˜ì§‘ í•¨ìˆ˜
# ============================================================
def collect_articles(days: int = 3, max_articles: int = 10, start_date: str = None, end_date: str = None) -> List[Dict]:
    """
    ë³´ë„ìë£Œë¥¼ ìˆ˜ì§‘í•˜ê³  ì„œë²„ë¡œ ì „ì†¡

    Args:
        days: ìˆ˜ì§‘í•  ê¸°ê°„ (ì¼)
        max_articles: ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜
        start_date: ìˆ˜ì§‘ ì‹œì‘ì¼ (YYYY-MM-DD)
        end_date: ìˆ˜ì§‘ ì¢…ë£Œì¼ (YYYY-MM-DD)
    """
    print(f"ğŸ›ï¸ {REGION_NAME} ë³´ë„ìë£Œ ìˆ˜ì§‘ ì‹œì‘ (ìµœê·¼ {days}ì¼)
    

    # Ensure dev server is running before starting

    if not ensure_server_running():

        print("[ERROR] Dev server could not be started. Aborting.")

        return []
")
    log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f'{REGION_NAME} ìŠ¤í¬ë˜í¼ v1.0 ì‹œì‘', 'info')

    if not end_date:
        end_date = datetime.now().strftime('%Y-%m-%d')
    if not start_date:
        start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
    
    collected_count = 0
    success_count = 0
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            viewport={'width': 1280, 'height': 1024}
        )
        page = context.new_page()
        
        page_num = 1
        stop = False
        
        while page_num <= 5 and not stop and collected_count < max_articles:
            list_url = f'{LIST_URL}?page={page_num}'
            print(f"   ğŸ“„ í˜ì´ì§€ {page_num} ìˆ˜ì§‘ ì¤‘...")
            log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f'í˜ì´ì§€ {page_num} íƒìƒ‰', 'info')
            
            if not safe_goto(page, list_url):
                page_num += 1
                continue
            
            time.sleep(1.5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # ëª©ë¡ í•­ëª© ì°¾ê¸°
            items = None
            for sel in LIST_ITEM_SELECTORS:
                try:
                    rows = page.locator(sel)
                    if rows.count() > 0:
                        items = rows
                        break
                except:
                    continue
            
            if not items:
                # ëŒ€ì•ˆ: ëª¨ë“  ë§í¬ì—ì„œ /show/ íŒ¨í„´ ì°¾ê¸°
                try:
                    items = page.locator('a[href*="/show/"]')
                    if items.count() == 0:
                        print("      âš ï¸ ê¸°ì‚¬ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        break
                except:
                    print("      âš ï¸ ê¸°ì‚¬ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
            
            item_count = items.count()
            print(f"      ğŸ“° {item_count}ê°œ ê¸°ì‚¬ ë°œê²¬")
            
            # ë§í¬ ì •ë³´ ìˆ˜ì§‘
            link_data = []
            seen_urls = set()

            for i in range(item_count):
                if collected_count + len(link_data) >= max_articles:
                    break
                    
                try:
                    item = items.nth(i)
                    
                    # trì¸ ê²½ìš° ë‚´ë¶€ a íƒœê·¸ì—ì„œ ë§í¬ ì¶”ì¶œ
                    link_elem = item.locator('a[href*="/show/"]')
                    if link_elem.count() == 0:
                        link_elem = item.locator('a').first
                    else:
                        link_elem = link_elem.first
                    
                    if link_elem.count() == 0:
                        # item ìì²´ê°€ a íƒœê·¸ì¸ ê²½ìš°
                        href = safe_get_attr(item, 'href')
                        title = safe_get_text(item)
                    else:
                        href = safe_get_attr(link_elem, 'href')
                        title = safe_get_text(link_elem)
                    
                    title = title.strip() if title else ""
                    
                    if not title or not href:
                        continue
                    
                    # ì œëª© ì •ë¦¬ (ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°)
                    title = re.sub(r'\s+', ' ', title).strip()
                    
                    # ìƒì„¸ í˜ì´ì§€ URL êµ¬ì„±
                    if href.startswith('http'):
                        full_url = href
                    else:
                        full_url = urljoin(BASE_URL, href)
                    
                    # ë‚ ì§œ ì¶”ì¶œ (ëª©ë¡ í–‰ì—ì„œ)
                    try:
                        row_text = item.inner_text()
                        date_match = re.search(r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})', row_text)
                        if date_match:
                            y, m, d = date_match.groups()
                            n_date = f"{y}-{int(m):02d}-{int(d):02d}"
                        else:
                            n_date = None
                    except:
                        n_date = None
                    
                    # ë‚ ì§œ í•„í„°ë§
                    if n_date:
                        if n_date < start_date:
                            stop = True
                            break
                        if n_date > end_date:
                            continue

                    # ì¤‘ë³µ URL ì²´í¬
                    if full_url in seen_urls:
                        continue
                    seen_urls.add(full_url)

                    link_data.append({
                        'title': title,
                        'url': full_url,
                        'list_date': n_date
                    })

                except Exception as e:
                    continue

            # ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ ë° ì „ì†¡
            for item in link_data:
                if collected_count >= max_articles:
                    break
                    
                title = item['title']
                full_url = item['url']
                
                print(f"      ğŸ“° {title[:35]}...")
                log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f"ìˆ˜ì§‘ ì¤‘: {title[:20]}...", 'info')
                
                content, thumbnail_url, detail_date = fetch_detail(page, full_url)

                # ë‚ ì§œ ê²°ì • (ìƒì„¸ > ëª©ë¡)
                final_date = detail_date or item.get('list_date') or datetime.now().strftime('%Y-%m-%d')

                # ë‚ ì§œ í•„í„°ë§ (ìƒì„¸ í˜ì´ì§€ì—ì„œ ì–»ì€ ì •í™•í•œ ë‚ ì§œë¡œ)
                if final_date < start_date:
                    stop = True
                    break

                if not content:
                    content = f"ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì›ë³¸ ë§í¬: {full_url}"

                # ë¶€ì œëª© ì¶”ì¶œ
                subtitle, content = extract_subtitle(content, title)

                # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
                cat_code, cat_name = detect_category(title, content)

                article_data = {
                    'title': title,
                    'subtitle': subtitle,
                    'content': content,
                    'published_at': f"{final_date}T09:00:00+09:00",
                    'original_link': full_url,
                    'source': REGION_NAME,
                    'category': cat_name,
                    'region': REGION_CODE,
                    'thumbnail_url': thumbnail_url,
                }
                
                # ì„œë²„ë¡œ ì „ì†¡
                result = send_article_to_server(article_data)
                collected_count += 1
                
                if result.get('status') == 'created':
                    success_count += 1
                    img_status = "âœ“ì´ë¯¸ì§€" if thumbnail_url else "âœ—ì´ë¯¸ì§€"
                    print(f"         âœ… ì €ì¥ ì™„ë£Œ ({img_status})")
                    log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f"ì €ì¥ ì™„ë£Œ: {title[:15]}...", 'success')
                elif result.get('status') == 'exists':
                    print(f"         â© ì´ë¯¸ ì¡´ì¬")
                
                time.sleep(0.5)  # Rate limiting
            
            page_num += 1
            if stop:
                print("      ğŸ›‘ ìˆ˜ì§‘ ê¸°ê°„ ì´ˆê³¼, ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            time.sleep(1)
        
        browser.close()
    
    final_msg = f"ìˆ˜ì§‘ ì™„ë£Œ (ì´ {collected_count}ê°œ, ì‹ ê·œ {success_count}ê°œ)"
    print(f"âœ… {final_msg}")
    log_to_server(REGION_CODE, 'ì„±ê³µ', final_msg, 'success')
    
    return []


# ============================================================
# 8. CLI ì§„ì…ì 
# ============================================================
def main():
    import argparse
    parser = argparse.ArgumentParser(description=f'{REGION_NAME} ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼ v1.0')
    parser.add_argument('--days', type=int, default=3, help='ìˆ˜ì§‘ ê¸°ê°„ (ì¼)')
    parser.add_argument('--max-articles', type=int, default=10, help='ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜')
    parser.add_argument('--dry-run', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì„œë²„ ì „ì†¡ ì•ˆí•¨)')
    # bot-service.ts í˜¸í™˜ ì¸ì (í•„ìˆ˜)
    parser.add_argument('--start-date', type=str, default=None, help='ìˆ˜ì§‘ ì‹œì‘ì¼ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, default=None, help='ìˆ˜ì§‘ ì¢…ë£Œì¼ (YYYY-MM-DD)')
    args = parser.parse_args()

    collect_articles(
        days=args.days,
        max_articles=args.max_articles,
        start_date=args.start_date,
        end_date=args.end_date
    )


if __name__ == "__main__":
    main()
