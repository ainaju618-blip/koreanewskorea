"""
í•´ì™¸ AI ë‰´ìŠ¤ ì „ë¬¸(Full Text) ìˆ˜ì§‘ê¸°
- RSSì—ì„œ ë§í¬ í™•ë³´ í›„ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
- ë³¸ë¬¸ ì „ì²´(ìµœì†Œ 1000ì) + ê³ í•´ìƒë„ ì´ë¯¸ì§€(og:image) ìˆ˜ì§‘
- Next.js API ì—°ë™ (/api/bot/ingest)
"""

import feedparser
import requests
from datetime import datetime
import json
import os
import sys
import time
from bs4 import BeautifulSoup

# Logs directory - all logs go to logs/ folder
LOG_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'logs')
os.makedirs(LOG_DIR, exist_ok=True)
from dateutil import parser as date_parser
from typing import List, Dict, Optional

# ê³µí†µ API í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
try:
    from utils.api_client import send_article_to_server
    from utils.ai_rewriter import rewrite_article
except ImportError:
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰ ì‹œ
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'scrapers'))
    from scrapers.utils.api_client import send_article_to_server
    from scrapers.utils.ai_rewriter import rewrite_article

# === ì„¤ì • ===
RSS_FEEDS = {
    "TechCrunch": "https://techcrunch.com/category/artificial-intelligence/feed/",
    "Wired AI": "https://www.wired.com/feed/tag/ai/latest/rss",
    "MIT Tech Review": "https://www.technologyreview.com/topic/artificial-intelligence/feed",
    "AI News": "https://artificialintelligence-news.com/feed/",
}

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.9,ko;q=0.8',
}

MAX_ARTICLES_PER_FEED = 5
MIN_CONTENT_LENGTH = 1000  # ìµœì†Œ 1000ì


class FullTextRSSCollector:
    """2ë‹¨ê³„ ìŠ¤í¬ë˜í•‘: RSS â†’ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§"""
    
    def __init__(self):
        self.collected_news: List[Dict] = []
        self.stats = {'created': 0, 'skipped': 0, 'failed': 0, 'scrape_fail': 0}

    def run(self, mode: str = 'api'):
        """
        RSS ìˆ˜ì§‘ ë° ì „ë¬¸ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        
        Args:
            mode: 'api' - APIë¡œ ì „ì†¡ (ê¸°ë³¸ê°’)
                  'json' - JSON íŒŒì¼ë¡œ ì €ì¥
                  'both' - ë‘˜ ë‹¤ ì‹¤í–‰
        """
        print("=" * 60)
        print("ğŸŒ í•´ì™¸ AI ë‰´ìŠ¤ ì „ë¬¸(Full Text) ìˆ˜ì§‘ê¸° ì‹œì‘")
        print("=" * 60)
        
        # Step 1: ëª¨ë“  í”¼ë“œì—ì„œ ë§í¬ ìˆ˜ì§‘ + ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
        for source_name, rss_url in RSS_FEEDS.items():
            self._process_feed(source_name, rss_url)
        
        if not self.collected_news:
            print("\nâš ï¸ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nâœ… ì´ {len(self.collected_news)}ê°œ ê¸°ì‚¬ ì „ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ëª¨ë“œë³„ ì²˜ë¦¬
        if mode in ('json', 'both'):
            self._save_to_json()
        
        if mode in ('api', 'both'):
            self._send_to_api()
        
        self._print_summary(mode)

    def _process_feed(self, source_name: str, rss_url: str):
        """ë‹¨ì¼ RSS í”¼ë“œ ì²˜ë¦¬ (2ë‹¨ê³„ ìŠ¤í¬ë˜í•‘)"""
        print(f"\n[*] {source_name} ì²˜ë¦¬ ì¤‘...")
        
        try:
            # Step 1: RSSì—ì„œ ë§í¬ ëª©ë¡ í™•ë³´
            feed = feedparser.parse(rss_url)
            
            if not feed.entries:
                print(f"    âš ï¸ í”¼ë“œì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            for entry in feed.entries[:MAX_ARTICLES_PER_FEED]:
                article_url = entry.link
                title = entry.title
                
                print(f"    ğŸ“° {title[:50]}...")
                
                # Step 2: ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
                full_content = self._scrape_full_content(article_url, source_name)
                
                if not full_content:
                    print(f"        âŒ ì „ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨")
                    self.stats['scrape_fail'] += 1
                    continue
                
                content_len = len(full_content['content'])
                print(f"        âœ… ë³¸ë¬¸ ê¸¸ì´: {content_len}ì")
                
                if content_len < MIN_CONTENT_LENGTH:
                    print(f"        âš ï¸ ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ ({content_len}ì < {MIN_CONTENT_LENGTH}ì), RSS ìš”ì•½ ì‚¬ìš©")
                    # í´ë°±: RSS ìš”ì•½ ì‚¬ìš©
                    summary = self._clean_html(entry.get('summary', ''))
                    full_content['content'] = summary
                
                # Step 3: AI ë²ˆì—­ ì‹œë„ (OPENAI_API_KEYê°€ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ)
                final_title = title
                final_content = full_content['content']
                ai_summary = ''
                
                ai_result = rewrite_article(title, full_content['content'], source_name)
                if ai_result:
                    print(f"        ğŸ¤– AI ë²ˆì—­ ì™„ë£Œ")
                    final_title = ai_result.get('title') or title
                    final_content = ai_result.get('content') or full_content['content']
                    ai_summary = ai_result.get('ai_summary', '')
                else:
                    # AI ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ + íƒœê·¸
                    print(f"        âš ï¸ AI ë²ˆì—­ ë¯¸ìˆ˜í–‰ (API í‚¤ ë¯¸ì„¤ì • ë˜ëŠ” ì˜¤ë¥˜)")
                    final_content = f"[AI ë²ˆì—­ ì¤€ë¹„ì¤‘]\n\n{full_content['content']}"
                
                # ë‚ ì§œ íŒŒì‹±
                published_at = self._parse_date(entry.get('published', ''))
                
                news_item = {
                    'source': source_name,
                    'title': final_title,
                    'link': article_url,
                    'published_at': published_at,
                    'content': final_content,
                    'content_length': len(final_content),
                    'thumbnail_url': full_content.get('image_url'),
                    'category': 'AI',
                    'ai_summary': ai_summary,
                }
                self.collected_news.append(news_item)
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€ (Anti-blocking)
                time.sleep(1)
                
        except Exception as e:
            print(f"    âŒ í”¼ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    def _scrape_full_content(self, url: str, source_name: str) -> Optional[Dict]:
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì „ë¬¸ + ì´ë¯¸ì§€ ì¶”ì¶œ"""
        try:
            resp = requests.get(url, headers=HEADERS, timeout=15)
            resp.raise_for_status()
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for tag in soup.find_all(['script', 'style', 'nav', 'footer', 'aside', 'iframe']):
                tag.decompose()
            for tag in soup.find_all(class_=['ad', 'advertisement', 'social-share', 'related-posts']):
                tag.decompose()
            
            # 1. ë³¸ë¬¸ ì¶”ì¶œ (ì‚¬ì´íŠ¸ë³„ ì „ëµ)
            content = self._extract_article_content(soup, source_name)
            
            # 2. ì´ë¯¸ì§€ ì¶”ì¶œ (og:image ìš°ì„ )
            image_url = self._extract_og_image(soup)
            
            if not content:
                return None
            
            # ìˆœìˆ˜ ë³¸ë¬¸ ë°˜í™˜ (AI ë²ˆì—­ì€ _process_feedì—ì„œ ì²˜ë¦¬)
            return {
                'content': content,
                'image_url': image_url
            }
            
        except requests.exceptions.Timeout:
            print(f"        â±ï¸ íƒ€ì„ì•„ì›ƒ")
            return None
        except Exception as e:
            print(f"        âŒ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {str(e)[:50]}")
            return None

    def _extract_article_content(self, soup: BeautifulSoup, source_name: str) -> str:
        """ì‚¬ì´íŠ¸ë³„ ë³¸ë¬¸ ì¶”ì¶œ ì „ëµ"""
        
        # ê³µí†µ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ìš°ì„ ìˆœìœ„ ìˆœ)
        article = (
            soup.find('article') or
            soup.find('div', class_='wp-block-post-content') or
            soup.find('div', class_='article-content') or
            soup.find('div', class_='entry-content') or
            soup.find('div', class_='post-content') or
            soup.find('div', class_='content-body') or
            soup.find('main')
        )
        
        if not article:
            # í´ë°±: body ì „ì²´ì—ì„œ p íƒœê·¸ ìˆ˜ì§‘
            article = soup.find('body')
        
        if not article:
            return ''
        
        # ëª¨ë“  p íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        paragraphs = article.find_all('p')
        
        # ì˜ë¯¸ ìˆëŠ” ë¬¸ë‹¨ë§Œ ìˆ˜ì§‘ (ìµœì†Œ 20ì ì´ìƒ)
        valid_paragraphs = []
        for p in paragraphs:
            text = p.get_text(strip=True)
            # ê´‘ê³ /ì†Œì…œ í…ìŠ¤íŠ¸ í•„í„°ë§
            if len(text) > 20 and not self._is_junk_text(text):
                valid_paragraphs.append(text)
        
        full_text = '\n\n'.join(valid_paragraphs)
        
        # 5000ì ì œí•œ
        return full_text[:5000]

    def _is_junk_text(self, text: str) -> bool:
        """ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ í•„í„°ë§"""
        junk_patterns = [
            'read more', 'continue reading', 'subscribe',
            'sign up', 'newsletter', 'advertisement',
            'click here', 'learn more', 'share this',
            'follow us', 'related articles', 'recommended',
        ]
        text_lower = text.lower()
        return any(pattern in text_lower for pattern in junk_patterns)

    def _extract_og_image(self, soup: BeautifulSoup) -> Optional[str]:
        """og:image ë©”íƒ€ íƒœê·¸ì—ì„œ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            return og_image['content']
        
        # í´ë°±: twitter:image
        twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
        if twitter_image and twitter_image.get('content'):
            return twitter_image['content']
        
        return None

    def _parse_date(self, date_str: str) -> str:
        """ë‚ ì§œ ë¬¸ìì—´ì„ ISO 8601ë¡œ ë³€í™˜"""
        if not date_str:
            return datetime.now().isoformat()
        try:
            dt = date_parser.parse(date_str)
            return dt.isoformat()
        except Exception:
            return datetime.now().isoformat()

    def _clean_html(self, html_content: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        if not html_content:
            return ""
        soup = BeautifulSoup(html_content, 'html.parser')
        return soup.get_text(strip=True)[:5000]

    def _send_to_api(self):
        """ìˆ˜ì§‘ëœ ê¸°ì‚¬ë¥¼ APIë¡œ ì „ì†¡"""
        print(f"\nğŸŒ API ì „ì†¡ ì‹œì‘ ({len(self.collected_news)}ê°œ)...\n")
        
        for news in self.collected_news:
            payload = {
                'title': news['title'],
                'content': news['content'],
                'original_link': news['link'],
                'published_at': news['published_at'],
                'source': news['source'],
                'category': news['category'],
                'thumbnail_url': news.get('thumbnail_url'),
                'ai_summary': news.get('ai_summary', ''),  # AI ìš”ì•½ ì¶”ê°€
            }
            
            result = send_article_to_server(payload)
            
            if result['status'] == 'created':
                self.stats['created'] += 1
            elif result['status'] == 'exists':
                self.stats['skipped'] += 1
            else:
                self.stats['failed'] += 1

    def _save_to_json(self):
        """JSON íŒŒì¼ë¡œ ì €ì¥ - all logs go to logs/ folder"""
        output_file = os.path.join(LOG_DIR, "ai_news_fulltext.json")
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(self.collected_news, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSON ì €ì¥ ì™„ë£Œ: {output_file}")

    def _print_summary(self, mode: str):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        print(f"   - ì´ ìˆ˜ì§‘: {len(self.collected_news)}ê±´")
        print(f"   - ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {self.stats['scrape_fail']}ê±´")
        
        if self.collected_news:
            avg_len = sum(n['content_length'] for n in self.collected_news) // len(self.collected_news)
            print(f"   - í‰ê·  ë³¸ë¬¸ ê¸¸ì´: {avg_len}ì")
        
        if mode in ('api', 'both'):
            print(f"   - ì‹ ê·œ ì €ì¥: {self.stats['created']}ê±´")
            print(f"   - ì¤‘ë³µ ìŠ¤í‚µ: {self.stats['skipped']}ê±´")
            print(f"   - ì „ì†¡ ì‹¤íŒ¨: {self.stats['failed']}ê±´")
        print("=" * 60)


if __name__ == "__main__":
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ìë¡œ ëª¨ë“œ ì§€ì • ê°€ëŠ¥
    # python rss_collector.py api|json|both
    mode = sys.argv[1] if len(sys.argv) > 1 else 'api'
    
    collector = FullTextRSSCollector()
    collector.run(mode=mode)
