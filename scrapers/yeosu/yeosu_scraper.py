"""
ì—¬ìˆ˜ì‹œ ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼
- ë²„ì „: v2.0
- ìµœì¢…ìˆ˜ì •: 2025-12-12
- ë‹´ë‹¹: AI Agent

íŠ¹ì´ì‚¬í•­:
- URL íŒ¨í„´: ?idx={ID}&mode=view
- í˜ì´ì§€ë„¤ì´ì…˜: ?page={N}
- ì´ë¯¸ì§€: ì²¨ë¶€íŒŒì¼ â†’ web/public/images/yeosu/ ë¡œì»¬ ì €ì¥

ë³€ê²½ì  (v2.0):
- cloudinary_uploader â†’ local_image_saver ì „í™˜
- ì´ë¯¸ì§€ ê²½ë¡œ: /images/yeosu/{filename} í˜•íƒœë¡œ ë°˜í™˜
"""

# ============================================================
# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ============================================================
import sys
import os
import time
import re
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
from urllib.parse import urljoin, unquote

# ============================================================
# 2. ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ============================================================
from playwright.sync_api import sync_playwright, Page

# ============================================================
# 3. ë¡œì»¬ ëª¨ë“ˆ
# ============================================================
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.api_client import send_article_to_server, log_to_server
from utils.scraper_utils import safe_goto, wait_and_find, safe_get_text, safe_get_attr
from utils.local_image_saver import download_and_save_locally

# ============================================================
# 4. ìƒìˆ˜ ì •ì˜
# ============================================================
REGION_CODE = 'yeosu'
REGION_NAME = 'ì—¬ìˆ˜ì‹œ'
CATEGORY_NAME = 'ì „ë‚¨'
BASE_URL = 'https://www.yeosu.go.kr'
LIST_URL = 'https://www.yeosu.go.kr/www/govt/news/release/press'

# í˜ì´ì§€ë„¤ì´ì…˜: ?page={N}
# ìƒì„¸ í˜ì´ì§€: ?idx={ê²Œì‹œë¬¼ID}&mode=view

# ëª©ë¡ í˜ì´ì§€ ë§í¬ ì…€ë ‰í„°
LIST_LINK_SELECTORS = [
    'a[href*="idx="][href*="mode=view"]',
    'a.basic_cont',
]

# ë³¸ë¬¸ í˜ì´ì§€ ì…€ë ‰í„°
CONTENT_SELECTORS = [
    '.view_cont',
    '.board_view',
    '.content_view',
    'div.view_content',
]


# ============================================================
# 5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def normalize_date(date_str: str) -> str:
    """ë‚ ì§œ ë¬¸ìì—´ì„ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
    if not date_str:
        return datetime.now().strftime('%Y-%m-%d')
    
    date_str = date_str.strip().replace('.', '-').replace('/', '-')
    try:
        # YYYY-MM-DD ë˜ëŠ” YYYY.MM.DD íŒ¨í„´
        match = re.search(r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})', date_str)
        if match:
            y, m, d = match.groups()
            return f"{y}-{int(m):02d}-{int(d):02d}"
    except:
        pass
    return datetime.now().strftime('%Y-%m-%d')


def extract_article_id(href: str) -> Optional[str]:
    """hrefì—ì„œ idx íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
    if not href:
        return None
    match = re.search(r'idx=(\d+)', href)
    return match.group(1) if match else None


# ============================================================
# 6. ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ í•¨ìˆ˜
# ============================================================
def fetch_detail(page: Page, url: str) -> Tuple[str, Optional[str], str, Optional[str]]:
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸, ì´ë¯¸ì§€, ë‚ ì§œ, ë‹´ë‹¹ë¶€ì„œë¥¼ ì¶”ì¶œ
    
    Returns:
        (ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì¸ë„¤ì¼ URL, ë‚ ì§œ, ë‹´ë‹¹ë¶€ì„œ)
    """
    if not safe_goto(page, url, timeout=20000):
        return "", None, datetime.now().strftime('%Y-%m-%d'), None
    
    time.sleep(1.5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
    
    pub_date = datetime.now().strftime('%Y-%m-%d')
    department = None
    content = ""
    thumbnail_url = None
    
    # 1. JavaScriptë¡œ ì •ë³´ ì¶”ì¶œ (ì—¬ìˆ˜ì‹œ í˜ì´ì§€ êµ¬ì¡°ì— ìµœì í™”)
    # ì „ëµ: og:description ë©”íƒ€íƒœê·¸ í™œìš© + board_view ë‚´ p íƒœê·¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    try:
        js_code = """
        () => {
            const result = {date: '', department: '', content: ''};
            
            // 1. og:description ë©”íƒ€íƒœê·¸ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ (ê°€ì¥ ì •í™•í•¨)
            const ogDesc = document.querySelector('meta[property="og:description"]');
            if (ogDesc) {
                result.content = ogDesc.getAttribute('content') || '';
            }
            
            // 2. board_view ë‚´ì—ì„œ ë©”íƒ€ì •ë³´ì™€ ì¶”ê°€ ë³¸ë¬¸ ì¶”ì¶œ
            const boardView = document.querySelector('.board_view, div.board_view');
            if (boardView) {
                // dl ë‚´ì—ì„œ ë‚ ì§œ, ë‹´ë‹¹ë¶€ì„œ ì¶”ì¶œ
                const dlInfo = boardView.querySelector('dl');
                if (dlInfo) {
                    const dts = dlInfo.querySelectorAll('dt');
                    const dds = dlInfo.querySelectorAll('dd');
                    
                    for (let i = 0; i < dts.length; i++) {
                        const dtText = dts[i]?.innerText?.trim() || '';
                        const ddText = dds[i]?.innerText?.trim() || '';
                        
                        if (dtText.includes('ë“±ë¡ì¼')) {
                            result.date = ddText;
                        }
                        if (dtText.includes('ë‹´ë‹¹ë¶€ì„œ')) {
                            result.department = ddText;
                        }
                    }
                }
                
                // og:descriptionì´ ë¹„ì–´ìˆìœ¼ë©´ p íƒœê·¸ë“¤ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ
                if (!result.content || result.content.length < 50) {
                    const paragraphs = boardView.querySelectorAll('p');
                    let pTexts = [];
                    for (const p of paragraphs) {
                        const text = p.innerText?.trim();
                        if (text && text.length > 10) {
                            pTexts.push(text);
                        }
                    }
                    if (pTexts.length > 0) {
                        result.content = pTexts.join('\\n\\n');
                    }
                }
                
                // ì—¬ì „íˆ ë¹„ì–´ìˆìœ¼ë©´ board_view ì „ì²´ì—ì„œ ì¶”ì¶œ (ë©”íƒ€ì •ë³´ ì œì™¸)
                if (!result.content || result.content.length < 50) {
                    const fullText = boardView.innerText || '';
                    // ì—°ë½ì²˜ ì´í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    const lines = fullText.split('\\n');
                    let contentLines = [];
                    let foundContact = false;
                    
                    for (const line of lines) {
                        // ì—°ë½ì²˜ ë¼ì¸ ì´í›„ë¶€í„° ìˆ˜ì§‘
                        if (line.match(/\\d{2,4}-\\d{3,4}-\\d{4}/)) {
                            foundContact = true;
                            continue;
                        }
                        if (foundContact && line.trim().length > 5) {
                            // í‘¸í„°/ë©”ë‰´ í…ìŠ¤íŠ¸ ì œì™¸
                            if (!line.includes('ì‚¬ì´íŠ¸ë§µ') && 
                                !line.includes('ê°œì¸ì •ë³´') && 
                                !line.includes('ë§Œì¡±í•˜ì‹­ë‹ˆê¹Œ') &&
                                !line.includes('ì²¨ë¶€íŒŒì¼')) {
                                contentLines.push(line.trim());
                            }
                        }
                    }
                    if (contentLines.length > 0) {
                        result.content = contentLines.join('\\n');
                    }
                }
            }
            
            return result;
        }
        """
        data = page.evaluate(js_code)
        
        if data.get('date'):
            pub_date = normalize_date(data['date'])
        if data.get('department'):
            department = data['department']
        if data.get('content'):
            content = data['content'][:5000]
    except Exception as e:
        print(f"      âš ï¸ JS ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    # Fallback: ì¼ë°˜ ì…€ë ‰í„°
    if not content or len(content) < 50:
        for sel in CONTENT_SELECTORS:
            try:
                content_elem = page.locator(sel)
                if content_elem.count() > 0:
                    text = safe_get_text(content_elem)
                    if text and len(text) > 50:
                        content = text[:5000]
                        break
            except:
                continue
    
    # 2. ì´ë¯¸ì§€ ì¶”ì¶œ (ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ì—ì„œ)
    # ì—¬ìˆ˜ì‹œ íŒ¨í„´: https://www.yeosu.go.kr/ybscript.io/common/file_download/{idx}/{file_id}/{filename}
    try:
        attach_links = page.locator('a[href*="file_download"]')
        attach_count = attach_links.count()
        print(f"      ğŸ” ì²¨ë¶€íŒŒì¼ ë§í¬ {attach_count}ê°œ ë°œê²¬")
        
        if attach_count > 0:
            for i in range(min(attach_count, 5)):
                link = attach_links.nth(i)
                href = link.get_attribute('href') or ''
                # text_content() ì§ì ‘ ì‚¬ìš©
                try:
                    link_text = link.text_content() or ''
                except:
                    link_text = safe_get_text(link) or ''
                
                print(f"      ğŸ“„ ì²¨ë¶€ #{i}: {link_text[:40]}...")
                
                # ì´ë¯¸ì§€ íŒŒì¼ì¸ì§€ í™•ì¸ (URL ë˜ëŠ” í…ìŠ¤íŠ¸ì—ì„œ)
                is_image = any(ext in link_text.lower() or ext in href.lower() 
                              for ext in ['.jpg', '.jpeg', '.png', '.gif'])
                
                if is_image and href:
                    full_url = urljoin(BASE_URL, href) if not href.startswith('http') else href
                    print(f"      ğŸ“ ì´ë¯¸ì§€ ì²¨ë¶€íŒŒì¼ ë°œê²¬!")
                    
                    # ë¡œì»¬ ì´ë¯¸ì§€ ì €ì¥ (web/public/images/yeosu/)
                    local_path = download_and_save_locally(full_url, BASE_URL, REGION_CODE)
                    if local_path:
                        thumbnail_url = local_path
                        print(f"      ğŸ’¾ ë¡œì»¬ ì €ì¥ ì™„ë£Œ: {local_path}")
                    break
    except Exception as e:
        print(f"      âš ï¸ ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    # 3. ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ (fallback)
    if not thumbnail_url:
        try:
            imgs = page.locator('.board_view img, .view_cont img, .content_view img, article img')
            for i in range(min(imgs.count(), 3)):
                src = safe_get_attr(imgs.nth(i), 'src')
                if src and not any(x in src.lower() for x in ['icon', 'btn', 'logo', 'banner', 'bg', 'bullet']):
                    full_url = urljoin(BASE_URL, src) if not src.startswith('http') else src
                    print(f"      ğŸ“· ë³¸ë¬¸ ì´ë¯¸ì§€ ë°œê²¬: {src[:50]}...")
                    local_path = download_and_save_locally(full_url, BASE_URL, REGION_CODE)
                    if local_path:
                        thumbnail_url = local_path
                        print(f"      ğŸ’¾ ë¡œì»¬ ì €ì¥ ì™„ë£Œ")
                    else:
                        thumbnail_url = full_url  # ë¡œì»¬ ì €ì¥ ì‹¤íŒ¨ ì‹œ ì›ë³¸ URL ì‚¬ìš©
                    break
        except:
            pass
    
    return content, thumbnail_url, pub_date, department


# ============================================================
# 7. ë©”ì¸ ìˆ˜ì§‘ í•¨ìˆ˜
# ============================================================
def collect_articles(days: int = 3, max_articles: int = 10, dry_run: bool = False) -> List[Dict]:
    """
    ë³´ë„ìë£Œë¥¼ ìˆ˜ì§‘í•˜ê³  ì„œë²„ë¡œ ì „ì†¡
    
    Args:
        days: ìˆ˜ì§‘í•  ê¸°ê°„ (ì¼)
        max_articles: ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜
        dry_run: í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì„œë²„ ì „ì†¡ ì•ˆí•¨)
    """
    print(f"ğŸ›ï¸ {REGION_NAME} ë³´ë„ìë£Œ ìˆ˜ì§‘ ì‹œì‘ (ìµœê·¼ {days}ì¼)")
    if not dry_run:
        log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f'{REGION_NAME} ìŠ¤í¬ë˜í¼ v1.0 ì‹œì‘', 'info')
    
    end_date = datetime.now().strftime('%Y-%m-%d')
    start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
    
    collected_count = 0
    success_count = 0
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            viewport={'width': 1280, 'height': 1024}
        )
        page = context.new_page()
        
        page_num = 1
        stop = False
        
        while page_num <= 5 and not stop and collected_count < max_articles:
            # ì—¬ìˆ˜ì‹œ í˜ì´ì§€ë„¤ì´ì…˜: ?page={N}
            list_url = f'{LIST_URL}?page={page_num}' if page_num > 1 else LIST_URL
            print(f"   ğŸ“„ í˜ì´ì§€ {page_num} ìˆ˜ì§‘ ì¤‘...")
            if not dry_run:
                log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f'í˜ì´ì§€ {page_num} íƒìƒ‰', 'info')
            
            if not safe_goto(page, list_url):
                page_num += 1
                continue
            
            time.sleep(1.5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # ëª©ë¡ ë§í¬ ì°¾ê¸°
            links = wait_and_find(page, LIST_LINK_SELECTORS, timeout=10000)
            if not links:
                print("      âš ï¸ ê¸°ì‚¬ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            link_count = links.count()
            print(f"      ğŸ“° {link_count}ê°œ ê¸°ì‚¬ ë°œê²¬")
            
            # ë§í¬ ì •ë³´ ìˆ˜ì§‘
            link_data = []
            for i in range(link_count):
                if collected_count + len(link_data) >= max_articles:
                    break
                    
                try:
                    link = links.nth(i)
                    
                    title = safe_get_text(link)
                    title = title.strip() if title else ""
                    # "ìƒˆë¡œìš´ê¸€" ì œê±°
                    title = title.replace('ìƒˆë¡œìš´ê¸€', '').strip()
                    
                    href = safe_get_attr(link, 'href')
                    
                    if not title or not href:
                        continue
                    
                    # idx= íŒŒë¼ë¯¸í„° í™•ì¸
                    if 'idx=' not in href:
                        continue
                    
                    # ìƒì„¸ í˜ì´ì§€ URL êµ¬ì„±
                    if href.startswith('http'):
                        full_url = href
                    else:
                        full_url = urljoin(BASE_URL, href)
                    
                    link_data.append({
                        'title': title,
                        'url': full_url,
                    })
                    
                except Exception as e:
                    continue
            
            # ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ ë° ì „ì†¡
            for item in link_data:
                if collected_count >= max_articles:
                    break
                    
                title = item['title']
                full_url = item['url']
                
                print(f"      ğŸ“° {title[:40]}...")
                if not dry_run:
                    log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f"ìˆ˜ì§‘ ì¤‘: {title[:20]}...", 'info')
                
                content, thumbnail_url, pub_date, department = fetch_detail(page, full_url)
                
                # ë‚ ì§œ í•„í„°ë§
                if pub_date < start_date:
                    stop = True
                    break
                
                if not content:
                    content = f"ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì›ë³¸ ë§í¬: {full_url}"
                
                article_data = {
                    'title': title,
                    'content': content,
                    'published_at': f"{pub_date}T09:00:00+09:00",
                    'original_link': full_url,
                    'source': REGION_NAME,
                    'category': CATEGORY_NAME,
                    'region': REGION_CODE,
                    'thumbnail_url': thumbnail_url,
                }
                
                if dry_run:
                    print(f"         [DRY-RUN] ì œëª©: {title[:30]}...")
                    print(f"         [DRY-RUN] ë‚ ì§œ: {pub_date}")
                    print(f"         [DRY-RUN] ë³¸ë¬¸: {len(content)}ì")
                    print(f"         [DRY-RUN] ì´ë¯¸ì§€: {'ìˆìŒ' if thumbnail_url else 'ì—†ìŒ'}")
                    success_count += 1
                else:
                    # ì„œë²„ë¡œ ì „ì†¡
                    result = send_article_to_server(article_data)
                    
                    if result.get('status') == 'created':
                        success_count += 1
                        img_status = "âœ“ì´ë¯¸ì§€" if thumbnail_url else "âœ—ì´ë¯¸ì§€"
                        print(f"         âœ… ì €ì¥ ì™„ë£Œ ({img_status})")
                        log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f"ì €ì¥ ì™„ë£Œ: {title[:15]}...", 'success')
                    elif result.get('status') == 'exists':
                        print(f"         â© ì´ë¯¸ ì¡´ì¬")
                
                collected_count += 1
                time.sleep(0.5)  # Rate limiting
            
            page_num += 1
            if stop:
                print("      ğŸ›‘ ìˆ˜ì§‘ ê¸°ê°„ ì´ˆê³¼, ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            time.sleep(1)
        
        browser.close()
    
    final_msg = f"ìˆ˜ì§‘ ì™„ë£Œ (ì´ {collected_count}ê°œ, ì‹ ê·œ {success_count}ê°œ)"
    print(f"âœ… {final_msg}")
    if not dry_run:
        log_to_server(REGION_CODE, 'ì„±ê³µ', final_msg, 'success')
    
    return []


# ============================================================
# 8. CLI ì§„ì…ì 
# ============================================================
def main():
    import argparse
    parser = argparse.ArgumentParser(description=f'{REGION_NAME} ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼ v2.0')
    parser.add_argument('--days', type=int, default=3, help='ìˆ˜ì§‘ ê¸°ê°„ (ì¼)')
    parser.add_argument('--max-articles', type=int, default=10, help='ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜')
    parser.add_argument('--dry-run', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì„œë²„ ì „ì†¡ ì•ˆí•¨)')
    args = parser.parse_args()
    
    collect_articles(args.days, args.max_articles, args.dry_run)


if __name__ == "__main__":
    main()
