# -*- coding: utf-8 -*-
"""
ê´‘ì£¼êµìœ¡ì²­ ê¸°ì¡´ ê¸°ì‚¬ ë³¸ë¬¸ ì—…ë°ì´íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- DBì— ì €ì¥ëœ ê´‘ì£¼êµìœ¡ì²­ ê¸°ì‚¬ì˜ source_urlë¡œ ë‹¤ì‹œ ì ‘ì†
- ë³¸ë¬¸ì„ ì¬ì¶”ì¶œí•˜ì—¬ ì—…ë°ì´íŠ¸
"""

import os
import sys
import time
import re

# ìƒìœ„ ë””ë ‰í† ë¦¬ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from playwright.sync_api import sync_playwright
from supabase import create_client

# Supabase ì„¤ì •
SUPABASE_URL = os.getenv("SUPABASE_URL", "https://oaciprcdcdegwufydjhv.supabase.co")
SUPABASE_KEY = os.getenv("SUPABASE_KEY", "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im9hY2lwcmNkY2RlZ3d1Znlkamh2Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3MzMxNjg3NjgsImV4cCI6MjA0ODc0NDc2OH0.z8EfkVKPBSr1ot4HfBWmHWNBTMBqJCkcKLdKOGguJ_w")

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)


def extract_content(page, title: str) -> str:
    """JavaScriptë¡œ ë³¸ë¬¸ ì¶”ì¶œ (ì œëª© ì˜ì—­ ì œì™¸)"""
    content = ""
    
    try:
        content = page.evaluate("""() => {
            const boardPress = document.querySelector('div.board_press');
            if (!boardPress) return '';
            
            const clone = boardPress.cloneNode(true);
            
            const excludeSelectors = [
                'div.view_top',
                'div.inquiry',
                'div.writer',
                'div.file_list',
                'div.view_bottom',
                '.btn_wrap',
            ];
            
            excludeSelectors.forEach(sel => {
                const els = clone.querySelectorAll(sel);
                els.forEach(el => el.remove());
            });
            
            return clone.textContent?.trim() || '';
        }""")
    except Exception as e:
        print(f"      âš ï¸ JS ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    # ì •ì œ
    if content:
        noise_patterns = [
            r'HOME\s*',
            r'ë³´ë„/í•´ëª…ìë£Œ\s*',
            r'ì˜¤ëŠ˜ì˜ ë³´ë„/í•´ëª…ìë£Œë€ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤\.?\s*',
            r'ë³´ë„ìë£Œ\s*(?=[^\w]|$)',
            r'ë§Œì¡±ë„\s*ì¡°ì‚¬.*',
            r'ì €ì‘ê¶Œ.*',
            r'COPYRIGHT.*',
            r'ëª©ë¡\s*ì´ì „ê¸€\s*ë‹¤ìŒê¸€.*',
            r'ìë£Œë¬¸ì˜\s*:.*',
        ]
        for pattern in noise_patterns:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE)
        
        # ì œëª© ì¤‘ë³µ ì œê±°
        if title and content.startswith(title):
            content = content[len(title):].strip()
        
        content = re.sub(r'\n{3,}', '\n\n', content)
        content = re.sub(r' {2,}', ' ', content)
        content = content.strip()[:5000]
    
    return content


def main():
    print("ğŸ”„ ê´‘ì£¼êµìœ¡ì²­ ê¸°ì¡´ ê¸°ì‚¬ ë³¸ë¬¸ ì—…ë°ì´íŠ¸ ì‹œì‘")
    
    # 1. DBì—ì„œ ê´‘ì£¼êµìœ¡ì²­ ê¸°ì‚¬ ì¡°íšŒ
    result = supabase.table('posts').select('id, title, source_url, content').eq('category', 'ê´‘ì£¼êµìœ¡ì²­').execute()
    posts = result.data or []
    
    print(f"   ğŸ“Š ì´ {len(posts)}ê°œ ê¸°ì‚¬ ë°œê²¬")
    
    if not posts:
        print("   âš ï¸ ì—…ë°ì´íŠ¸í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    updated = 0
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        for i, post in enumerate(posts):
            post_id = post['id']
            title = post['title']
            source_url = post.get('source_url', '')
            old_content = post.get('content', '')
            
            # ì´ë¯¸ ì •ìƒì ì¸ ë³¸ë¬¸ì¸ì§€ í™•ì¸ (ë©”ë‰´ í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€)
            if 'ë³´ë„/í•´ëª…ìë£Œë€ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤' not in old_content and 'HOME' not in old_content[:50]:
                print(f"   [{i+1}/{len(posts)}] âœ… ì´ë¯¸ ì •ìƒ: {title[:30]}...")
                continue
            
            if not source_url:
                print(f"   [{i+1}/{len(posts)}] âš ï¸ source_url ì—†ìŒ: {title[:30]}...")
                continue
            
            print(f"   [{i+1}/{len(posts)}] ğŸ”„ ì¬ìˆ˜ì§‘: {title[:30]}...")
            
            try:
                page.goto(source_url, timeout=30000)
                time.sleep(2)
                
                new_content = extract_content(page, title)
                
                if new_content and len(new_content) > 100:
                    # DB ì—…ë°ì´íŠ¸
                    supabase.table('posts').update({'content': new_content}).eq('id', post_id).execute()
                    updated += 1
                    print(f"      âœ… ì—…ë°ì´íŠ¸ ì™„ë£Œ ({len(new_content)}ì)")
                else:
                    print(f"      âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë„ˆë¬´ ì§§ìŒ")
            
            except Exception as e:
                print(f"      âŒ ì˜¤ë¥˜: {e}")
            
            time.sleep(1)  # ìš”ì²­ ê°„ê²©
        
        browser.close()
    
    print(f"\nğŸ‰ ì™„ë£Œ: {updated}/{len(posts)}ê°œ ê¸°ì‚¬ ì—…ë°ì´íŠ¸ë¨")


if __name__ == "__main__":
    main()
