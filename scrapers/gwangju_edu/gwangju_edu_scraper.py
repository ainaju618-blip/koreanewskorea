"""
ê´‘ì£¼ê´‘ì—­ì‹œêµìœ¡ì²­ ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼
- ë²„ì „: v4.2
- ìµœì¢…ìˆ˜ì •: 2025-12-14
- ë‹´ë‹¹: AI Agent

ë³€ê²½ì  (v4.2):
- ë©”íƒ€ë°ì´í„° ì œê±° íŒ¨í„´ ëŒ€í­ ê°•í™” (ì¶”ì²œìˆ˜, ì²¨ë¶€íŒŒì¼, ì‘ì„±ì, ì‚¬ì§„ ìº¡ì…˜ ë“±)
- ì¤„ë°”ê¿ˆ ì •ë¦¬ ê°œì„  (2ì¤„ ì´ìƒ â†’ 1ì¤„, ì—°ì† ê³µë°± ì •ë¦¬)

ì‚¬ì´íŠ¸ íŠ¹ì§•:
- URL ê²½ë¡œì— /v5/ì™€ /v4/ê°€ í˜¼ìš©ë¨ (ì´ë¯¸ì§€ëŠ” v4 ê²½ë¡œ)
- ì´ë¯¸ì§€ í•«ë§í¬ í—ˆìš© (ì§ì ‘ ì ‘ê·¼ ê°€ëŠ¥)
- ë³¸ë¬¸ì´ ì—¬ëŸ¬ ê°œì˜ generic ìš”ì†Œë¡œ ë¶„ë¦¬ë˜ì–´ ìˆìŒ
- bo_table=0201 = ë³´ë„ìë£Œ ê²Œì‹œíŒ ì‹ë³„ì
"""

# ============================================================
# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ============================================================
import sys
import os
import time
import re
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
from urllib.parse import urljoin

# ============================================================
# 2. ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ============================================================
from playwright.sync_api import sync_playwright, Page

# ============================================================
# 3. ë¡œì»¬ ëª¨ë“ˆ
# ============================================================
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.api_client import send_article_to_server, log_to_server, ensure_server_running
from utils.scraper_utils import safe_goto, wait_and_find, safe_get_text, safe_get_attr, clean_article_content, extract_subtitle
from utils.cloudinary_uploader import download_and_upload_image
from utils.category_detector import detect_category

# ============================================================
# 4. ìƒìˆ˜ ì •ì˜
# ============================================================
REGION_CODE = 'gwangju_edu'
REGION_NAME = 'ê´‘ì£¼ê´‘ì—­ì‹œêµìœ¡ì²­'
CATEGORY_NAME = 'ê´‘ì£¼'
BASE_URL = 'https://enews.gen.go.kr'
LIST_URL = 'https://enews.gen.go.kr/v5/?sid=25'

# ìƒì„¸ í˜ì´ì§€ URL íŒ¨í„´: https://enews.gen.go.kr/v5/?sid=25&wbb=md:view;uid:{ID};
# í˜ì´ì§€ë„¤ì´ì…˜: &page={N}
# ì´ë¯¸ì§€ ì§ì ‘ ì ‘ê·¼ URL: https://enews.gen.go.kr/v4/decoboard/data/file/0201/{íŒŒì¼ëª…}

# ëª©ë¡ í˜ì´ì§€ ì…€ë ‰í„°
LIST_ROW_SELECTORS = [
    'form ul li',
    'ul > li',
]

# ============================================================
# 5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def normalize_date(date_str: str) -> str:
    """ë‚ ì§œ ë¬¸ìì—´ì„ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
    if not date_str:
        return datetime.now().strftime('%Y-%m-%d')
    
    date_str = date_str.strip().replace('.', '-').replace('/', '-')
    try:
        match = re.search(r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})', date_str)
        if match:
            y, m, d = match.groups()
            return f"{y}-{int(m):02d}-{int(d):02d}"
    except:
        pass
    return datetime.now().strftime('%Y-%m-%d')


def extract_id_from_href(href: str) -> Optional[str]:
    """hrefì—ì„œ uid ì¶”ì¶œ: uid:(\\d+);"""
    if not href:
        return None
    match = re.search(r'uid[=:](\d+)', href)
    if match:
        return match.group(1)
    return None


def convert_image_url(view_url: str) -> Optional[str]:
    """
    ì´ë¯¸ì§€ ë·°ì–´ URLì„ ì§ì ‘ ì ‘ê·¼ URLë¡œ ë³€í™˜
    view_image.php?fn={íŒŒì¼ëª…}&bo_table=0201 
    â†’ https://enews.gen.go.kr/v4/decoboard/data/file/0201/{íŒŒì¼ëª…}
    """
    if not view_url:
        return None
    
    # fn íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    match = re.search(r'fn=([^&]+)', view_url)
    if match:
        filename = match.group(1)
        return f"https://enews.gen.go.kr/v4/decoboard/data/file/0201/{filename}"
    
    return None


# ============================================================
# 6. ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ í•¨ìˆ˜
# ============================================================
def fetch_detail(page: Page, url: str) -> Tuple[str, Optional[str], str, Optional[str]]:
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸, ì´ë¯¸ì§€, ë‚ ì§œ, ë‹´ë‹¹ë¶€ì„œë¥¼ ì¶”ì¶œ
    
    Returns:
        (ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì¸ë„¤ì¼ URL, ë‚ ì§œ, ë‹´ë‹¹ë¶€ì„œ)
    """
    if not safe_goto(page, url, timeout=20000):
        return "", None, datetime.now().strftime('%Y-%m-%d'), None
    
    time.sleep(1)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
    
    content = ""
    thumbnail_url = None
    pub_date = datetime.now().strftime('%Y-%m-%d')
    department = None
    
    # 1. ë©”íƒ€ ì •ë³´ ì¶”ì¶œ (.view-info í™œìš©)
    try:
        # ë‚ ì§œ ì¶”ì¶œ (ë³¸ë¬¸ í…ìŠ¤íŠ¸ íŒ¨í„´ ë˜ëŠ” ë©”íƒ€ ì˜ì—­)
        page_text = page.content()
        date_match = re.search(r'(\d{4}-\d{2}-\d{2})', page_text)
        if date_match:
            pub_date = date_match.group(1)
        
        # ê¸°ê´€ëª… (ë‹´ë‹¹ë¶€ì„œ) ì¶”ì¶œ
        # í…ìŠ¤íŠ¸ "ê¸°ê´€ëª… :" ì°¾ê¸°
        dept_match = re.search(r'ê¸°ê´€ëª…\s*[:]\s*([^\s<]+)', page.inner_text('body'))
        if dept_match:
            department = dept_match.group(1).strip()
        else:
            # fallback: .view-info ì²« ë²ˆì§¸ í•­ëª©
            info_item = page.locator('ul.view-info li').first
            if info_item.count() > 0:
                department = safe_get_text(info_item)

    except Exception as e:
        print(f"      âš ï¸ ë©”íƒ€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    # 2. ë³¸ë¬¸ ì¶”ì¶œ (.view-contents í´ë˜ìŠ¤ í™œìš©)
    try:
        # .view-contentsê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        content_elem = page.locator('.view-contents')
        if content_elem.count() > 0:
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ (innerText ì‚¬ìš©)
            content = safe_get_text(content_elem)
        else:
            # Fallback: ì²¨ë¶€íŒŒì¼ ì˜ì—­(.file-list) ì´í›„ì˜ ëª¨ë“  p, div íƒœê·¸ í…ìŠ¤íŠ¸
            js_code = """
            () => {
                const fileSection = document.querySelector('.file-list') || document.querySelector('[class*="file"]');
                let text = '';
                if (fileSection) {
                    let next = fileSection.nextElementSibling;
                    while(next) {
                        if (next.tagName === 'P' || next.tagName === 'DIV' || next.tagName === 'SPAN') {
                            text += (next.innerText || '') + '\\n';
                        }
                        next = next.nextElementSibling;
                    }
                }
                
                // ë§Œì•½ ìœ„ ë°©ë²•ìœ¼ë¡œ ì‹¤íŒ¨í•˜ë©´, 'ê¸°ê´€ëª… :' ì´í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                if (text.length < 50) {
                    const bodyText = document.body.innerText;
                    const startIdx = bodyText.indexOf('ê¸°ê´€ëª… :');
                    if (startIdx > -1) {
                        const contentArea = bodyText.substring(startIdx);
                        const endIdx = contentArea.indexOf('ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨');
                        if (endIdx > -1) {
                            text = contentArea.substring(0, endIdx);
                        }
                    }
                }
                return text;
            }
            """
            content = page.evaluate(js_code)
            
        if content:
            # ê³µí†µ ë³¸ë¬¸ ì •ì œ í•¨ìˆ˜ ì‚¬ìš© (v4.3)
            content = clean_article_content(content)
            
    except Exception as e:
        print(f"      âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    # 3. ì´ë¯¸ì§€ ì¶”ì¶œ (ê°œì„ ëœ ë¡œì§)
    try:
        # ë°©ë²• A: .view-contents img
        imgs = page.locator('.view-contents img')
        for i in range(imgs.count()):
            src = safe_get_attr(imgs.nth(i), 'src')
            if src and not any(x in src.lower() for x in ['icon', 'btn', 'logo']):
                img_url = urljoin(BASE_URL, src)
                # v5 -> v5/ ê²½ë¡œ ë¬¸ì œ í•´ê²° (í•„ìš”ì‹œ)
                if '/v5/../' in img_url:
                    img_url = img_url.replace('/v5/../', '/')
                
                cloudinary_url = download_and_upload_image(img_url, BASE_URL, folder=REGION_CODE)
                if cloudinary_url:
                    thumbnail_url = cloudinary_url
                    break
        
        # ë°©ë²• B: view_image.php ë§í¬ (ì´ë¯¸ì§€ ë·°ì–´)
        if not thumbnail_url:
            view_links = page.locator('a[href*="view_image.php"]')
            for i in range(view_links.count()):
                href = safe_get_attr(view_links.nth(i), 'href')
                direct_url = convert_image_url(href)
                if direct_url:
                    cloudinary_url = download_and_upload_image(direct_url, BASE_URL, folder=REGION_CODE)
                    if cloudinary_url:
                        thumbnail_url = cloudinary_url
                        break
                        
        # ë°©ë²• C: ì²¨ë¶€íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ì°¾ê¸° (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
        if not thumbnail_url:
            attach_links = page.locator('a[href*="javascript:file_download"]')
            for i in range(min(attach_links.count(), 5)):
                link = attach_links.nth(i)
                title_text = safe_get_text(link) or ''
                if any(ext in title_text.lower() for ext in ['.jpg', '.png', '.jpeg']):
                    onclick = safe_get_attr(link, 'href') or ''
                    match = re.search(r"file_download\(['\"]?(\d+)['\"]?\)", onclick)
                    if match:
                        file_uid = match.group(1)
                        # v5 ê²½ë¡œ ì‚¬ìš©
                        download_url = f"https://enews.gen.go.kr/v5/decoboard/download.php?uid={file_uid}"
                        cloudinary_url = download_and_upload_image(download_url, BASE_URL, folder=REGION_CODE)
                        if cloudinary_url:
                            thumbnail_url = cloudinary_url
                            break
                            
    except Exception as e:
        print(f"      âš ï¸ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    return content, thumbnail_url, pub_date, department


# ============================================================
# 7. ëª©ë¡ í˜ì´ì§€ íŒŒì‹± í•¨ìˆ˜
# ============================================================
def parse_list_page(page: Page) -> List[Dict]:
    """
    ëª©ë¡ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
    
    Returns:
        [{'title': ..., 'url': ..., 'date': ..., 'id': ...}, ...]
    """
    articles = []
    
    try:
        # JavaScriptë¡œ ëª©ë¡ íŒŒì‹± (íŠ¹ìˆ˜í•œ DOM êµ¬ì¡° ëŒ€ì‘)
        js_code = """
        () => {
            const items = [];
            // li > a êµ¬ì¡°ì—ì„œ ì¶”ì¶œ (uid:{ID}; íŒ¨í„´)
            const links = document.querySelectorAll('a[href*="wbb=md:view;uid:"]');
            
            for (const link of links) {
                const href = link.getAttribute('href') || '';
                const uidMatch = href.match(/uid[=:](\\d+)/);
                if (!uidMatch) continue;
                
                const uid = uidMatch[1];
                
                // ë¶€ëª¨ li ë˜ëŠ” ë§í¬ ìì²´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                const li = link.closest('li');
                let title = '';
                let date = '';
                
                // generic ìš”ì†Œë“¤ì—ì„œ ì •ë³´ ì¶”ì¶œ
                const generics = li ? li.querySelectorAll('.generic, span, div') : link.querySelectorAll('*');
                for (const g of generics) {
                    const text = (g.textContent || '').trim();
                    
                    // ë‚ ì§œ íŒ¨í„´
                    if (/^\\d{4}-\\d{2}-\\d{2}$/.test(text)) {
                        date = text;
                        continue;
                    }
                    
                    // ì¡°íšŒìˆ˜ íŒ¨í„´ ìŠ¤í‚µ
                    if (text.includes('ì¡°íšŒìˆ˜')) continue;
                    
                    // ì œëª© (ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸)
                    if (text.length > title.length && text.length < 200 && !text.includes('ì¡°íšŒìˆ˜')) {
                        title = text;
                    }
                }
                
                // ì œëª©ì´ ì—†ìœ¼ë©´ ë§í¬ í…ìŠ¤íŠ¸ ì‚¬ìš©
                if (!title) {
                    title = link.textContent?.trim() || '';
                }
                
                if (title && uid) {
                    items.push({
                        id: uid,
                        title: title.substring(0, 200),
                        date: date || '',
                        href: href
                    });
                }
            }
            
            return items;
        }
        """
        raw_items = page.evaluate(js_code)
        
        for item in raw_items:
            full_url = urljoin(BASE_URL, f"/v5/?sid=25&wbb=md:view;uid:{item['id']};")
            articles.append({
                'id': item['id'],
                'title': item['title'],
                'date': item['date'],
                'url': full_url
            })
            
    except Exception as e:
        print(f"      âš ï¸ ëª©ë¡ íŒŒì‹± ì‹¤íŒ¨: {e}")
    
    return articles


# ============================================================
# 8. ê²€ì¦ í•¨ìˆ˜
# ============================================================
def validate_article(article_data: Dict) -> bool:
    """
    ê¸°ì‚¬ ë°ì´í„° ê²€ì¦
    
    Returns:
        True if valid, False otherwise
    """
    title = article_data.get('title', '')
    content = article_data.get('content', '')
    
    # ì œëª© ê²€ì¦
    if len(title) < 5:
        print(f"         âš ï¸ ê²€ì¦ ì‹¤íŒ¨: ì œëª© ë„ˆë¬´ ì§§ìŒ ({len(title)}ì)")
        return False
    
    # ë³¸ë¬¸ ê²€ì¦
    if len(content) < 50:
        print(f"         âš ï¸ ê²€ì¦ ì‹¤íŒ¨: ë³¸ë¬¸ ë„ˆë¬´ ì§§ìŒ ({len(content)}ì)")
        return False
    
    # ì—ëŸ¬ ë©”ì‹œì§€ í¬í•¨ ì—¬ë¶€
    if "ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in content:
        print(f"         âš ï¸ ê²€ì¦ ì‹¤íŒ¨: ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
        return False
    
    return True


# ============================================================
# 9. ë©”ì¸ ìˆ˜ì§‘ í•¨ìˆ˜
# ============================================================
def collect_articles(days: int = 3, max_articles: int = 10, start_date: str = None, end_date: str = None) -> List[Dict]:
    """
    ë³´ë„ìë£Œë¥¼ ìˆ˜ì§‘í•˜ê³  ì„œë²„ë¡œ ì „ì†¡

    Args:
        days: ìˆ˜ì§‘í•  ê¸°ê°„ (ì¼)
        max_articles: ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜
        start_date: ìˆ˜ì§‘ ì‹œì‘ì¼ (YYYY-MM-DD)
        end_date: ìˆ˜ì§‘ ì¢…ë£Œì¼ (YYYY-MM-DD)
    """
    print(f"ğŸ›ï¸ {REGION_NAME} ë³´ë„ìë£Œ ìˆ˜ì§‘ ì‹œì‘ (ìµœê·¼ {days}ì¼, ìµœëŒ€ {max_articles}ê°œ)
    

    # Ensure dev server is running before starting

    if not ensure_server_running():

        print("[ERROR] Dev server could not be started. Aborting.")

        return []
")
    log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f'{REGION_NAME} ìŠ¤í¬ë˜í¼ v4.0 ì‹œì‘', 'info')

    if not end_date:
        end_date = datetime.now().strftime('%Y-%m-%d')
    if not start_date:
        start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
    
    collected_count = 0
    success_count = 0
    image_count = 0
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            viewport={'width': 1280, 'height': 1024}
        )
        page = context.new_page()
        
        page_num = 1
        stop = False
        all_links = []
        
        # Phase 1: Collect - ëª©ë¡ì—ì„œ ë§í¬ ìˆ˜ì§‘
        while page_num <= 3 and not stop and len(all_links) < max_articles:
            # í˜ì´ì§€ë„¤ì´ì…˜: &page={N}
            if page_num == 1:
                list_url = LIST_URL
            else:
                list_url = f'{LIST_URL}&wbb=md%3Alist%3B&page={page_num}'
            
            print(f"   ğŸ“„ í˜ì´ì§€ {page_num} ìˆ˜ì§‘ ì¤‘...")
            log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f'í˜ì´ì§€ {page_num} íƒìƒ‰', 'info')
            
            if not safe_goto(page, list_url):
                page_num += 1
                continue
            
            time.sleep(1.5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            articles = parse_list_page(page)
            print(f"      ğŸ“° {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            
            for article in articles:
                if len(all_links) >= max_articles:
                    break
                
                n_date = article['date']
                if n_date:
                    if n_date < start_date:
                        stop = True
                        break
                    if n_date > end_date:
                        continue
                
                all_links.append(article)
            
            page_num += 1
            if stop:
                print("      ğŸ›‘ ìˆ˜ì§‘ ê¸°ê°„ ì´ˆê³¼, ìˆ˜ì§‘ ì¢…ë£Œ")
                break
            
            time.sleep(1)
        
        print(f"   ğŸ“‹ ì´ {len(all_links)}ê°œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # Phase 2: Visit - ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸ ë° ì „ì†¡
        for item in all_links[:max_articles]:
            title = item['title']
            full_url = item['url']
            n_date = item['date'] or datetime.now().strftime('%Y-%m-%d')
            
            print(f"      ğŸ“° {title[:35]}... ({n_date})")
            log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f"ìˆ˜ì§‘ ì¤‘: {title[:20]}...", 'info')
            
            content, thumbnail_url, detail_date, department = fetch_detail(page, full_url)
            
            # ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•œ ë‚ ì§œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if detail_date != datetime.now().strftime('%Y-%m-%d'):
                n_date = detail_date
            
            if not content:
                content = f"ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì›ë³¸ ë§í¬: {full_url}"

            # ë¶€ì œëª© ì¶”ì¶œ
            subtitle, content = extract_subtitle(content, title)

            # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
            cat_code, cat_name = detect_category(title, content)

            article_data = {
                'title': title,
                'subtitle': subtitle,
                'content': content,
                'published_at': f"{n_date}T09:00:00+09:00",
                'original_link': full_url,
                'source': department or REGION_NAME,
                'category': cat_name,
                'region': REGION_CODE,
                'thumbnail_url': thumbnail_url,
            }
            
            # ê²€ì¦
            if not validate_article(article_data):
                continue
            
            # ì„œë²„ë¡œ ì „ì†¡
            result = send_article_to_server(article_data)
            collected_count += 1
            
            if result.get('status') == 'created':
                success_count += 1
                if thumbnail_url:
                    image_count += 1
                img_status = "âœ“ì´ë¯¸ì§€" if thumbnail_url else "âœ—ì´ë¯¸ì§€"
                print(f"         âœ… ì €ì¥ ì™„ë£Œ ({img_status})")
                log_to_server(REGION_CODE, 'ì‹¤í–‰ì¤‘', f"ì €ì¥ ì™„ë£Œ: {title[:15]}...", 'success')
            elif result.get('status') == 'exists':
                print(f"         â© ì´ë¯¸ ì¡´ì¬")
            
            time.sleep(0.5)  # Rate limiting
        
        browser.close()
    
    final_msg = f"ìˆ˜ì§‘ ì™„ë£Œ (ì´ {collected_count}ê°œ, ì‹ ê·œ {success_count}ê°œ, ì´ë¯¸ì§€ {image_count}ê°œ)"
    print(f"âœ… {final_msg}")
    log_to_server(REGION_CODE, 'ì„±ê³µ', final_msg, 'success')
    
    return []


# ============================================================
# 10. CLI ì§„ì…ì 
# ============================================================
def main():
    import argparse
    parser = argparse.ArgumentParser(description=f'{REGION_NAME} ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼ v4.0')
    parser.add_argument('--days', type=int, default=3, help='ìˆ˜ì§‘ ê¸°ê°„ (ì¼)')
    parser.add_argument('--max-articles', type=int, default=10, help='ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜')
    parser.add_argument('--dry-run', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì„œë²„ ì „ì†¡ ì•ˆí•¨)')
    # bot-service.ts í˜¸í™˜ ì¸ì (í•„ìˆ˜)
    parser.add_argument('--start-date', type=str, default=None, help='ìˆ˜ì§‘ ì‹œì‘ì¼ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, default=None, help='ìˆ˜ì§‘ ì¢…ë£Œì¼ (YYYY-MM-DD)')
    args = parser.parse_args()

    collect_articles(
        days=args.days,
        max_articles=args.max_articles,
        start_date=args.start_date,
        end_date=args.end_date
    )


if __name__ == "__main__":
    main()
