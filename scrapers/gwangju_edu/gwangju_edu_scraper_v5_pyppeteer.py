# -*- coding: utf-8 -*-
"""
ê´‘ì£¼êµìœ¡ì²­ ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼ v5.0 (Pyppeteer)
- Pythonìš© Puppeteer í¬íŠ¸ ì‚¬ìš©
- ì›ë³¸ JS ì½”ë“œì™€ ìœ ì‚¬í•œ API
"""

import sys, os, time, re, argparse, asyncio, json
from datetime import datetime
from typing import List, Dict, Tuple, Optional

# Pyppeteer import
from pyppeteer import launch

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.api_client import send_article_to_server
from utils.cloudinary_uploader import download_and_upload_image

# ===== ìƒìˆ˜ ì •ì˜ =====
REGION_CODE = 'kedu'
REGION_NAME = 'ê´‘ì£¼ì‹œêµìœ¡ì²­'
CATEGORY_NAME = 'ê´‘ì£¼êµìœ¡ì²­'
BASE_URL = 'https://enews.gen.go.kr'
LIST_URL = 'https://enews.gen.go.kr/v5/?sid=25'


def parse_args():
    """CLI ì˜µì…˜ íŒŒì„œ"""
    parser = argparse.ArgumentParser(
        description='ê´‘ì£¼ê´‘ì—­ì‹œêµìœ¡ì²­ ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼ v5.0 (Pyppeteer)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python gwangju_edu_scraper_v5_pyppeteer.py
  python gwangju_edu_scraper_v5_pyppeteer.py --exact-date 2025-12-11
  python gwangju_edu_scraper_v5_pyppeteer.py --max-articles 3 --dry-run
        """
    )
    parser.add_argument('--start-date', help='ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)')
    parser.add_argument('--end-date', help='ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)')
    parser.add_argument('--exact-date', help='íŠ¹ì • ë‚ ì§œë§Œ ìˆ˜ì§‘ (YYYY-MM-DD)')
    parser.add_argument('--max-pages', type=int, default=3, help='ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 3)')
    parser.add_argument('--max-articles', type=int, default=12, help='ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸: 12)')
    parser.add_argument('--dry-run', action='store_true', help='DB ì €ì¥ ì—†ì´ í…ŒìŠ¤íŠ¸ë§Œ')
    parser.add_argument('--output', help='ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ')
    return parser.parse_args()


def is_date_in_range(date_str: str, start_date: str = None, 
                     end_date: str = None, exact_date: str = None) -> bool:
    """ë‚ ì§œê°€ ì§€ì •ëœ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸"""
    if not date_str:
        return True
    
    if exact_date:
        return date_str == exact_date
    
    if start_date and date_str < start_date:
        return False
    if end_date and date_str > end_date:
        return False
    
    return True


def validate_article(article_data: Dict) -> Tuple[bool, str]:
    """ê¸°ì‚¬ ë°ì´í„° ê²€ì¦"""
    if not article_data.get('title') or len(article_data['title']) < 5:
        return False, "âŒ ì œëª© ë„ˆë¬´ ì§§ìŒ"
    content = article_data.get('content', '')
    if not content or len(content) < 30:
        return False, f"âŒ ë³¸ë¬¸ ë¶€ì¡± ({len(content)}ì)"
    return True, "âœ… ê²€ì¦ í†µê³¼"


async def collect_list_with_metadata(page) -> List[Dict]:
    """ëª©ë¡ì—ì„œ ë‚ ì§œ/ì¡°íšŒìˆ˜ë„ í•¨ê»˜ ì¶”ì¶œ (Pyppeteer - ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)"""
    try:
        items = await page.evaluate('''() => {
            const results = [];
            // ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°: a íƒœê·¸ê°€ ì§ì ‘ ê¸°ì‚¬ ë§í¬ (ul/li êµ¬ì¡° ì•„ë‹˜)
            const links = document.querySelectorAll("a[href*='wbb=md:view;uid:']");
            
            links.forEach(link => {
                const href = link.getAttribute('href') || '';
                const uidMatch = href.match(/uid:(\\d+)/);
                if (!uidMatch) return;
                
                // ì œëª© ì¶”ì¶œ (ë§í¬ ë‚´ë¶€ í…ìŠ¤íŠ¸ì—ì„œ)
                let title = '';
                const divs = link.querySelectorAll('div');
                if (divs.length > 0) {
                    title = divs[0].textContent?.trim() || '';
                } else {
                    title = link.textContent?.trim() || '';
                }
                
                // ë§í¬ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                const allText = link.textContent || '';
                const dateMatch = allText.match(/(\\d{4}-\\d{2}-\\d{2})/);
                const date = dateMatch ? dateMatch[1] : '';
                
                // ì¡°íšŒìˆ˜ ì¶”ì¶œ
                const viewsMatch = allText.match(/ì¡°íšŒ\\s*(\\d+)/);
                const views = viewsMatch ? viewsMatch[1] : '';
                
                // ì¤‘ë³µ ì œê±° (ê°™ì€ uid ìˆìœ¼ë©´ ìŠ¤í‚µ)
                if (!results.some(r => r.uid === uidMatch[1])) {
                    results.push({
                        uid: uidMatch[1],
                        title: title.substring(0, 100),
                        date: date,
                        views: views,
                        href: href
                    });
                }
            });
            
            return results;
        }''')
        return items if items else []
    except Exception as e:
        print(f"   âš ï¸ ëª©ë¡ ì¶”ì¶œ ì—ëŸ¬: {e}")
        return []


async def fetch_detail(page, url: str) -> Tuple[str, str, Optional[str]]:
    """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª©, ë³¸ë¬¸, ì´ë¯¸ì§€ ì¶”ì¶œ (Pyppeteer)"""
    try:
        await page.goto(url, {'waitUntil': 'networkidle2', 'timeout': 30000})
    except Exception as e:
        print(f"   âš ï¸ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {str(e)[:30]}")
        return "", "", None
    
    await asyncio.sleep(2)
    
    title = ""
    content = ""
    thumbnail_url = None
    
    try:
        # 1. ì œëª© ì¶”ì¶œ
        try:
            title = await page.evaluate('''() => {
                const viewTop = document.querySelector('div.view_top');
                if (!viewTop) return '';
                
                const text = viewTop.textContent || '';
                const lines = text.split('\\n').map(l => l.trim()).filter(l => l.length > 5);
                
                for (const line of lines) {
                    if (!line.includes('ì‘ì„±ì¼:') && 
                        !line.includes('ì‘ì„±ì:') && 
                        !line.includes('ê¸°ê´€ëª…') &&
                        !line.includes('ìë£Œë¬¸ì˜') &&
                        !line.includes('ì¡°íšŒìˆ˜') &&
                        !line.includes('ì¶”ì²œìˆ˜') &&
                        !line.includes('ë“±ë¡ì¼')) {
                        return line;
                    }
                }
                return lines[0] || '';
            }''')
        except:
            title = ""

        # 2. ë³¸ë¬¸ ì¶”ì¶œ
        try:
            content = await page.evaluate('''() => {
                const boardPress = document.querySelector('div.board_press');
                if (!boardPress) return '';
                
                const clone = boardPress.cloneNode(true);
                
                const excludeSelectors = [
                    'div.view_top', 'div.inquiry', 'div.writer',
                    'div.file_list', 'div.view_bottom', '.btn_wrap',
                ];
                
                excludeSelectors.forEach(sel => {
                    const els = clone.querySelectorAll(sel);
                    els.forEach(el => el.remove());
                });
                
                return clone.textContent?.trim() || '';
            }''')
        except:
            content = ""
        
        # ë³¸ë¬¸ ì •ì œ
        if content:
            noise_patterns = [
                r'HOME\s*', r'ë³´ë„/í•´ëª…ìë£Œ\s*', r'ë§Œì¡±ë„\s*ì¡°ì‚¬.*',
                r'ì €ì‘ê¶Œ.*', r'COPYRIGHT.*', r'ëª©ë¡\s*ì´ì „ê¸€\s*ë‹¤ìŒê¸€.*',
            ]
            for pattern in noise_patterns:
                content = re.sub(pattern, '', content, flags=re.IGNORECASE)
            
            if title and content.startswith(title):
                content = content[len(title):].strip()
            
            content = re.sub(r'\n{3,}', '\n\n', content)
            content = re.sub(r' {2,}', ' ', content)
            content = content.strip()[:5000]

        # 3. ì´ë¯¸ì§€ ì¶”ì¶œ
        DOWNLOAD_BASE = 'https://enews.gen.go.kr/v5/decoboard/download.php?uid='
        
        try:
            js_result = await page.evaluate('''() => {
                const links = Array.from(document.querySelectorAll('a'));
                for (const a of links) {
                    const href = a.getAttribute('href') || '';
                    const text = (a.textContent || '').toLowerCase();
                    if (href.includes('file_download') && 
                        (text.includes('.jpg') || text.includes('.jpeg') || text.includes('.png'))) {
                        const match = href.match(/file_download\\(['"]?(\\d+)['"]?\\)/);
                        if (match) {
                            return { uid: match[1], text: a.textContent.trim() };
                        }
                    }
                }
                return null;
            }''')
            
            if js_result and js_result.get('uid'):
                download_url = DOWNLOAD_BASE + js_result['uid']
                print(f"      ğŸ“· ì´ë¯¸ì§€ ë°œê²¬: {js_result['text'][:30]}...")
                cloud_url = download_and_upload_image(download_url, BASE_URL, folder="gwangju_edu")
                if cloud_url and 'cloudinary' in cloud_url:
                    thumbnail_url = cloud_url
                    print(f"      âœ… ì´ë¯¸ì§€ ì—…ë¡œë“œ ì™„ë£Œ")
                else:
                    thumbnail_url = download_url
        except Exception as img_err:
            print(f"      âš ï¸ ì´ë¯¸ì§€ ì—ëŸ¬: {str(img_err)[:30]}")
        
    except Exception as e:
        print(f"   âš ï¸ ìƒì„¸ íŒŒì‹± ì—ëŸ¬: {str(e)[:50]}")
    
    return title, content, thumbnail_url


async def collect_articles(args):
    """ë©”ì¸ ìˆ˜ì§‘ í•¨ìˆ˜ (async)"""
    start_time = time.time()
    
    print(f"ğŸ›ï¸ {REGION_NAME} ë³´ë„ìë£Œ ìˆ˜ì§‘ ì‹œì‘ (v5.0 Pyppeteer)")
    print(f"   ì„¤ì •: ìµœëŒ€ {args.max_pages}í˜ì´ì§€, ìµœëŒ€ {args.max_articles}ê°œ")
    
    if args.exact_date:
        print(f"   ğŸ“… í•„í„°: {args.exact_date} ë‚ ì§œë§Œ")
    elif args.start_date or args.end_date:
        print(f"   ğŸ“… í•„í„°: {args.start_date or 'ì‹œì‘'} ~ {args.end_date or 'ì¢…ë£Œ'}")
    
    date_filter = {
        'start_date': args.start_date,
        'end_date': args.end_date,
        'exact_date': args.exact_date
    }
    
    all_items = []
    consecutive_empty = 0
    results = []
    
    # Pyppeteer ë¸Œë¼ìš°ì € ì‹œì‘
    browser = await launch(
        headless=True,
        args=['--no-sandbox', '--disable-setuid-sandbox']
    )
    page = await browser.newPage()
    await page.setViewport({'width': 1280, 'height': 1024})
    
    try:
        # 1ë‹¨ê³„: ëª©ë¡ ìˆ˜ì§‘ (í˜ì´ì§€ë„¤ì´ì…˜)
        for page_num in range(1, args.max_pages + 1):
            page_url = f"{LIST_URL}&wbb=md%3Alist%3B&page={page_num}"
            print(f"\nğŸ“„ í˜ì´ì§€ {page_num} ìŠ¤ìº” ì¤‘...")
            
            try:
                await page.goto(page_url, {'waitUntil': 'networkidle2', 'timeout': 30000})
            except Exception as e:
                print(f"   âŒ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {str(e)[:30]}")
                break
            
            await asyncio.sleep(2)
            
            items = await collect_list_with_metadata(page)
            print(f"   ğŸ”— ë°œê²¬: {len(items)}ê°œ")
            
            # ë‚ ì§œ í•„í„° ì ìš©
            filtered = [i for i in items if is_date_in_range(i['date'], **date_filter)]
            
            if not filtered:
                consecutive_empty += 1
                print(f"   âš ï¸ í•„í„° í†µê³¼ í•­ëª© ì—†ìŒ (ì—°ì† {consecutive_empty}íšŒ)")
                
                if consecutive_empty >= 3 and (args.start_date or args.end_date or args.exact_date):
                    print("   ğŸ“Œ ë‚ ì§œ ë²”ìœ„ ì´ˆê³¼ë¡œ íŒë‹¨, ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
            else:
                consecutive_empty = 0
                all_items.extend(filtered)
                print(f"   âœ… {len(filtered)}ê°œ í•„í„° í†µê³¼ (ëˆ„ì : {len(all_items)}ê°œ)")
            
            if len(all_items) >= args.max_articles:
                print(f"   ğŸ“Œ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜({args.max_articles}) ë„ë‹¬, ìˆ˜ì§‘ ì¤‘ë‹¨")
                break
            
            await asyncio.sleep(0.5)
        
        # 2ë‹¨ê³„: ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘
        print(f"\nğŸ“° ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘ (ì´ {min(len(all_items), args.max_articles)}ê°œ)")
        
        success_count = 0
        
        for idx, item in enumerate(all_items[:args.max_articles]):
            url = BASE_URL + '/v5/' + item['href'] if item['href'].startswith('?') else item['href']
            if not url.startswith('http'):
                url = BASE_URL + '/v5/' + url
            
            print(f"\n   ğŸ” [{idx+1}] {item['title'][:25]}... ({item['date']})")
            
            real_title, content, thumbnail_url = await fetch_detail(page, url)
            
            # ì œëª© ê²°ì •
            if real_title and len(real_title) > 10 and 'í™ë³´ê´€' not in real_title:
                final_title = real_title
            else:
                final_title = item['title']
            
            published_at = f"{item['date'] or datetime.now().strftime('%Y-%m-%d')}T09:00:00+09:00"
            
            article_data = {
                'title': final_title,
                'content': content,
                'published_at': published_at,
                'original_link': url,
                'source': REGION_NAME,
                'category': CATEGORY_NAME,
                'region': REGION_CODE,
                'thumbnail_url': thumbnail_url,
            }
            
            is_valid, msg = validate_article(article_data)
            print(f"      {msg}")
            
            if is_valid:
                results.append(article_data)
                
                if args.dry_run:
                    print(f"      ğŸ§ª [DRY-RUN] DB ì €ì¥ ìŠ¤í‚µ")
                    success_count += 1
                else:
                    result = send_article_to_server(article_data)
                    if result and result.get('status') == 'created':
                        print(f"      âœ… [DB ì €ì¥ ì™„ë£Œ]")
                        success_count += 1
                    else:
                        status = result.get('status', 'unknown') if result else 'no response'
                        print(f"      âš ï¸ [DB ê²°ê³¼] {status}")
            
            await asyncio.sleep(1)
        
    finally:
        await browser.close()
    
    elapsed_time = time.time() - start_time
    
    print(f"\nğŸ‰ ì‘ì—… ì™„ë£Œ: {success_count}ê±´ ì„±ê³µ")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    
    # ê²°ê³¼ JSON ì €ì¥
    if args.output:
        output_data = {
            'scraper_version': 'v5.0_pyppeteer',
            'scraped_at': datetime.now().isoformat(),
            'elapsed_seconds': round(elapsed_time, 2),
            'total_count': len(results),
            'success_count': success_count,
            'articles': results
        }
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {args.output}")
    
    return results


def main():
    args = parse_args()
    # Python 3.10+ í˜¸í™˜
    asyncio.run(collect_articles(args))


if __name__ == "__main__":
    main()
