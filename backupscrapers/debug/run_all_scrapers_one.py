"""
ì‹¤ì œ ìŠ¤í¬ë˜í¼ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ê° ì‹œêµ°ì—ì„œ 1ê°œ ê¸°ì‚¬ì”© ì¶”ì¶œ
API ì„œë²„ ì—†ì´ ë¡œì»¬ì—ì„œ ê²°ê³¼ë§Œ ì¶œë ¥
"""
import sys
import os
from urllib.parse import urljoin

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from playwright.sync_api import sync_playwright

# ì‹¤ì œ ìŠ¤í¬ë˜í¼ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜¨ ì„¤ì •
SCRAPERS = [
    {'code': 'gwangju', 'name': 'ê´‘ì£¼ì‹œ', 'file': 'gwangju_scraper'},
    {'code': 'gwangju_edu', 'name': 'ê´‘ì£¼êµìœ¡ì²­', 'file': 'gwangju_edu_scraper'},
    {'code': 'jeonnam', 'name': 'ì „ë‚¨ë„', 'file': 'jeonnam_scraper'},
    {'code': 'jeonnam_edu', 'name': 'ì „ë‚¨êµìœ¡ì²­', 'file': 'jeonnam_edu_scraper'},
    {'code': 'mokpo', 'name': 'ëª©í¬ì‹œ', 'file': 'mokpo_scraper'},
    {'code': 'yeosu', 'name': 'ì—¬ìˆ˜ì‹œ', 'file': 'yeosu_scraper'},
    {'code': 'suncheon', 'name': 'ìˆœì²œì‹œ', 'file': 'suncheon_scraper'},
    {'code': 'naju', 'name': 'ë‚˜ì£¼ì‹œ', 'file': 'naju_scraper'},
    {'code': 'gwangyang', 'name': 'ê´‘ì–‘ì‹œ', 'file': 'gwangyang_scraper'},
    {'code': 'damyang', 'name': 'ë‹´ì–‘êµ°', 'file': 'damyang_scraper'},
    {'code': 'gokseong', 'name': 'ê³¡ì„±êµ°', 'file': 'gokseong_scraper'},
    {'code': 'gurye', 'name': 'êµ¬ë¡€êµ°', 'file': 'gurye_scraper'},
    {'code': 'goheung', 'name': 'ê³ í¥êµ°', 'file': 'goheung_scraper'},
    {'code': 'boseong', 'name': 'ë³´ì„±êµ°', 'file': 'boseong_scraper'},
    {'code': 'hwasun', 'name': 'í™”ìˆœêµ°', 'file': 'hwasun_scraper'},
    {'code': 'jangheung', 'name': 'ì¥í¥êµ°', 'file': 'jangheung_scraper'},
    {'code': 'gangjin', 'name': 'ê°•ì§„êµ°', 'file': 'gangjin_scraper'},
    {'code': 'haenam', 'name': 'í•´ë‚¨êµ°', 'file': 'haenam_scraper'},
    {'code': 'yeongam', 'name': 'ì˜ì•”êµ°', 'file': 'yeongam_scraper'},
    {'code': 'muan', 'name': 'ë¬´ì•ˆêµ°', 'file': 'muan_scraper'},
    {'code': 'hampyeong', 'name': 'í•¨í‰êµ°', 'file': 'hampyeong_scraper'},
    {'code': 'yeonggwang', 'name': 'ì˜ê´‘êµ°', 'file': 'yeonggwang_scraper'},
    {'code': 'jangseong', 'name': 'ì¥ì„±êµ°', 'file': 'jangseong_scraper'},
    {'code': 'wando', 'name': 'ì™„ë„êµ°', 'file': 'wando_scraper'},
    {'code': 'jindo', 'name': 'ì§„ë„êµ°', 'file': 'jindo_scraper'},
    {'code': 'shinan', 'name': 'ì‹ ì•ˆêµ°', 'file': 'shinan_scraper'},
]

def extract_one_article(scraper_module, context):
    """ìŠ¤í¬ë˜í¼ ëª¨ë“ˆì—ì„œ 1ê°œ ê¸°ì‚¬ ì¶”ì¶œ"""
    result = {
        'title': '',
        'content': '',
        'image': '',
        'link': '',
        'status': 'âŒ'
    }
    
    try:
        # ëª¨ë“ˆì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        list_url = getattr(scraper_module, 'LIST_URL', '')
        base_url = getattr(scraper_module, 'BASE_URL', '')
        list_selectors = getattr(scraper_module, 'LIST_SELECTORS', [])
        content_selectors = getattr(scraper_module, 'CONTENT_SELECTORS', [])
        
        if not list_url:
            result['status'] = 'âŒ URLì—†ìŒ'
            return result
        
        page = context.new_page()
        page.goto(list_url, timeout=20000, wait_until='domcontentloaded')
        page.wait_for_timeout(2000)
        
        # ë¦¬ìŠ¤íŠ¸ì—ì„œ ì²« ë²ˆì§¸ ê¸°ì‚¬ ì°¾ê¸°
        link_elem = None
        for sel in list_selectors:
            items = page.locator(sel)
            if items.count() > 0:
                link_elem = items.first
                break
        
        if not link_elem:
            result['status'] = 'âŒ ë¦¬ìŠ¤íŠ¸ì‹¤íŒ¨'
            page.close()
            return result
        
        result['title'] = link_elem.inner_text().strip()[:40]
        href = link_elem.get_attribute('href')
        result['link'] = urljoin(base_url, href) if href else ''
        
        # ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
        try:
            with page.expect_navigation(timeout=10000):
                link_elem.click()
            page.wait_for_timeout(2000)
        except:
            if result['link']:
                page.goto(result['link'], timeout=15000)
                page.wait_for_timeout(2000)
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        for sel in content_selectors:
            elem = page.locator(sel)
            if elem.count() > 0:
                text = elem.first.inner_text()[:100].replace('\n', ' ')
                if text:
                    result['content'] = text[:50] + '...'
                    break
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ
        for sel in content_selectors:
            imgs = page.locator(f'{sel} img')
            if imgs.count() > 0:
                src = imgs.first.get_attribute('src')
                if src and 'icon' not in src.lower() and 'logo' not in src.lower():
                    result['image'] = urljoin(base_url, src)[:60] + '...'
                    break
        
        result['status'] = 'âœ…'
        page.close()
        return result
        
    except Exception as e:
        result['status'] = f'âŒ {str(e)[:15]}'
        return result


def main():
    print("=" * 100)
    print("ğŸ” ì‹¤ì œ ìŠ¤í¬ë˜í¼ë¡œ ê° ì‹œêµ° 1ê°œ ê¸°ì‚¬ ì¶”ì¶œ")
    print("=" * 100)
    
    results = []
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )
        
        for scraper_info in SCRAPERS:
            print(f"\nğŸ“ {scraper_info['name']} ({scraper_info['code']})...")
            
            try:
                # ìŠ¤í¬ë˜í¼ ëª¨ë“ˆ import
                module = __import__(scraper_info['file'])
                result = extract_one_article(module, context)
                result['name'] = scraper_info['name']
                results.append(result)
                
                if result['status'] == 'âœ…':
                    print(f"   âœ… ì œëª©: {result['title']}")
                    print(f"   ğŸ“ ë³¸ë¬¸: {result['content']}")
                    print(f"   ğŸ–¼ï¸ ì´ë¯¸ì§€: {result['image'] or 'ì—†ìŒ'}")
                else:
                    print(f"   {result['status']}")
                    
            except Exception as e:
                print(f"   âŒ ëª¨ë“ˆ ì˜¤ë¥˜: {e}")
                results.append({'name': scraper_info['name'], 'status': f'âŒ ëª¨ë“ˆì˜¤ë¥˜'})
        
        browser.close()
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 100)
    print("ğŸ“Š ìµœì¢… ìš”ì•½")
    print("=" * 100)
    
    success = [r for r in results if r['status'] == 'âœ…']
    failed = [r for r in results if r['status'] != 'âœ…']
    
    print(f"\nâœ… ì„±ê³µ: {len(success)}ê°œ")
    for r in success:
        print(f"   - {r['name']}: {r['title'][:25]}...")
    
    print(f"\nâŒ ì‹¤íŒ¨: {len(failed)}ê°œ")
    for r in failed:
        print(f"   - {r['name']}: {r['status']}")


if __name__ == "__main__":
    main()
