"""
ê´‘ì£¼ê´‘ì—­ì‹œ ìŠ¤í¬ëž˜í¼ í…ŒìŠ¤íŠ¸ - ìµœê·¼ 3ê°œ ê¸°ì‚¬ë§Œ ì¶”ì¶œ (v3 - ë””ë²„ê·¸ ê°•í™”)
"""

import sys
import os
import time
from datetime import datetime
from urllib.parse import urljoin

from playwright.sync_api import sync_playwright

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from utils.api_client import send_article_to_server

REGION_CODE = 'gwangju'
REGION_NAME = 'ê´‘ì£¼ê´‘ì—­ì‹œ'
CATEGORY_NAME = 'ê´‘ì£¼'
BASE_URL = 'https://www.gwangju.go.kr'
LIST_URL = 'https://www.gwangju.go.kr/boardList.do?boardId=BD_0000000022'

MAX_ARTICLES = 3

def fetch_detail(page, url):
    """ìƒì„¸ íŽ˜ì´ì§€ì—ì„œ ë³¸ë¬¸ê³¼ ì¸ë„¤ì¼ ì¶”ì¶œ"""
    try:
        page.goto(url, wait_until='domcontentloaded', timeout=20000)
        time.sleep(2)
        
        content = ""
        content_elem = page.locator('div.board_view_body, div.view_content, div.board_view, .view_body, .content')
        if content_elem.count() > 0:
            content = content_elem.first.inner_text()[:5000]
        
        thumbnail_url = None
        img = page.locator('div.board_view_body img, div.view_content img, .view_body img').first
        if img.count() > 0:
            src = img.get_attribute('src')
            if src:
                thumbnail_url = urljoin(BASE_URL, src)
        
        return content, thumbnail_url
    except Exception as e:
        print(f"      âŒ ìƒì„¸íŽ˜ì´ì§€ ì˜¤ë¥˜: {e}")
        return "", None


def main():
    print(f"ðŸ›ï¸ {REGION_NAME} ë³´ë„ìžë£Œ í…ŒìŠ¤íŠ¸ ìˆ˜ì§‘ (ìµœê·¼ {MAX_ARTICLES}ê°œ) - v3")
    print("=" * 60)
    
    articles = []
    collected_urls = []  # ë¨¼ì € URLë§Œ ìˆ˜ì§‘
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            viewport={'width': 1280, 'height': 1024}
        )
        page = context.new_page()
        
        print(f"   ðŸ“„ ëª©ë¡ íŽ˜ì´ì§€ ì ‘ì† ì¤‘...")
        try:
            page.goto(LIST_URL, wait_until='domcontentloaded', timeout=30000)
            time.sleep(3)
        except Exception as e:
            print(f"      âŒ ëª©ë¡ íŽ˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
            browser.close()
            return
        
        # íŽ˜ì´ì§€ HTMLì—ì„œ href ì§ì ‘ ì¶”ì¶œ
        html = page.content()
        
        # boardView.do ë§í¬ë“¤ ì°¾ê¸° (ì •ê·œì‹)
        import re
        pattern = r'href="(/boardView\.do\?[^"]+boardId=BD_0000000022[^"]*)"[^>]*>([^<]+)<'
        matches = re.findall(pattern, html)
        
        if not matches:
            # ëŒ€ì²´ íŒ¨í„´ ì‹œë„
            pattern2 = r'href="(/boardView\.do\?[^"]+)"'
            matches_alt = re.findall(pattern2, html)
            print(f"      ëŒ€ì²´ íŒ¨í„´ìœ¼ë¡œ {len(matches_alt)}ê°œ ë°œê²¬")
            for m in matches_alt[:5]:
                print(f"         - {m[:80]}...")
        else:
            print(f"      ðŸ“° ì •ê·œì‹ìœ¼ë¡œ {len(matches)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            for i, (href, title) in enumerate(matches[:MAX_ARTICLES]):
                full_url = urljoin(BASE_URL, href)
                collected_urls.append((title.strip(), full_url))
                print(f"         [{i+1}] {title.strip()[:40]}...")
        
        # ìˆ˜ì§‘í•œ URLë“¤ë¡œ ìƒì„¸íŽ˜ì´ì§€ ì ‘ê·¼
        for i, (title, full_url) in enumerate(collected_urls):
            print(f"\n   [{i+1}/{len(collected_urls)}] ìƒì„¸íŽ˜ì´ì§€ ì ‘ê·¼: {title[:40]}...")
            
            content, thumbnail_url = fetch_detail(page, full_url)
            
            print(f"         ë³¸ë¬¸ ê¸¸ì´: {len(content)}ìž")
            if thumbnail_url:
                print(f"         ðŸ–¼ï¸ ì¸ë„¤ì¼: {thumbnail_url[:50]}...")
            
            # ì¤‘ë³µ ìš°íšŒ
            timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
            force_unique_link = f"{full_url}&_test={timestamp}_{i}"
            n_date = datetime.now().strftime('%Y-%m-%d')
            
            articles.append({
                'title': f"[í…ŒìŠ¤íŠ¸] {title}",
                'content': content or f"ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì›ë³¸: {full_url}",
                'published_at': f"{n_date}T09:00:00+09:00",
                'original_link': force_unique_link,
                'source': REGION_NAME,
                'category': CATEGORY_NAME,
                'region': REGION_CODE,
                'thumbnail_url': thumbnail_url,
            })
        
        browser.close()
    
    print(f"\n{'=' * 60}")
    print(f"âœ… ì´ {len(articles)}ê°œ ê¸°ì‚¬ ì¤€ë¹„ ì™„ë£Œ")
    
    if not articles:
        print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ì„œë²„ë¡œ ì „ì†¡
    print("\nðŸ“¤ ì„œë²„ë¡œ ì „ì†¡ ì¤‘...")
    stats = {'created': 0, 'skipped': 0, 'failed': 0}
    
    for article in articles:
        print(f"   â†’ {article['title'][:40]}...")
        result = send_article_to_server(article)
        if result.get('status') == 'created':
            stats['created'] += 1
        elif result.get('status') == 'exists':
            stats['skipped'] += 1
        else:
            stats['failed'] += 1
    
    print(f"\n{'=' * 60}")
    print(f"ðŸ“Š ìµœì¢… ê²°ê³¼: ì‹ ê·œ {stats['created']}, ì¤‘ë³µ {stats['skipped']}, ì‹¤íŒ¨ {stats['failed']}")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    main()
