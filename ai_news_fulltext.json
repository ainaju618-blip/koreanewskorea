[
  {
    "source": "TechCrunch",
    "title": "Ex-Googler’s Yoodli triples valuation to $300M+ with AI built to assist, not replace, people",
    "link": "https://techcrunch.com/2025/12/05/ex-googlers-yoodli-triples-valuation-to-300m-with-ai-built-to-assist-not-replace-people/",
    "published_at": "2025-12-05T23:43:58+00:00",
    "content": "[AI 번역 준비중]\n\nYoodli, an AI-powered communication training startup, has reached a valuation of more than $300 million — more than triple its level six months ago — as it builds technology meant to assist people rather than replace them with machines.\n\nThe valuation increase follows Yoodli’s $40 million Series B round, led by WestBridge Capital with participation from Neotribe and Madrona. It comes after a$13.7 million Series A roundannounced in May, bringing the startup’s total funding to nearly $60 million.\n\nAs AI tools spread into workplaces and fuel fears of automation, Yoodli positions itself differently. The four-year-old, Seattle-based startup uses AI to run simulated scenarios — including sales calls, leadership coaching, interviews, and feedback sessions — and provides users with structured, repeatable practice to improve their speaking skills.\n\nVarun Puri (pictured above, right), who previously worked at Google’s X division and handled special projects for Sergey Brin, co-founded Yoodli with former Apple engineer Esha Joshi (pictured above, left) in 2021. He became aware of communication challenges after moving to the U.S. at 18 and seeing how difficulty expressing ideas or speaking confidently affected students and young professionals from countries such as India — himself included — Puri said in an interview.\n\nInitially, Yoodli was meant to help people practice public speaking — a skill two out of three people struggle with, Puri told TechCrunch, citing internal data. However, the startup soon saw users turning to the platform for interview preparation, sales pitches, and difficult conversations. That shift pushed Yoodli from a consumer-focused product to enterprise training, and it now offers AI role-plays and experiential learning tools for go-to-market enablement, partner certification, and management coaching.\n\n“In the old world, companies would be training people using static, long-form content or passive videos that we’d all watch at 4x-5x speed, just to get the thing done,” said Puri. “But that doesn’t actually mean you’ve learned it.”\n\nCompanies including Google, Snowflake, Databricks, RingCentral, and Sandler Sales use Yoodli for employee or partner training. The startup also sells its platform to coaching firms such as Franklin Covey and LHH, which can tailor the system to their own methodology and training frameworks, Puri stated. He added that the tool is not designed to replace human coaches but to keep a human in the loop delivering personalized guidance.\n\n“I philosophically believe that AI can get you, let’s call it from a zero to an eight or a zero to nine,” said Puri. “But the pure essence of who you are and how you show up, and your authenticity and vulnerability that a human gives you feedback on will always exist.”\n\nThe platform works with multiple large language models, meaning users can run it with models such as Google’s Gemini or OpenAI’s GPT based on their preference. Enterprises can also embed it into their existing software, or users can access it directly through a web browser. The AI supports most major languages, including Korean, Japanese, French, Canadian French, and a list of Indian languages.\n\nYoodli does not offer a dedicated mobile app, a decision Puri said was made to avoid adding extra steps for users during training sessions.\n\nPuri did not disclose how many people use the platform but said most of Yoodli’s revenue now comes from enterprise customers. He added that between the Series A and B rounds, Yoodli saw a 50% increase in the number of role-plays run on the platform and in the total time users spent practicing. The startup also said it grew its average recurring revenue by 900% over the last 12 months, though it did not provide specific figures.\n\nYoodli had not planned to raise more funding so soon after its last round but saw unanticipated investor interest, with WestBridge leading the latest raise, Puri said. He noted that strong performance metrics, key customers, and senior hires helped attract investors. The startup has recently hired former Tableau and Salesforce executive Josh Vitello as chief revenue officer (CRO), former Remitly CFO Andy Larson as CFO, and former Tableau chief product officer (CPO) Padmashree Koneti as CPO.\n\nYoodli is not alone in the market for AI-based communication tools, but Puri told TechCrunch the startup differentiates itself through deep customization and a focus on specific training verticals, allowing companies to tailor the system to their use cases and coaching methods.\n\nThe Seattle-headquartered startup has about 40 employees. Puri said the latest funding will be used to expand Yoodli’s AI coaching, analytics, and personalization tools, and to grow its presence in enterprise learning and professional development. The company also plans to hire across product, AI research, and customer success, and to expand into markets in the Asia-Pacific region while deepening its footprint in the U.S.",
    "content_length": 4971,
    "thumbnail_url": "https://techcrunch.com/wp-content/uploads/2025/12/yoodli-founders.jpg?resize=1200,800",
    "category": "AI",
    "ai_summary": ""
  },
  {
    "source": "TechCrunch",
    "title": "Sources: AI synthetic research startup Aaru raised a Series A at a $1B ‘headline’ valuation",
    "link": "https://techcrunch.com/2025/12/05/ai-synthetic-research-startup-aaru-raised-a-series-a-at-a-1b-headline-valuation/",
    "published_at": "2025-12-05T23:38:59+00:00",
    "content": "[AI 번역 준비중]\n\nAaru, a startup that provides near-instant customer research by using AI to simulate user behavior, has raised a Series A led by Redpoint Ventures, according to three people familiar with the deal.\n\nThe funding round included different valuation tiers, these people said. Although some equity was acquired at a $1 billion valuation, a lower valuation for other investors resulted in a blended valuation below $1 billion, according to people familiar with the deal. Multi-tier valuations within the same round are an unusual mechanism in venture capital, but investors say they are becoming increasingly common for desirable AI startups in the current market. This approach allows the company to report a higher “headline” valuation while simultaneously offering better terms to specific investors.\n\nAaru and Redpoint Ventures didn’t respond to a request for comment.\n\nThe exact round size couldn’t be learned, but one person said that it is above $50 million. Another source said that the startup is growing quickly, but its annual recurring revenue (ARR) is still below $10 million.\n\nAaru was founded in March 2024 by Cameron Fink, Ned Koh, and John Kessler, according to their LinkedIn profiles.\n\nThe startup’s prediction model generates thousands of AI agents that simulate human behavior using public and proprietary data. Aaru replaces traditional market research methods, which generally include surveys and focus groups, by using agents to predict how groups in specific demographics or geographies will respond to future events.\n\nThe company’s customer partners include Accenture,EY,Interpublic Group, and political campaigns. Last year, Aaru AI’s polling methodology accurately predicted the outcome of the New York Democratic primary, according toreporting by Semafor.\n\nAaru competes with other social simulation startups, includingCulturePulseandSimile, as well as startups that apply AI to query humans about their product preferences, such as Listen Labs,Keplar, and Outset.\n\nThe startup raised an undisclosed amount of seed and pre-seed capital from investors, including A*, Abstract Ventures, General Catalyst, Accenture Ventures, and Z Fellows, according to people familiar with the deal and PitchBook data.",
    "content_length": 2236,
    "thumbnail_url": "https://techcrunch.com/wp-content/uploads/2022/08/GettyImages-1224590022.jpg?resize=1200,632",
    "category": "AI",
    "ai_summary": ""
  },
  {
    "source": "TechCrunch",
    "title": "AWS needs you to believe in AI agents",
    "link": "https://techcrunch.com/video/aws-needs-you-to-believe-in-ai-agents/",
    "published_at": "2025-12-05T22:00:00+00:00",
    "content": "[AI 번역 준비중]\n\nAWS announced a wave of new AI agent tools at re:Invent 2025, but can Amazon actually catch up to the AI leaders? While the cloud giant is betting big on enterprise AI with its third-gen chip and database discounts that got developers cheering, it’s still fighting to prove it can compete beyond infrastructure.  This week […]",
    "content_length": 339,
    "thumbnail_url": "https://techcrunch.com/wp-content/uploads/2024/02/GettyImages-1424498694.jpg?resize=1200,800",
    "category": "AI",
    "ai_summary": ""
  },
  {
    "source": "TechCrunch",
    "title": "Meta acquires AI device startup Limitless",
    "link": "https://techcrunch.com/2025/12/05/meta-acquires-ai-device-startup-limitless/",
    "published_at": "2025-12-05T21:02:13+00:00",
    "content": "[AI 번역 준비중]\n\nLimitless, the AI startup formerly known as Rewind, has been acquired by Meta, the company announced Friday on itswebsite. The company, which made an AI-powered pendant to record your conversations, says it will no longer sell its hardware devices and will maintain support for its existing customers for a year.\n\nCustomers will no longer have to pay a subscription fee and will be moved to the Unlimited Plan for the time being. Other functionality will be wound down, including its non-pendant software “Rewind,” which recorded users’desktop activityand turned it into a searchable record.\n\nThe startup, founded byBrett BejcekandDan Siroker,the co-founder and former chief executive ofOptimizely, pivoted to become an AI device maker last year, offering its Limitless pendant for $99. The wearable could attach to your shirt like a wireless mic or be worn like a necklace. The device is one of several AI hardware devices on the market, including another (not very well-received) AI pendant known asFriend.\n\nAccording to Limitless’ announcement, the company shares in Meta’s vision to “bring personal superintelligence to everyone,” which includes building AI-enabled wearables. (Meta is focused for now on AR/AI glasses, like its Ray-Ban Meta and Oakley Meta, and its in-lens AI glasses, the Meta Ray-Ban Display.) Limitless said it will help bring that vision to life — which likely means supporting Meta’s existing products, not helping Meta add an AI pendant to its lineup.\n\nThe company hinted that the increased competition in the market made it difficult for it to compete, especially as thelarger players like OpenAIand Meta are developing their ownhardwaredevices, too.\n\n“When we started Limitless five years ago, the world was very different,” wrote Siroker in the announcement. “AI was a pipe dream to many. Hardware startups were considered unfundable, and a business that did both AI and hardware would have been considered ludicrous. But today is different. The world has changed. We’re no longer working on a weird fringe idea. We’re building a future that now seems inevitable. We’re not alone.”\n\nMeta shared the following statement with TechCrunch via email: “We’re excited that Limitless will be joining Meta to help accelerate our work to build AI-enabled wearables.” The tech giant didn’t share further information about its plans, beyond noting that the team will work in the wearables organization of Reality Labs.\n\nLimitless will offer its customers a way toexporttheir data, the company said, or users can choose todeletetheir data from within the app.\n\nThe startup hadraisedmore than $33 million in funding from investors, including a16z, First Round Capital, and NEA.\n\nUpdated after publication with Meta’s comment.",
    "content_length": 2755,
    "thumbnail_url": "https://techcrunch.com/wp-content/uploads/2024/04/Limitless-pendant.jpg?resize=1200,675",
    "category": "AI",
    "ai_summary": ""
  },
  {
    "source": "TechCrunch",
    "title": "ChatGPT’s user growth has slowed, report finds",
    "link": "https://techcrunch.com/2025/12/05/chatgpts-user-growth-has-slowed-report-finds/",
    "published_at": "2025-12-05T20:06:03+00:00",
    "content": "[AI 번역 준비중]\n\nChatGPT’s growth is starting to taper off, according to new data from market intelligence firmSensor Tower. Today, the OpenAI-owned AI chatbot remains the leader in the space, accounting for 50% of global downloads on mobile devices and 55% of the global monthly active users. However, Google’s Gemini has begun to outpace ChatGPT in terms of download growth, growth of monthly active users, and growth of time spent in app, the firm found.\n\nOver time, that increased pace of adoption could help Gemini narrow the gap with ChatGPT. That’s something OpenAI is now worried about, as its recent“code red” memoindicated. The missive, penned by OpenAI CEO Sam Altman, instructed staff to focus on improving the company’s AI products, particularly in areas like personalization, reliability, image generation, and more.\n\nWhen looking at the recent data, it’s clear the race is not over yet: Both ChatGPT and Gemini continue to see sizable growth.\n\nChatGPT has seen its global monthly active users climb by 180% year-over-year as of November 2025, while Gemini’s monthly active users are up 170%.\n\nBut the new data indicates that ChatGPT’s global monthly active users only grew by around 6% from August to November, to reach roughly 810 million. (The monthly active user numbers in the above chart are rounded, the firm notes.) This figure could suggest the AI chatbot is nearing market saturation, Sensor Tower says.\n\nMeanwhile, Google Gemini’s global monthly active users jumped by around 30% during the same time frame, as the release of its new image generation model, Nano Banana, drove increased adoption.\n\nIn addition, the report noted that around two times more U.S. Android users now engage with Gemini directly through the Android operating system compared with using the standalone Gemini mobile app. This could provide Google with a competitive advantage in the global market, where Android dominates, as it means Gemini isn’t constrained to only being used within a mobile app or web interface.\n\nGemini is also increasing its share of the overall AI chatbot market when compared across all top apps like ChatGPT, Copilot, Claude, Perplexity, and Grok. Over the past seven months (May-November 2025), Gemini increased its share of global monthly active users by three percentage points, the firm estimates.\n\nBut ChatGPT saw its share of global monthly active users drop three percentage points over the past four months (August-November 2025), by comparison.\n\nChallenges from Perplexity and Claude may also be impacting ChatGPT, as both rivals saw triple-digit growth for their respective chatbots in 2025, with the former up 370% year-over-year, and the latter up 190%.\n\nChatGPT also saw its global downloads grow by 85% year-over-year as of November, but this lagged the overall cohort’s average growth of 110%.\n\nPerplexity and Gemini saw the largest growth, up 215% and 190% year-over-year, respectively.\n\nFinally, Gemini app users’ time spent in the app has more than doubled over the past few months, Sensor Tower said. As of November, Gemini users were spending 11 minutes per day in the app, up 120% from March. This is likely due to thepopularityof its image generation model,Nano Banana, in September.\n\nChatGPT’s users’ daily time spent only increased by 6% during the same time frame. Plus, ChatGPT users’ time spent was down 10% in November, compared with July.\n\nWhile the current data indicates Google could be catching up with the market leader, much of its recent gains have to do with the success of Nano Banana. OpenAI could speed up growth again with the release of its own new products, if they make a similar impact.",
    "content_length": 3653,
    "thumbnail_url": "https://techcrunch.com/wp-content/uploads/2025/10/sora-app-GettyImages-2240278671.jpeg?resize=1200,800",
    "category": "AI",
    "ai_summary": ""
  },
  {
    "source": "Wired AI",
    "title": "WIRED Roundup: DOGE Isn’t Dead, Facebook Dating Is Real, and Amazon’s AI Ambitions",
    "link": "https://www.wired.com/story/uncanny-valley-podcast-wired-roundup-doge-facebook-dating-amazon-artificial-intelligence/",
    "published_at": "2025-12-05T22:29:11+00:00",
    "content": "[AI 번역 준비중]\n\nUncanny Valleyhost Zoë Schiffer is joined by senior editor Leah Feiger to discuss five stories you need to know about this week, from how Amazon is trying to catch up in the AI race to why Facebook Dating is more popular than ever. Then, they dive into how—despite recent reports claiming that it’s over—DOGE operatives are still very much working across federal agencies.\n\nArticles mentioned in this episode:\n\nYou can follow Zoë Schiffer on Bluesky at@zoeschifferand Leah Feiger on Bluesky at@leahfeiger. Write to us atuncannyvalley@wired.com.\n\nIf you're on an iPhone or iPad, open the app called Podcasts, or just tapthis link. You can also download an app like Overcast or Pocket Casts and search for “uncanny valley.” We’re onSpotifytoo.\n\nNote: This is an automated transcript, which may contain errors.\n\nZoë Schiffer:Welcome to WIRED'sUncanny Valley. I'm WIRED's director of business and industry, Zoë Schiffer. Today on the show, we're bringing you five stories that you need to know about this week, including how despite some reports claiming that the so-called Department of Government Efficiency is pretty much over, DOGE people are actually still at work across federal agencies.\n\nI'm joined today by our senior politics editor, Leah Feiger. Leah, welcome back toUncanny Valley.\n\nLeah Feiger:Thanks so much, Zoë. How are you doing today?\n\nZoë Schiffer:I am great because I've spent the day with you, but our gentle listeners don't know that. Today's a good day. Let's dive right in.\n\nSo the first story this week is one that I saw and I thought, you know what? Leah's going to want to talk about Amazon's artificial intelligence prowess.\n\nLeah Feiger:That's all I ever want to talk about.\n\nZoë Schiffer:I know. I know. But I'm actually going to make a hard pitch for you right now.\n\nLeah Feiger:OK. Hit me.\n\nZoë Schiffer:Because you might think Amazon's biggest contribution to the field of artificial intelligence is the $8 billion that it gave to Anthropic. But au contrair, it is actually developing frontier models itself. And it has this very interesting edge in the AI race because so much of the AI advancements have been built on top of AWS's computing technology.\n\nLeah Feiger:Why don't I know anything about them then?\n\nZoë Schiffer:Great question. I think number one, its executives are very media trained, AKA. I wouldn't say they're the spiciest meatballs in the bunch, if you know what I mean. They tend to be pretty middle of the road, moderate. They'll say things like, \"Look, these other companies are chasing wild valuations. We're really focused on just creating value for our customers, making sure that people are actually making money from AI.\" You know what I mean?\n\nLeah Feiger:OK. So what did they reveal this week?\n\nZoë Schiffer:So this week they announced some new and improved LLMs. So there's Nova Light, Nova Pro, a new real-time voice model called Nova Sonic, and a more experimental model called Nova Omni that performs a simulated kind of reasoning using images, audio, and video as well as text. And the big reveal was Nova Forge, which is a customizable LLM that can be modified according to a specific user's business needs. I think the other thing that's important to say here is that the fact that Amazon has AWS is a real edge in the AI race.\n\nAnd one thing that it really made me think about is the fact that OpenAI has been pouring, they say, it's going to be trillions of dollars into AI infrastructure. And the CEO, Sam Altman, has hinted that at a certain point they might sell that capacity to other companies. They might be like, \"We have so much computing power. We are going to essentially be an AWS competitor,\" although he hasn't said those words exactly. So there is this level in which if that becomes the new frontier in the AI race, AWS could be pretty far ahead and OpenAI could be trying to compete on that level. Where I think the narrative right now has really been OpenAI, Anthropic, Google leading the pack like Amazon, where are they?\n\nLeah Feiger:But Amazon getting into all of this and describing themselves as this middle of the road, we have all of these things all set to go, they've been laying off employees left and right.\n\nZoë Schiffer:Yes. I know. I know. I know. OK. So this was the thing I wanted to talk to you about because the executives are really positioning themselves, like you said, it's like, as we're the moderate AI company. We're focused on value, enterprise, blah, blah, blah, blah, blah. But at the same time, other parts of the business have been seemingly cording big buzzy headlines by saying, \"Look, we're going to lay all these people off. And also because of artificial intelligence, we might need fewer people on specific teams.\"\n\nLeah Feiger:And this is where everyone in California loses me.\n\nZoë Schiffer:Yes. Yes. Yes. Yes. Yes. Yes.\n\nLeah Feiger:Immediately.\n\nZoë Schiffer:But stay with me here because I talked to an engineer at Amazon and they said that the way that AI tools are being pu",
    "content_length": 5013,
    "thumbnail_url": "https://media.wired.com/photos/69333f0402bd8e9fe7b724ff/191:100/w_1280,c_limit/Uncanny-Valley-WIRED-Roundup-DOGE-Isnt-Dead-Politics-2249462067.jpg",
    "category": "AI",
    "ai_summary": ""
  },
  {
    "source": "Wired AI",
    "title": "Here’s What You Should Know About Launching an AI Startup",
    "link": "https://www.wired.com/story/artificial-intelligence-startups-daydream-fashion-recommendations/",
    "published_at": "2025-12-05T16:00:00+00:00",
    "content": "[AI 번역 준비중]\n\nJulie Bornstein thoughtit would be a cinch to implement her idea for anAI startup. Her résumé indigital commerceis impeccable: VP of ecommerce at Nordstrom, COO of the startup Stitch Fix, and founder of a personalized shopping platform acquired byPinterest. Fashion has been her obsession since she was a Syracuse high schooler inhaling spreads in Seventeen and hanging out in local malls. So she felt well-positioned to create a company for customers to discover the perfect garments using AI.\n\nThe reality was much harder than she expected. I had breakfast recently with Bornstein and her CTO, Maria Belousova, to learn about her startup,Daydream, funded with $50 million from VCs like Google Ventures. The conversation took an unexpected turn as the women schooled me on the surprising difficulty of translating the magic of AI systems into something people actually find useful.T\n\nBornstein’s original pitch to VCs seemed obvious: Use AI to solve tricky fashion problems by matching customers with the perfect garments, which they’d be delighted to pay for. (Daydream would take a cut.) You’d think the setup would be simple—just connect to an API for a model like ChatGPT and you’re good to go, right? Um, no. Signing up over 265 partners, with access to more than 2 million products from boutique shops to retail giants, was the easy part. It turns out that fulfilling even a simple request like “I need a dress for a wedding in Paris” is incredibly complex. Are you the bride, the mother-in-law, or a guest? What season is it? How formal a wedding? What statement do you want to make? Even when those questions are resolved, different AI models have different views on such things. “What we found was, because of the lack of consistency and reliability of the model—and the hallucinations—sometimes the model would drop one or two elements of the queries,” says Bornstein. A user in Daydream’s long-extended beta test would say something like, “I’m a rectangle, but I need a dress to make me look like an hourglass.” The model would respond by showing dresses with geometric patterns.\n\nUltimately, Bornstein understood that she had to do two things: postpone the app’s planned fall 2024 launch (though it’s now available, Daydream is still technically in beta until sometime in 2026) and upgrade her technical team. In December 2024 she hired Belousova, the former CTO of Grubhub, who in turn brought in a team of top engineers. Daydream’s secret weapon in the fierce talent war is the chance to work on a fascinating problem. “Fashion is such a juicy space because it has taste and personalization and visual data,” says Belousova. “It’s an interesting problem that hasn't been solved.”\n\nWhat’s more, Daydream has to solve this problemtwice—first by interpreting what the customer says and then by matching their sometimes quirky criteria with the wares on the catalog side. With inputs likeI need a revenge dress for a bat mitzvah where my ex is attending with his new wife,that understanding is critical. “We have this notion at Daydream of shopper vocabulary and a merchant vocabulary, right?” says Bornstein. “Merchants speak in categories and attributes, and shoppers say things like, ‘I’m going to this event, it’s going to be on the rooftop, and I'm going to be with my boyfriend.’ How do you actually merge these two vocabularies into something at run time? And sometimes it takes several iterations in a conversation.” Daydream learned that language isn’t enough. “We’re using visual models, so we actually understand the products in a much more nuanced way,” she says. A customer might share a specific color or show a necklace that they’ll be wearing.\n\nBornstein says Daydream’s subsequent rehaul has produced better results. (Though when I tried it out, a request for black tuxedo pants showed me beige athletic-fit trousers in addition to what I asked for. Hey, it’s a beta.) “We ended up deciding to move from a single call to an ensemble of many models,” says Bornstein. “Each one makes a specialized call. We have one for color, one for fabric, one for season, one for location.” For instance, Daydream has found that for its purposes, OpenAI models are really good at understanding the world from the clothing point of view. Google’s Gemini is less so, but it is fast and precise.\n\nFrom the beginning, Daydream has also understood that AI needs human help. A popular request among users is to view the kinds of clothes that Hailey Bieber wears. Rather than leave this to the robots, Daydream’s people created a collection of dresses that satisfy that urge, enough to let the model understand what else can fulfill the desire. When a sudden trend likecottagecoreemerges, her team jumps into action and creates a collection. Bornstein now believes that with the extra effort and lots of patience, she’s on the right track.\n\nBornstein tells me that her peers at other AI startups have faced similar challenges.Duckbillis a service that uses AI to efficiently provide personal se",
    "content_length": 5013,
    "thumbnail_url": "https://media.wired.com/photos/692de340fbb01f28f4e39193/191:100/w_1280,c_limit/Backchannel-The-Extreme-Challenge-of-Building-Great-Apps-with-AI-Business.jpg",
    "category": "AI",
    "ai_summary": ""
  },
  {
    "source": "Wired AI",
    "title": "AI Slop Is Ruining Reddit for Everyone",
    "link": "https://www.wired.com/story/ai-slop-is-ruining-reddit-for-everyone/",
    "published_at": "2025-12-05T12:00:00+00:00",
    "content": "[AI 번역 준비중]\n\nA Reddit postabout a bride who demands a wedding guest wear a specific, unflattering shade is sure to provoke rage, let alone one about a bridesmaid or mother of the groom who wants to wear white. A scenario where a parent asks someone on an airplane to switch seats so they can sit next to their young child is likely to invoke the same rush of anger. But those posts may trigger aRedditmoderator’s annoyance for a different reason—they are common themes within a growing genre ofAI-generated, fake posts.\n\nThese are examples that spring to mind for Cassie, one of dozens of moderators forr/AmItheAsshole. With over 24 million members, it's one of the biggest subreddits, and it explicitly bans AI-generated content and other made-up stories. Since late 2022, when ChatGPT first launched to the public, Cassie (who wanted to be referred to by first name only) and other people who volunteer their time to moderate Reddit posts have been struggling with an influx of AI content. Some of it is entirely AI-generated, while other users have taken to editing their posts and comments with AI programs like Grammarly.\n\n“It’s probably more prevalent than anybody wants to really admit, because it’s just so easy to shove your post into ChatGPT and say ‘Hey, make this more exciting,’” says Cassie, who thinks as much as half of all content being posted to Reddit may have been created or reworked with AI in some way.\n\nr/AmItheAsshole is a pillar of Reddit culture, a format that has inspired dozens if not hundreds of derivatives like r/AmIOverreacting, r/AmITheDevil, and r/AmItheKameena, a subreddit with over 100,000 members described as “Am I the asshole, but the Indian version.” Posts tend to feature stories about interpersonal conflicts, where Redditors can weigh in on who is wrong (“YTA” means “You’re the asshole,” while “ESH” means “Everyone sucks here”), who is right, and what the best course of action to take is moving forward. Users and moderators across these r/AmItheAsshole variants have reported seeing more content they suspect is AI-generated, and others say it's a sitewide issue happening in all kinds of subreddits.\n\n“If you have a general wedding sub or AITA, relationships, or something like that, you will get hit hard,” says a moderator of r/AITAH, a variant of r/AmItheAsshole that has almost 7 million members. This moderator, a retiree who spoke on the condition of anonymity, has been active on Reddit for 18 years—most of its existence—and also had decades of experience in the web business before that. She views AI as a potential existential threat to the platform.\n\n“Reddit itself is either going to have to do something, or the snake is going to swallow its own tail,” she says. “It’s getting to the point where the AI is feeding the AI.”\n\nIn a response to a request for comment, a Reddit spokesperson said: “Reddit is the most human place on the Internet, and we want it to stay that way. We prohibit manipulated content and inauthentic behavior, including misleading AI bot accounts posing as people and foreign influence campaigns. Clearly labeled AI-generated content is generally allowed as long as it’s within a community’s rules and our sitewide rules.” The spokesperson added that there were over 40 million “spam and manipulated content removals” in the first half of 2025.\n\nAlly, a 26-year-old who tutors at a community college in Florida and spoke using her first name only for her privacy, has noticed Reddit “really going downhill” in the past year because of AI. Her feelings are shared by other users in subreddits like r/EntitledPeople, r/simpleliving, and r/self, where posts in the last year have bemoaned the rise of suspected AI. The mere possibility that something could be AI-generated has already eroded trust between users. “AI is turning Reddit into a heap of garbage,” one account wrote in r/AmITheJerk. “Even if a post suspected of being AI isn’t, just the existence of AI is like having a spy in the room. Suspicion itself is an enemy.” Ally used to enjoy reading subreddits like r/AmIOverreacting. But now she doesn’t know if her interactions are real anymore, and she’s spending less time on the platform than in years past.\n\n“AI burns everybody out,” says the r/AITAH moderator. “I see people put an immense amount of effort into finding resources for people, only to get answered back with ‘Ha, you fell for it, this is all a lie.’”\n\nThere are few foolproof ways to prove that something is AI or not, and most everyday people are relying on their own intuition. Text can be even harder to evaluate than photos and videos, which often have pretty definitive tells. Five Redditors who spoke to WIRED all had different strategies for identifying AI-generated text. Cassie notices when posts restate their title verbatim in the body or useem dashes, as well as when a poster has terrible spelling and punctuation in their comment history but posts something with perfect grammar. Ally is thrown off by newly created Reddit accounts and p",
    "content_length": 5013,
    "thumbnail_url": "https://media.wired.com/photos/692f329c56ed04538b1ea189/191:100/w_1280,c_limit/Die-Hard-Redditors-Feeling-Pushed-out-by-AI-Culture-.jpg",
    "category": "AI",
    "ai_summary": ""
  },
  {
    "source": "Wired AI",
    "title": "Huge Trove of Nude Images Leaked by AI Image Generator Startup’s Exposed Database",
    "link": "https://www.wired.com/story/huge-trove-of-nude-images-leaked-by-ai-image-generator-startups-exposed-database/",
    "published_at": "2025-12-05T11:00:00+00:00",
    "content": "[AI 번역 준비중]\n\nAn AI imagegenerator startup left more than 1 million images and videos created with its systems exposed and accessible to anyone online, according to new research reviewed by WIRED. The “overwhelming majority” of the images involved nudity and were “depicted adult content,” according to the researcher who uncovered the exposed trove of data, with some appearing to depict children or the faces of children swapped onto the AI-generated bodies of nude adults.\n\nMultiple websites—including MagicEdit and DreamPal—all appeared to be using the same unsecured database, says security researcher Jeremiah Fowler, who discovered the security flaw in October. At the time, Fowler says, around 10,000 new images were being added to the database every day. Indicating how people may have been using the image-generation and editing tools, these images included “unaltered” photos of real people who may have been nonconsensually “nudified,” or had their faces swapped onto other, naked bodies.\n\n“The real issue is just innocent people, and especially underage people, having their images used without their consent to make sexual content,” says Fowler, a prolific hunter of exposed databases, who published the findings on theExpressVPN blog. Fowler says it is the third misconfiguredAI-image-generation databasehe has found accessible online this year—with all of them appearing to contain nonconsensual explicit imagery, including those of young people and children.\n\nFowler’s findings come as AI-image-generation tools continue to be used to maliciously create explicit imagery of people. An enormous ecosystem of “nudify” services, which areused by millions of peopleandmake millions of dollars per year, uses AI to “strip” the clothes off of people—almost entirely women—in photos. Photos stolen from social media can be edited in just a couple of clicks: leading to the harrowing abuse and harassment of women. Meanwhile,reportsof criminals using AI to create child sexual abuse material, which covers arange of indecent imagesinvolving children, have doubled over the past year.\n\n“We take these concerns extremely seriously,” says a spokesperson for a startup called DreamX, which operates MagicEdit and DreamPal. The spokesperson says that an influencer marketing firm linked to the database, called SocialBook, is run “by a separate legal entity and is not involved” in the operation of other sites. “These entities share some historical relationships through founders and legacy assets, but they operate independently with separate product lines,” the spokesperson says.\n\n“SocialBook is not connected to the database you referenced, does not use this storage, and was not involved in its operation or management at any time,” a SocialBook spokesperson tells WIRED. “The images referenced were not generated, processed, or stored by SocialBook’s systems. SocialBook operates independently and has no role in the infrastructure described.”\n\nIn his report, Fowler writes that the database indicated it was linked to SocialBook and included images with a SocialBook watermark. Multiple pages on the SocialBook website that previously mentionedMagicEditorDreamPalnow return error pages. “The bucket in question contained a mix of legacy assets, primarily from MagicEdit and DreamPal. SocialBook does not use this bucket for its operational infrastructure,” the DreamX spokesperson says.\n\n“Our priority is the safety of users and the public, adherence to all legal requirements, and complete transparency throughout this process,” the DreamX spokesperson adds. “We do not condone, support, or tolerate the creation or distribution of child sexual abuse material (‘CSAM’) under any circumstances.”\n\nAfter Fowler got in touch with the AI-image-generator firm, the spokesperson says, it closed access to the exposed database and launched an “internal investigation with external legal counsel.” It also “suspended access to our products pending the investigation’s outcome,” the spokesperson says. The MagicEdit and DreamPal websites and mobile applications were accessible until WIRED got in touch with those who run it.\n\nAt the time of writing, the DreamPal website is unavailable, returning a 502 error. “We are temporarily suspending certain features of the product,” a message on the homepage of the MagicEdit website says. “During this period, the service may be unavailable.” Another associated website also displays the same message. Both MagicEdit and DreamPal were listed as being owned by the developer BoostInsider on Apple’s iOS App Store. MagicEdit, DreamPal, and two other AI apps listed by BoostInsider are now no longer available on the App Store.\n\nThe DreamX spokesperson says Boostinsider is a “defunct entity,” and the company “temporarily removed” the apps as “part of a broader restructuring of our product lines and infrastructure” and it is “strengthening our content-moderation framework.”\n\nThe apps do not seem to appear on Google’s Play Store. However, when a BoostInsider accoun",
    "content_length": 5013,
    "thumbnail_url": "https://media.wired.com/photos/6930c34d0c11887cee72a787/191:100/w_1280,c_limit/sec-ai-nudes-1365721742.jpg",
    "category": "AI",
    "ai_summary": ""
  },
  {
    "source": "Wired AI",
    "title": "Can AI Look at Your Retina and Diagnose Alzheimer’s? Eric Topol Hopes So",
    "link": "https://www.wired.com/story/big-interview-event-eric-topol-super-agers/",
    "published_at": "2025-12-04T23:11:21+00:00",
    "content": "[AI 번역 준비중]\n\nFor decades now,it’s been fairly well established that once you turn 40 you should start paying more attention to your body. That’s when women are supposed to start getting mammograms and men are supposed to start paying a bit more attention to their prostates. Over the next decade, you’ll start getting colonoscopies, and from then on out, it feels like a gradual march of doctor’s appointments and tests until your body collapses sometime in your seventies or eighties.\n\nBut what if modern medicine has the timeline all wrong? What if we’re testing some middle-aged people unnecessarily for diseases they’ll most likely never get, while blindly ignoring twentysomethings who might be prone to colon cancer? Is there a way that, even as we age, we can stay healthy in a way that’s both meaningful and not reliant on taking 12 horse-sized pills every morning?\n\nEric Topol certainly thinks so. The cardiologist, vice president of Scripps Research, and author ofSuper Agersis convinced that new innovations in AI-assisted medicine, bioengineering, and anti-inflammatory awareness could have potential to revolutionize the way people age.\n\nDuring WIRED’s Big Interview event in San Francisco on Thursday, Topol told features editor Sandra Upson that while he was working onSuper Agershe learned that there’s a difference between lifespan and health span and that neither has much to do with genetics. Someone who’s “wellderly,” or over 65 and generally healthy, has pretty much the same genetic makeup as someone who’s elderly and facing major health challenges, like heart disease, cancer, or a neurodegenerative disorder.\n\nIf possible, Topol said, people should avoid environmental stressors, like air pollution, micro and nano plastics, and forever chemicals, all of which Topol said are pro-inflammatory. All of these, Topol noted, are not being addressed by President Trump and health secretary Robert F. Kennedy Jr., despite theirMake America Healthy Again agenda.\n\nFor the average American, Topol said, health span is about 63 to 65 years. Lifespan, on the other hand, is about 80 years. That means most Americans will spend the last 15 or so years of their lives in relatively poor health, with one World Health Organization stat saying that most elderly people will only experience one “healthy birthday” after the age of 65.\n\n“Health span should be extended as close as we can to lifespan, and I think we can do it,” Topol told Upson. “This is a unique moment in medicine. Part of it is because we have multimodal AI, but part of it is because we have new layers of data. We never had organ clocks, which track the pace of aging for every organ of your body, including your immune system. We never had biomarkers like p-tau217, which tells us about our risk of Alzheimer's 10, 15, even 20 years in advance. The biggest jump in recent biomedicine is the ability to quantify metrics of aging.”\n\nTopol believes humans are only starting to see the true potential of GLP-1s, adding that it’s still the “early innings for this family of drugs.” GLP-1s create signals from a body’s gut to its brain, but they also talk to a body’s immune system, which Topol says can provide the “perfect intervention for bringing down inflammation,” creating benefits for the heart, kidneys, and liver, but also limiting someone’s addiction to everything from cigarettes to nail-biting. Topol said there’s a study scheduled for early next year that will examine whether GLP-1s could have an impact on preventing Alzheimer’s in people with preexisting dispositions for the disease but who can begin to take the drug before they turn 50.\n\nFurther innovations in the world of medicine could also come through the use of AI, which Topol said he’s “pretty bullish” about. Large language models can review medical records en masse, noticing problems in single individuals or groups of people that even the most discerning physicians might not. AI can analyze billions of points of individual data in a person’s medical chart and, using just an image of someone’s retina, find risks for arterial issues, Parkinson’s, or Alzheimer’s years before anyone else could. Topol said AI can even be used to pick up lab trends that indicate pancreatic cancer years before a diagnosis could be made and at a relatively low cost, something that could be revolutionary for the healthcare industry.\n\nAnd while much has been made about influencers and tech personalities biohacking their own aging process, Topol said it’s really not that hard. “Lifestyle is what’s important,” he concluded during the interview. “It’s the biggest driver we have, and it’s the most inexpensive way to extend your health span. And if there’s anyone here who’s not interested in extending their health span, please, let me know.”",
    "content_length": 4786,
    "thumbnail_url": "https://media.wired.com/photos/6931f5996919b9776a454f2c/191:100/w_1280,c_limit/Dr-Eric-Topol-Big-Interview-2025-1.jpg",
    "category": "AI",
    "ai_summary": ""
  },
  {
    "source": "MIT Tech Review",
    "title": "Harnessing human-AI collaboration for an AI roadmap that moves beyond pilots",
    "link": "https://www.technologyreview.com/2025/12/05/1128730/harnessing-human-ai-collaboration-for-an-ai-roadmap-that-moves-beyond-pilots/",
    "published_at": "2025-12-05T15:00:00+00:00",
    "content": "[AI 번역 준비중]\n\nIn this exclusive webcast, Concentrix’s Ryan Peterson, Everest Group’s Shirley Hung, and Valmont’s Heidi Hough discuss turning AI ambitions into operational advantages.\n\nIn partnership withConcentrix\n\nThe past year has marked a turning point in the corporate AI conversation. After a period of eager experimentation, organizations are now confronting a more complex reality: While investment in AI has never been higher, the path from pilot to production remains elusive.Three-quartersof enterprises remain stuck in experimentation mode, despite mounting pressure to convert early tests into operational gains.\n\n“Most organizations can suffer from what we like to call PTSD, or process technology skills and data challenges,” says Shirley Hung, partner at Everest Group. “They have rigid, fragmented workflows that don't adapt well to change, technology systems that don't speak to each other, talent that is really immersed in low-value tasks rather than creating high impact. And they are buried in endless streams of information, but no unified fabric to tie it all together.”\n\nThe central challenge, then, lies in rethinking how people, processes, and technology work together.\n\nAcross industries as different as customer experience and agricultural equipment, the same pattern is emerging: Traditional organizational structures—centralized decision-making, fragmented workflows, data spread across incompatible systems—are proving too rigid to support agentic AI. To unlock value, leaders must rethink how decisions are made, how work is executed, and what humans should uniquely contribute.\n\n\"It is very important that humans continue to verify the content. And that is where you're going to see more energy being put into,\" Ryan Peterson, EVP and chief product officer at Concentrix.\n\nMuch of the conversation centered on what can be described as the next major unlock: operationalizing human-AI collaboration. Rather than positioning AI as a standalone tool or a “virtual worker,” this approach reframes AI as a system-level capability that augments human judgment, accelerates execution, and reimagines work from end to end. That shift requires organizations to map the value they want to create; design workflows that blend human oversight with AI-driven automation; and build the data, governance, and security foundations that make these systems trustworthy.\n\n\"My advice would be to expect some delays because you need to make sure you secure the data,” says Heidi Hough, VP for North America aftermarket at Valmont. “As you think about commercializing or operationalizing any piece of using AI, if you start from ground zero and have governance at the forefront, I think that will help with outcomes.\"\n\nEarly adopters are already showing what this looks like in practice: starting with low-risk operational use cases, shaping data into tightly scoped enclaves, embedding governance into everyday decision-making, and empowering business leaders, not just technologists, to identify where AI can create measurable impact. The result is a new blueprint for AI maturity grounded in reengineering how modern enterprises operate.\n\n\"Optimization is really about doing existing things better, but reimagination is about discovering entirely new things that are worth doing,\" says Hung.\n\nThis webcast is produced in partnership with Concentrix.\n\nThis content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.\n\nThe idea that machines will be as smart as—or smarter than—humans has hijacked an entire industry. But look closely and you’ll see it’s a myth that persists for many of the same reasons conspiracies do.\n\nThe experimental model won't compete with the biggest and best, but it could tell us why they behave in weird ways—and how trustworthy they really are.\n\nThey managed to cut the size of the AI reasoning model by more than half—and claim it can now answer politically sensitive questions once off limits in Chinese AI systems.\n\nCompetition is heating up, with Mattel and OpenAI expected to launch a product for kids this year.\n\nDiscover special offers, top stories,\n            upcoming events, and more.\n\nThank you for submitting your email!\n\nIt looks like something went wrong.",
    "content_length": 4581,
    "thumbnail_url": "https://wp.technologyreview.com/wp-content/uploads/2025/12/Concentrix-social-card-v2.png?resize=1200,600",
    "category": "AI",
    "ai_summary": ""
  },
  {
    "source": "MIT Tech Review",
    "title": "The era of AI persuasion in elections is about to begin",
    "link": "https://www.technologyreview.com/2025/12/05/1128837/the-era-of-ai-persuasion-in-elections-is-about-to-begin/",
    "published_at": "2025-12-05T10:00:00+00:00",
    "content": "[AI 번역 준비중]\n\nAI is eminently capable of political persuasion and could automate it at a mass scale. We are not prepared.\n\nIn January 2024, the phone rang in homes all around New Hampshire. On the other end was Joe Biden’s voice, urging Democrats to“save your vote”by skipping the primary. It sounded authentic, but it wasn’t. The call was a fake, generated by artificial intelligence.\n\nToday, the technology behind that hoax looks quaint. Tools like OpenAI’s Sora now make it possible to create convincing synthetic videos with astonishing ease. AI can be used to fabricate messages from politicians and celebrities—even entire news clips—in minutes. The fear that elections could be overwhelmed by realistic fake media has gone mainstream—and for good reason.\n\nBut that’s only half the story. The deeper threat isn’t that AI can just imitate people—it’s that it can actively persuade people. And new research published this weekshows just how powerfulthat persuasion can be. In two large peer-reviewed studies, AI chatbots shifted voters’ views by a substantial margin, far more than traditional political advertising tends to do.\n\nIn the coming years, we will see the rise of AI that can personalize arguments, test what works, and quietly reshape political views at scale. That shift—from imitation to active persuasion—should worry us deeply.\n\nThe challenge is that modern AI doesn’t just copy voices or faces; it holds conversations, reads emotions, and tailors its tone to persuade. And it can now command other AIs—directing image, video, and voice models to generate the most convincing content for each target. Putting these pieces together, it’s not hard to imagine how one could build a coordinated persuasion machine. One AI might write the message, another could create the visuals, another could distribute it across platforms and watch what works. No humans required.\n\nA decade ago, mounting an effective online influence campaign typically meant deployingarmiesof people running fake accounts and meme farms. Now that kind of work can be automated—cheaply and invisibly. The same technology that powers customer service bots and tutoring apps can be repurposed to nudge political opinions or amplify a government’s preferred narrative. And the persuasion doesn’t have to be confined to ads or robocalls. It can be woven into the tools people already use every day—social media feeds, language learning apps, dating platforms, or even voice assistants built and sold by parties trying to influence the American public. That kind of influence could come from malicious actors using the APIs of popular AI tools people already rely on, or from entirely new apps built with the persuasion baked in from the start.\n\nAnd it’s affordable. For less than a million dollars, anyone can generate personalized, conversational messages for every registered voter in America. The math isn’t complicated. Assume 10 brief exchanges per person—around 2,700 tokens of text—and price them at current rates for ChatGPT’s API. Even with a population of 174 million registered voters, the total still comes in under $1 million. The80,000 swing voterswho decided the 2016 election could be targeted for less than $3,000.\n\nAlthough this is a challenge in elections across the world, the stakes for the United States are especially high, given the scale of its elections and the attention they attract from foreign actors. If the US doesn’t move fast, the next presidential election in 2028, or even the midterms in 2026, could be won by whoever automates persuasion first.\n\nWhilethere have been indicationsthat the threat AI poses to elections is overblown, a growing body of research suggests the situation could be changing. Recent studies have shown that GPT-4 canexceedthe persuasive capabilities of communications experts when generating statements on polarizing US political topics, and it is more persuasive than non-expert humanstwo-thirds of the timewhen debating real voters.\n\nTwo major studies published yesterday extend those findings to real election contexts in the United States, Canada, Poland, and the United Kingdom, showing that brief chatbot conversations can move voters’ attitudes by up to 10 percentage points, with US participant opinions shifting nearly four times more than it did in response to tested 2016 and 2020 political ads. And when models were explicitly optimized for persuasion, the shift soared to 25 percentage points—an almost unfathomable difference.\n\nWhile previously confined to well-resourced companies, modern large language models are becoming increasingly easy to use. Major AI providers likeOpenAI,Anthropic, andGooglewrap their frontier models in usage policies, automated safety filters, and account-level monitoring, and they do sometimessuspendusers who violate those rules. But those restrictions apply only to traffic that goes through their platforms; they don’t extend to the rapidly growing ecosystem of open-source and open-weight models, which  can be downloaded",
    "content_length": 5013,
    "thumbnail_url": "https://wp.technologyreview.com/wp-content/uploads/2025/12/ai-election-ops.jpg?resize=1200,600",
    "category": "AI",
    "ai_summary": ""
  },
  {
    "source": "MIT Tech Review",
    "title": "AI chatbots can sway voters better than political advertisements",
    "link": "https://www.technologyreview.com/2025/12/04/1128824/ai-chatbots-can-sway-voters-better-than-political-advertisements/",
    "published_at": "2025-12-04T19:54:57+00:00",
    "content": "[AI 번역 준비중]\n\nA conversation with a chatbot can shift people's political views—but the most persuasive models also spread the most misinformation.\n\nIn 2024, a Democratic congressional candidate in Pennsylvania, Shamaine Daniels, used anAI chatbot named Ashleyto call voters and carry on conversations with them. “Hello. My name is Ashley, and I’m an artificial intelligence volunteer for Shamaine Daniels’s run for Congress,” the calls began. Daniels didn’t ultimately win. But maybe those calls helped her cause: New research reveals that AI chatbots can shift voters’ opinions in a single conversation—and they’re surprisingly good at it.\n\nThe findings, detailed in a pair of studies published in the journalsNatureandScience, are the latest in an emerging body of research demonstrating the persuasive power of LLMs. They raise profound questions about how generative AI could reshape elections.\n\nIn similar experiments conducted during the lead-ups to the 2025 Canadian federal election and the 2025 Polish presidential election, the team found an even larger effect. The chatbots shifted opposition voters’ attitudes by about 10 points.\n\nLong-standing theories of politically motivated reasoning hold that partisan voters are impervious to facts and evidence that contradict their beliefs. But the researchers found that the chatbots, which used a range of models including variants of GPT and DeepSeek, were more persuasive when they were instructed to use facts and evidence than when they were told not to do so. “People are updating on the basis of the facts and information that the model is providing to them,” says Thomas Costello, a psychologist at American University, who worked on the project.\n\nThe catch is, some of the “evidence” and “facts” the chatbots presented were untrue. Across all three countries, chatbots advocating for right-leaning candidates made a larger number of inaccurate claims than those advocating for left-leaning candidates. The underlying models are trained on vast amounts of human-written text, which means they reproduce real-world phenomena—including “political communication that comes from the right, which tends to be less accurate,” according to studies of partisan social media posts, says Costello.\n\nIn the other study published this week,inScience, an overlapping team of researchers investigated what makes these chatbots so persuasive. They deployed 19 LLMs to interact with nearly 77,000 participants from the UK on more than 700 political issues while varying factors like computational power, training techniques, and rhetorical strategies.\n\nThe most effective way to make the models persuasive was to instruct them to pack their arguments with facts and evidence and then give them additional training by feeding them examples of persuasive conversations. In fact, the most persuasive model shifted participants who initially disagreed with a political statement 26.1 points toward agreeing. “These are really large treatment effects,” says Kobi Hackenburg, a research scientist at the UK AI Security Institute, who worked on the project.\n\nBut optimizing persuasiveness came at the cost of truthfulness. When the models became more persuasive, they increasingly provided misleading or false information—and no one is sure why. “It could be that as the models learn to deploy more and more facts, they essentially reach to the bottom of the barrel of stuff they know, so the facts get worse-quality,” says Hackenburg.\n\nThe chatbots’ persuasive power could have profound consequences for the future of democracy, the authors note. Political campaigns that use AI chatbots could shape public opinion in ways that compromise voters’ ability to make independent political judgments.\n\nStill, the exact contours of the impact remain to be seen. “We’re not sure what future campaigns might look like and how they might incorporate these kinds of technologies,” says Andy Guess, a political scientist at Princeton University. Competing for voters’ attention is expensive and difficult, and getting them to engage in long political conversations with chatbots might be challenging. “Is this going to be the way that people inform themselves about politics, or is this going to be more of a niche activity?” he asks.\n\nEven if chatbots do become a bigger part of elections, it’s not clear whether they’ll do more to  amplify truth or fiction. Usually, misinformation has an informational advantage in a campaign, so the emergence of electioneering AIs “might mean we’re headed for a disaster,” says Alex Coppock, a political scientist at Northwestern University. “But it’s also possible that means that now, correct information will also be scalable.”\n\nAnd then the question is who will have the upper hand. “If everybody has their chatbots running around in the wild, does that mean that we’ll just persuade ourselves to a draw?” Coppock asks. But there are reasons to doubt a stalemate. Politicians' access to the most persuasive models may not be evenly distribute",
    "content_length": 5013,
    "thumbnail_url": "https://wp.technologyreview.com/wp-content/uploads/2025/12/persuasion2.jpg?resize=1200,600",
    "category": "AI",
    "ai_summary": ""
  },
  {
    "source": "MIT Tech Review",
    "title": "Delivering securely on data and AI strategy",
    "link": "https://www.technologyreview.com/2025/12/04/1128311/delivering-securely-on-data-and-ai-strategy/",
    "published_at": "2025-12-04T14:00:00+00:00",
    "content": "[AI 번역 준비중]\n\nLeading data-driven organizations balance protection and access as AI powers ahead.\n\nIn partnership withDatabricks\n\nMost organizations feel the imperative to keep pace with continuing advances in AI capabilities, as highlighted in a recentMIT Technology Review Insights report. That clearly has security implications, particularly as organizations navigate a surge in the volume, velocity, and variety of security data. This explosion of data, coupled with fragmented toolchains, is making it increasingly difficult for security and data teams to maintain a proactive and unified security posture.\n\nData and AI teams must move rapidly to deliver the desired business results, but they must do so without compromising security and governance. As they deploy more intelligent and powerful AI capabilities, proactive threat detection and response against the expanded attack surface, insider threats, and supply chain vulnerabilities must remain paramount. “I’m passionate about cybersecurity not slowing us down,” says Melody Hildebrandt, chief technology officer at Fox Corporation, “but I also own cybersecurity strategy. So I’m also passionate about us not introducing security vulnerabilities.”\n\nThat’s getting more challenging, says Nithin Ramachandran, who is global vice president for data and AI at industrial and consumer products manufacturer 3M. “Our experience with generative AI has shown that we need to be looking at security differently than before,” he says. “With every tool we deploy, we look not just at its functionality but also its security posture. The latter is now what we lead with.”\n\nOur survey of 800 technology executives (including 100 chief information security officers), conducted in June 2025, shows that many organizations struggle to strike this balance.\n\nThis content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. AI tools that may have been used were limited to secondary production processes that passed thorough human review.\n\nThe idea that machines will be as smart as—or smarter than—humans has hijacked an entire industry. But look closely and you’ll see it’s a myth that persists for many of the same reasons conspiracies do.\n\nThe experimental model won't compete with the biggest and best, but it could tell us why they behave in weird ways—and how trustworthy they really are.\n\nThey managed to cut the size of the AI reasoning model by more than half—and claim it can now answer politically sensitive questions once off limits in Chinese AI systems.\n\nCompetition is heating up, with Mattel and OpenAI expected to launch a product for kids this year.\n\nDiscover special offers, top stories,\n            upcoming events, and more.\n\nThank you for submitting your email!\n\nIt looks like something went wrong.",
    "content_length": 2948,
    "thumbnail_url": "https://wp.technologyreview.com/wp-content/uploads/2025/11/MITTR2025_V1_1200DatabricksCoverSocialV1.png?resize=1200,600",
    "category": "AI",
    "ai_summary": ""
  },
  {
    "source": "MIT Tech Review",
    "title": "OpenAI has trained its LLM to confess to bad behavior",
    "link": "https://www.technologyreview.com/2025/12/03/1128740/openai-has-trained-its-llm-to-confess-to-bad-behavior/",
    "published_at": "2025-12-03T18:01:39+00:00",
    "content": "[AI 번역 준비중]\n\nLarge language models often lie and cheat. We can’t stop that—but we can make them own up.\n\nOpenAI is testinganothernew way to expose the complicated processes at work inside large language models. Researchers at the company can make an LLM produce what they call a confession, in which the model explains how it carried out a task and (most of the time) owns up to any bad behavior.\n\nFiguring out why large language models do what they do—and in particular why they sometimes appear to lie, cheat, and deceive—is one of the hottest topics in AI right now. If this multitrillion-dollar technology is to be deployed as widely as its makers hope it will be, it must be made more trustworthy.\n\nOpenAI sees confessions as one step toward that goal. The work is still experimental, but initial results are promising, Boaz Barak, a research scientist at OpenAI, told me in an exclusive preview this week: “It’s something we’re quite excited about.”\n\nAnd yet other researchers question just how far we should trust the truthfulness of a large language model even when it has been trained to be truthful.\n\nA confession is a second block of text that comes after a model’s main response to a request, in which the model marks itself on how well it stuck to its instructions. The idea is to spot when an LLM has done something it shouldn’t have and diagnose what went wrong, rather than prevent that behavior in the first place. Studying how models work now will help researchers avoid bad behavior in future versions of the technology, says Barak.\n\nOne reason LLMs go off the rails is that they have to juggle multiple goals at the same time. Models are trained to be useful chatbots via a technique calledreinforcement learning from human feedback, which rewards them for performing well (according to human testers) across a number of criteria.\n\n“When you ask a model to do something, it has to balance a number of different objectives—you know, be helpful, harmless, and honest,” says Barak. “But those objectives can be in tension, and sometimes you have weird interactions between them.”\n\nFor example, if you ask a model something it doesn’t know, the drive to be helpful can sometimes overtake the drive to be honest. And faced with a hard task, LLMs sometimes cheat. “Maybe the model really wants to please, and it puts down an answer that sounds good,” says Barak. “It’s hard to find the exact balance between a model that never says anything and a model that does not make mistakes.”\n\nTo train an LLM to produce confessions, Barak and his colleagues rewarded the model only for honesty, without pushing it to be helpful or helpful. Importantly, models were not penalized for confessing bad behavior. “Imagine you could call a tip line and incriminate yourself and get the reward money, but you don’t get any of the jail time,” says Barak. “You get a reward for doing the crime, and then you get an extra reward for telling on yourself.”\n\nResearchers scored confessions as “honest” or not by comparing them with the model’s chains of thought, a kind of internal monologue that so-called reasoning models produce as they work through problems step by step.\n\nChains of thought are like scratch pads that models use to break down tasks, make notes, and plan their next actions. Analyzing them can give clear clues about what an LLM is doing. But they are not always easy to understand. And as models get larger and more efficient, some researchers think that chains of thought may become terser and even harder for humans to read.\n\nConfessions are a way to get a sense of what an LLM is doing without having to rely on chains of thought. But Naomi Saphra, who studies large language models at Harvard University, notes that no account produced by an LLM of its own behavior can be fully trusted. In practice, LLMs are still black boxes, and it is impossible to know for sure exactly what goes on inside them. “It seems that the method relies on the model having already provided a faithful chain-of-thought description of its own reasoning, which is already a problematic assumption,” she says of OpenAI’s approach.\n\nThese confessions should be taken as best guesses about what a model really did, she says—“not a faithful reflection of any hidden reasoning.”\n\nTo test their idea, Barak and his colleagues trained OpenAI’s GPT-5-Thinking, the company’s flagship reasoning model, to produce confessions. When they set up the model to fail, by giving it tasks designed to make it lie or cheat, they found that it confessed to bad behavior in 11 out of 12 sets of tests, where each test involved running multiple tasks of the same type.\n\nFor example, in one test the researchers asked GPT-5-Thinking to write and test code that would solve a math problem in nanoseconds, even though no code could run that fast. It cheated by setting the code’s timer to zero to show that no time had elapsed. But it also then explained what it had done.\n\nIn another test, the researchers set GPT-5-Thinking 10 simple math q",
    "content_length": 5013,
    "thumbnail_url": "https://wp.technologyreview.com/wp-content/uploads/2025/12/confessions3.jpg?resize=1200,600",
    "category": "AI",
    "ai_summary": ""
  }
]