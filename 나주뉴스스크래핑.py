"""
ë‚˜ì£¼ì‹œ ë³´ë„ìë£Œ ìˆ˜ì§‘ ë´‡
- ì§€ì • ê¸°ê°„ ë‚´ ë³´ë„ìë£Œ ìë™ ìˆ˜ì§‘
- ìµœì‹ ìˆœ ì •ë ¬
- ì¡°íšŒìˆ˜ ëœë¤ ìƒì„± (30-70)
- Supabase ì €ì¥ ì§€ì›
"""

import re
import base64
import json
import random
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from typing import Optional, List, Dict
import time

BASE_URL = 'https://www.naju.go.kr'
LIST_URL = f'{BASE_URL}/www/administration/reporting/coverage'

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml',
    'Accept-Language': 'ko-KR,ko;q=0.9',
}

# ì‹ ë¬¸ì‚¬ ë“±ë¡ì¼
REGISTRATION_DATE = '2024-09-19'


def decode_php_serialize(data: str) -> str:
    """PHP serialize í˜•ì‹ì—ì„œ ë¬¸ìì—´ ì¶”ì¶œ"""
    pattern = r's:\d+:"(.*?)";?\}'
    match = re.search(pattern, data, re.DOTALL)
    if match:
        return match.group(1)
    return data


def extract_content_from_og_description(html: str) -> str:
    """og:description ë©”íƒ€ íƒœê·¸ì—ì„œ Base64 ì¸ì½”ë”©ëœ ë³¸ë¬¸ ì¶”ì¶œ"""
    soup = BeautifulSoup(html, 'html.parser')
    og_desc = soup.find('meta', property='og:description')
    
    if not og_desc or not og_desc.get('content'):
        return ''
    
    encoded = og_desc['content']
    
    try:
        decoded_bytes = base64.b64decode(encoded)
        decoded_str = decoded_bytes.decode('utf-8')
        
        if decoded_str.startswith('a:'):
            content = decode_php_serialize(decoded_str)
        else:
            content = decoded_str
        
        content = re.sub(r'<[^>]+>', ' ', content)
        content = re.sub(r'\s+', ' ', content).strip()
        
        return content[:5000]
        
    except Exception as e:
        print(f'  âš ï¸ Base64 ë””ì½”ë”© ì‹¤íŒ¨: {e}')
        return ''


def fetch_press_list(page: int = 1) -> List[Dict]:
    """ë³´ë„ìë£Œ ëª©ë¡ í˜ì´ì§€ì—ì„œ í•­ëª© ìˆ˜ì§‘"""
    url = LIST_URL if page == 1 else f'{LIST_URL}?page={page}'
    
    response = requests.get(url, headers=HEADERS)
    response.raise_for_status()
    
    soup = BeautifulSoup(response.text, 'html.parser')
    items = []
    
    for row in soup.select('tbody tr'):
        link = row.select_one('a[href*="coverage?idx="]')
        if not link:
            continue
        
        href = link.get('href', '')
        idx_match = re.search(r'idx=(\d+)', href)
        if not idx_match:
            continue
        
        idx = idx_match.group(1)
        title = link.get_text(strip=True)
        
        # ë‹´ë‹¹ë¶€ì„œ ë° ë‚ ì§œ ì¶”ì¶œ
        info_tds = row.select('td.mob_dp_inflex')
        dept = info_tds[0].get_text(strip=True) if len(info_tds) > 0 else ''
        date = info_tds[1].get_text(strip=True) if len(info_tds) > 1 else ''
        
        items.append({
            'idx': idx,
            'title': title,
            'date': date,
            'department': dept,
            'url': f'{BASE_URL}{href.replace("&amp;", "&")}',
        })
    
    return items


def fetch_press_detail(url: str) -> Dict:
    """ë³´ë„ìë£Œ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ë° ì´ë¯¸ì§€ ì¶”ì¶œ"""
    response = requests.get(url, headers=HEADERS)
    response.raise_for_status()
    
    html = response.text
    soup = BeautifulSoup(html, 'html.parser')
    
    content = extract_content_from_og_description(html)
    
    og_image = soup.find('meta', property='og:image')
    image_url = og_image['content'] if og_image and og_image.get('content') else None
    
    og_title = soup.find('meta', property='og:title')
    title = og_title['content'] if og_title and og_title.get('content') else ''
    
    return {
        'title': title.split(',')[0] if title else '',
        'content': content,
        'content_length': len(content),
        'image_url': image_url,
    }


def is_within_date_range(date_str: str, start_date: str, end_date: str) -> bool:
    """ë‚ ì§œê°€ ì§€ì • ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸"""
    try:
        article_date = datetime.strptime(date_str, '%Y-%m-%d')
        start = datetime.strptime(start_date, '%Y-%m-%d')
        end = datetime.strptime(end_date, '%Y-%m-%d')
        return start <= article_date <= end
    except ValueError:
        return False


def generate_random_view_count(min_val: int = 30, max_val: int = 70) -> int:
    """ëœë¤ ì¡°íšŒìˆ˜ ìƒì„±"""
    return random.randint(min_val, max_val)


def format_published_at(date_str: str) -> str:
    """ë‚ ì§œë¥¼ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì˜¤ì „ 9ì‹œ ê¸°ì¤€)"""
    try:
        dt = datetime.strptime(date_str, '%Y-%m-%d')
        return dt.strftime('%Y-%m-%dT09:00:00+09:00')
    except ValueError:
        return datetime.now().strftime('%Y-%m-%dT09:00:00+09:00')


def collect_articles(start_date: str = REGISTRATION_DATE, end_date: str = None) -> List[Dict]:
    """
    ì§€ì • ê¸°ê°„ ë‚´ ëª¨ë“  ë³´ë„ìë£Œ ìˆ˜ì§‘
    
    Args:
        start_date: ì‹œì‘ì¼ (ê¸°ë³¸ê°’: ì‹ ë¬¸ì‚¬ ë“±ë¡ì¼ 2024-09-19)
        end_date: ì¢…ë£Œì¼ (ê¸°ë³¸ê°’: ì˜¤ëŠ˜)
    """
    if end_date is None:
        end_date = datetime.now().strftime('%Y-%m-%d')
    
    print(f'ğŸ›ï¸ ë‚˜ì£¼ì‹œ ë³´ë„ìë£Œ ìˆ˜ì§‘ ë´‡ ì‹œì‘')
    print(f'ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}\n')
    
    all_articles = []
    page = 1
    stop_collecting = False
    
    while not stop_collecting:
        print(f'ğŸ“‹ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...')
        
        try:
            items = fetch_press_list(page)
            
            if not items:
                print('   ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.')
                break
            
            for item in items:
                # ë‚ ì§œ ë²”ìœ„ í™•ì¸
                if not item['date']:
                    continue
                
                # ì‹œì‘ì¼ë³´ë‹¤ ì´ì „ì´ë©´ ìˆ˜ì§‘ ì¤‘ë‹¨
                if item['date'] < start_date:
                    print(f'   ğŸ“Œ {item["date"]} - ì‹œì‘ì¼ ì´ì „, ìˆ˜ì§‘ ì¢…ë£Œ')
                    stop_collecting = True
                    break
                
                # ì¢…ë£Œì¼ë³´ë‹¤ ì´í›„ë©´ ìŠ¤í‚µ
                if item['date'] > end_date:
                    continue
                
                # ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘
                print(f'   ğŸ“° {item["title"][:30]}... ({item["date"]})')
                
                try:
                    detail = fetch_press_detail(item['url'])
                    
                    article = {
                        'idx': item['idx'],
                        'title': item['title'],
                        'content': detail['content'],
                        'content_length': detail['content_length'],
                        'published_at': format_published_at(item['date']),
                        'date': item['date'],
                        'department': item['department'],
                        'image_url': detail['image_url'],
                        'view_count': generate_random_view_count(30, 70),
                        'source': 'ë‚˜ì£¼ì‹œ',
                        'source_url': item['url'],
                        'category': 'ì§€ì—­',
                    }
                    
                    all_articles.append(article)
                    
                    # ì„œë²„ ë¶€í•˜ ë°©ì§€
                    time.sleep(0.3)
                    
                except Exception as e:
                    print(f'      âŒ ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}')
            
            page += 1
            time.sleep(0.5)
            
        except Exception as e:
            print(f'   âŒ í˜ì´ì§€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}')
            break
    
    # ìµœì‹ ìˆœ ì •ë ¬
    all_articles.sort(key=lambda x: x['date'], reverse=True)
    
    print(f'\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_articles)}ê°œ ê¸°ì‚¬')
    
    return all_articles


def save_to_json(articles: List[Dict], filename: str = 'naju_articles.json'):
    """JSON íŒŒì¼ë¡œ ì €ì¥"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f'ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}')


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    # 2024-09-19 (ì‹ ë¬¸ì‚¬ ë“±ë¡ì¼) ~ ì˜¤ëŠ˜ê¹Œì§€ ìˆ˜ì§‘
    articles = collect_articles(
        start_date='2024-09-19',
        end_date=datetime.now().strftime('%Y-%m-%d')
    )
    
    if articles:
        save_to_json(articles, 'naju_articles.json')
        
        # ìš”ì•½ ì¶œë ¥
        print(f'\nğŸ“Š ìˆ˜ì§‘ ìš”ì•½:')
        print(f'   - ì´ ê¸°ì‚¬ ìˆ˜: {len(articles)}ê°œ')
        print(f'   - ê¸°ê°„: {articles[-1]["date"]} ~ {articles[0]["date"]}')
        print(f'   - í‰ê·  ë³¸ë¬¸ ê¸¸ì´: {sum(a["content_length"] for a in articles) // len(articles)}ì')
    
    return articles


if __name__ == '__main__':
    main()
