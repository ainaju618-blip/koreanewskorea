# Korea NEWS Scraping Bot Technical Consultation

> Date: 2025-12-29
> System: Playwright-based Monitoring + Regional Scrapers

---

## 1. System Overview

```
┌─────────────────┐       ┌──────────────────┐       ┌─────────────────┐
│  Monitoring Bot │──────▶│  trigger_scraper │──────▶│ Regional Scraper│
│  (playwright_   │       │  (subprocess)    │       │ (jeonnam_scraper│
│   monitor.py)   │       │                  │       │  .py, etc.)     │
└─────────────────┘       └──────────────────┘       └─────────────────┘
        │                                                    │
        ▼                                                    ▼
┌─────────────────┐                                 ┌─────────────────┐
│ scraper_state   │                                 │ posts table     │
│ (detection log) │                                 │ (article data)  │
└─────────────────┘                                 └─────────────────┘
```

### Flow:
1. **Monitoring Bot** detects new articles on government websites
2. If new articles found → calls `trigger_scraper()`
3. `trigger_scraper()` spawns regional scraper as subprocess
4. **Regional Scraper** collects article content → saves to `posts` table
5. **AI Processing** cleans/processes articles

---

## 2. Current Problem

**Symptom:**
- Monitoring detects new articles ✅ (scraper_state saving correctly)
- BUT posts table has 0 entries for today ❌
- Scraping (content collection) is NOT happening

**Evidence:**
```
Console shows:
[STATE] Saved mokpo latest_id=541688
[STATE] Saved jeonnam latest_id=...

But NO:
[SCRAPER] Starting scraper for jeonnam...
[SCRAPER] Completed for jeonnam
```

---

## 3. Root Cause Analysis

### Issue 1: Argument Mismatch (FIXED)

**trigger_scraper sends:**
```python
cmd = [
    'python', scraper_path,
    '--max-articles', '10',
    '--article-ids', '123,456,789'  # ← NOT SUPPORTED!
]
```

**jeonnam_scraper.py accepts:**
```python
parser.add_argument('--days', ...)
parser.add_argument('--max-articles', ...)  # ← Only this
parser.add_argument('--dry-run', ...)
# NO --article-ids argument!
```

**Result:** argparse error → scraper exits immediately

**Fix Applied:** Removed `--article-ids` from trigger_scraper()

---

### Issue 2: Trigger Condition (NEEDS VERIFICATION)

**Question:** Is `trigger_scraper()` being called at all?

**Code path:**
```python
# playwright_monitor.py line 1209
if trigger_ai and regions_with_new:
    for region_info in regions_with_new:
        trigger_scraper(region_info['region_code'], region_info['article_ids'])
```

**Variables to check:**
1. Is `trigger_ai` = True? (should be, unless --no-ai flag)
2. Is `regions_with_new` populated? (depends on has_new detection)

---

### Issue 3: has_new Detection Logic

**Question:** How is `has_new` determined?

**Code path:**
```python
# playwright_monitor.py
result.has_new = (latest_id != last_known_id)
```

**Potential issues:**
1. If `scraper_state` was empty → `last_known_id` = None → first article = "new"
2. If same article detected again → `last_known_id` matches → no trigger

---

## 4. Specific Questions for Expert

### Q1. subprocess.run Behavior

```python
result = subprocess.run(
    cmd,
    capture_output=True,
    text=True,
    timeout=300,
    cwd=os.path.dirname(os.path.dirname(scraper_path))
)
```

- Is `capture_output=True` blocking stdout from showing in parent console?
- Should we use `subprocess.Popen` with `stdout=sys.stdout` instead?
- Does `cwd` setting affect module imports in the scraper?

### Q2. Conditional Trigger Logic

Currently, trigger only happens if:
```python
if trigger_ai and regions_with_new:
```

- Should scraping be independent of AI processing toggle?
- Should there be a separate `trigger_scrape` flag?

### Q3. Error Handling in Scrapers

Current scrapers don't have good error reporting:
```python
# jeonnam_scraper.py - no sys.exit() with meaningful codes
def main():
    collect_articles(...)
    # No return code on success/failure
```

- Should scrapers return exit codes (0=success, 1=failure, 2=partial)?
- Should scrapers write status to scraper_state table?

### Q4. Article ID vs Latest Detection

Current flow:
1. Monitor detects "new article" by comparing latest ID
2. trigger_scraper receives article_ids list
3. BUT scraper ignores article_ids and collects ALL recent articles

**Question:** Is this correct behavior, or should:
- Scraper only collect specified article IDs?
- Or current behavior is intentional (collect all recent)?

---

## 5. Test Commands

```bash
# Test scraper directly
cd d:\cbt\koreanews\scrapers
python jeonnam/jeonnam_scraper.py --max-articles 3 --dry-run

# Test trigger_scraper function
python -c "
import sys
sys.path.insert(0, 'd:\\cbt\\koreanews\\scrapers')
from utils.playwright_monitor import trigger_scraper
result = trigger_scraper('jeonnam', ['123456'])
print(f'Result: {result}')
"

# Test full monitoring cycle
python scrapers/utils/playwright_monitor.py --max-cycles 1 --visible
```

---

## 6. Expected Console Output (When Working)

```
[DAEMON] Cycle #1 starting at 2025-12-29 15:30:00
...
[DAEMON] Cycle #1 complete:
  - New articles: 2 regions
  - Blocked: 0 regions
  - Errors: 0 regions

[DAEMON] Triggering scrapers for 2 regions...
[SCRAPER] Command: python d:\...\jeonnam\jeonnam_scraper.py --max-articles 3
[SCRAPER] Starting scraper for jeonnam: 3 articles
[SCRAPER] Completed for jeonnam
[SCRAPER] Command: python d:\...\mokpo\mokpo_scraper.py --max-articles 1
[SCRAPER] Starting scraper for mokpo: 1 articles
[SCRAPER] Completed for mokpo
[DAEMON] Scrapers completed: 2/2

[DAEMON] Triggering AI processing...
```

---

## 7. Files Involved

| File | Role |
|------|------|
| `scrapers/utils/playwright_monitor.py` | Main monitoring orchestrator |
| `scrapers/jeonnam/jeonnam_scraper.py` | Regional scraper (example) |
| `scrapers/configs/regional_configs.py` | Region URL/selector configs |
| `src/app/api/bot/realtime-monitor/route.ts` | Dashboard API |

---

## 8. Summary

| Issue | Status | Fix |
|-------|--------|-----|
| `--article-ids` not supported | ✅ FIXED | Removed from trigger_scraper |
| trigger_scraper not being called | ❓ UNKNOWN | Need console log verification |
| Scraper subprocess errors | ❓ UNKNOWN | Need error output verification |
| has_new detection logic | ❓ UNKNOWN | Need state comparison verification |

**Immediate Next Step:**
Run monitoring with `--visible` and `--max-cycles 1` to see full console output, especially looking for:
1. `[DAEMON] Triggering scrapers...` message
2. `[SCRAPER] Command:` message
3. `[SCRAPER] Starting/Completed/Failed` messages
