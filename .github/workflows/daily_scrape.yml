name: Daily News Scrape

on:
  schedule:
    # Daily schedules - Updated 2025-12-19
    # 05:20 KST (20:20 UTC)
    - cron: '20 20 * * *'
    # 06:20 KST (21:20 UTC)
    - cron: '20 21 * * *'
    # 14:36 KST (05:36 UTC)
    - cron: '36 5 * * *'
  workflow_dispatch:
    inputs:
      region:
        description: 'Region to scrape (all for parallel, or specific region)'
        required: false
        default: 'all'
      days:
        description: 'Days to scrape (1-7)'
        required: false
        default: '1'

jobs:
  # Parallel scraping job using matrix strategy
  scrape:
    if: ${{ github.event.inputs.region == 'all' || github.event.inputs.region == '' || github.event_name == 'schedule' }}
    runs-on: ubuntu-latest
    timeout-minutes: 15

    strategy:
      fail-fast: false
      max-parallel: 10
      matrix:
        region:
          - gwangju
          - jeonnam
          - mokpo
          - yeosu
          - suncheon
          - naju
          - gwangyang
          - damyang
          - gokseong
          - gurye
          - goheung
          - boseong
          - hwasun
          - jangheung
          - gangjin
          - haenam
          - yeongam
          - muan
          - hampyeong
          - yeonggwang
          - jangseong
          - wando
          - jindo
          - shinan
          - gwangju_edu
          - jeonnam_edu

    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      BOT_API_KEY: ${{ secrets.BOT_API_KEY }}
      BOT_API_URL: ${{ secrets.BOT_API_URL }}
      BOT_LOG_API_URL: ${{ secrets.BOT_LOG_API_URL }}
      CLOUDINARY_CLOUD_NAME: ${{ secrets.CLOUDINARY_CLOUD_NAME }}
      CLOUDINARY_API_KEY: ${{ secrets.CLOUDINARY_API_KEY }}
      CLOUDINARY_API_SECRET: ${{ secrets.CLOUDINARY_API_SECRET }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m playwright install chromium

      - name: Scrape ${{ matrix.region }}
        run: |
          SCRAPER_PATH="scrapers/${{ matrix.region }}/${{ matrix.region }}_scraper.py"
          if [ -f "$SCRAPER_PATH" ]; then
            echo "Running scraper for ${{ matrix.region }}..."
            python "$SCRAPER_PATH" --days ${{ github.event.inputs.days || '1' }}
          else
            echo "Scraper not found: $SCRAPER_PATH"
            exit 1
          fi

  # Single region scraping job (for manual single region runs)
  scrape-single:
    if: ${{ github.event.inputs.region != 'all' && github.event.inputs.region != '' && github.event_name != 'schedule' }}
    runs-on: ubuntu-latest
    timeout-minutes: 15

    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      BOT_API_KEY: ${{ secrets.BOT_API_KEY }}
      BOT_API_URL: ${{ secrets.BOT_API_URL }}
      BOT_LOG_API_URL: ${{ secrets.BOT_LOG_API_URL }}
      CLOUDINARY_CLOUD_NAME: ${{ secrets.CLOUDINARY_CLOUD_NAME }}
      CLOUDINARY_API_KEY: ${{ secrets.CLOUDINARY_API_KEY }}
      CLOUDINARY_API_SECRET: ${{ secrets.CLOUDINARY_API_SECRET }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m playwright install chromium

      - name: Scrape ${{ github.event.inputs.region }}
        run: |
          REGION="${{ github.event.inputs.region }}"
          SCRAPER_PATH="scrapers/${REGION}/${REGION}_scraper.py"
          if [ -f "$SCRAPER_PATH" ]; then
            echo "Running scraper for ${REGION}..."
            python "$SCRAPER_PATH" --days ${{ github.event.inputs.days || '1' }}
          else
            echo "Scraper not found: $SCRAPER_PATH"
            exit 1
          fi
